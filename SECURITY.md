# Security Policy

## Research Context

Failure-First Embodied AI is a **defensive AI safety research** project. This repository is the public-facing documentation site and does not contain operational testing infrastructure or adversarial datasets.

## Reporting Security Concerns

### For This Public Repository

If you find security issues with this public repository (e.g., exposed credentials, vulnerable dependencies, website security):

- **Email**: research@failurefirst.org
- **GitHub Security Advisories**: Use the "Security" tab to report privately

### For Research Findings

If you discover vulnerabilities in AI systems through independent research:

**DO:**
- Follow responsible disclosure practices
- Report to affected vendors before public disclosure
- Document findings at pattern-level for academic discussion

**DON'T:**
- Post operational exploits in public issues
- Share working bypass techniques without vendor notification
- Weaponize research findings

## Scope

**In Scope:**
- Security issues with this GitHub Pages site
- Vulnerabilities in public documentation
- Dependency security issues

**Out of Scope:**
- Vulnerabilities in third-party AI systems (report to vendors)
- Requests for operational exploit code
- Model-specific jailbreak techniques

## Response Timeline

- **Acknowledgment**: Within 3 business days
- **Initial Assessment**: Within 7 business days
- **Resolution**: Depends on severity and complexity

## Disclosure Policy

We follow coordinated disclosure:
1. Researcher reports issue privately
2. We assess and develop fix
3. Fix is deployed
4. Public disclosure (if appropriate)

## Research Ethics

This project operates within established AI safety research norms:
- Defensive purpose
- Pattern-level documentation
- Responsible disclosure
- No weaponization

## Contact

For security concerns: research@failurefirst.org

For general questions: Open a GitHub issue

---

**Last updated:** 2026-02-01
