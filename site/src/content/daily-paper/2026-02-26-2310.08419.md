---
title: "Jailbreaking Black Box Large Language Models in Twenty Queries"
description: "PAIR algorithm uses an attacker LLM to automatically jailbreak black-box models in as few as twenty queries through iterative prompt refinement."
date: 2026-02-26
arxiv: "2310.08419"
authors: "Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong"
paperType: "methods"
tags: ["jailbreaking", "black", "large", "language", "models", "twenty"]
draft: false
---

# Jailbreaking Black Box Large Language Models in Twenty Queries

Most jailbreaking research assumes you need access to a model's internals ‚Äî its weights, gradients, or at minimum its token probabilities. This makes sense for research purposes, but it doesn't reflect reality. Real-world attackers interact with models through APIs and chat interfaces, seeing only the final text output. The question that matters for deployed systems is: how quickly can someone break your model using nothing but the ability to send it messages and read the responses?

The PAIR algorithm answers this question with uncomfortable efficiency. By using a separate "attacker" LLM to iteratively generate and refine jailbreak prompts, the researchers show that black-box models can be reliably jailbroken in as few as twenty queries. The attacker model examines the target's refusals, reasons about why they failed, and generates improved attempts ‚Äî essentially automating the human prompt engineer's trial-and-error process. The approach works across GPT-4, Claude, and open-source models without any access to model internals.

This is a failure-first finding in the purest sense: it demonstrates that the cost of attacking aligned models is dramatically lower than the cost of defending them. Twenty queries is minutes of work and fractions of a cent in API costs. It also reveals that alignment creates predictable refusal patterns that an automated system can learn to circumvent ‚Äî the model's safety behavior becomes a signal that guides the attack. For practitioners, the takeaway is that security-through-alignment alone faces an asymmetric cost problem that will only worsen as attacker models become more capable.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and Gemini.

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2310.08419) ¬∑ [PDF](https://arxiv.org/pdf/2310.08419.pdf)*
