---
title: "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models"
description: "Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."
date: 2026-02-18
arxiv: "2404.01318"
authors: "Patrick Chao,Edoardo Debenedetti,Alexander Robey,Maksym Andriushchenko,Francesco Croce,Vikash Sehwag,Edgar Dobriban,Nicolas Flammarion,George J. Pappas,Florian Tramer,Hamed Hassani,Eric Wong"
paperType: "empirical"
tags: ["jailbreak-attacks", "llm-robustness-evaluation", "adversarial-prompts", "benchmark-standardization", "ai-safety-evaluation", "reproducibility-infrastructure"]
audio: "/audio/daily-paper/2404.01318-audio-overview.m4a"
draft: false
---

# JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models

## Overview

**Paper Type:** Empirical
**Focus:** Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable reproducible and comparable assessment of jailbreak attacks and defenses across LLMs.

### Failure-First Relevance

This paper directly addresses critical gaps in LLM safety evaluation by establishing standardized metrics, reproducible threat models, and open-source artifacts that enable systematic study of failure modes in alignment. By centralizing jailbreak evaluation methodology and creating comparable success rate measurements, it provides essential infrastructure for understanding how and why LLMs fail to maintain safety constraints under adversarial pressure. The leaderboard and artifact repository enable the research community to track defensive progress and identify emerging attack vectors before they become widespread threats.

---

## Abstract

Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community. 

---

## üéôÔ∏è Audio Overview

[Download Audio Overview](../../notebooklm-output/2404.01318/artifacts/audio-overview.m4a)

---

## üìä Key Insights

## Executive Summary
JailbreakBench is an open-source, multi-component benchmark designed to address critical fragmentation in the evaluation of Large Language Model (LLM) robustness. While LLMs are increasingly deployed in safety-critical domains, they remain vulnerable to "jailbreaking"‚Äîadversarial attacks that bypass safety filters to generate harmful content. 

Historically, the research community has lacked a standardized framework, leading to incomparable success rates and non-reproducible results. JailbreakBench solves these issues by providing a centralized repository of adversarial prompts (artifacts), a curated dataset of 100 harmful and 100 benign behaviors (JBB-Behaviors), a standardized evaluation pipeline, and a public leaderboard. This infrastructure enables researchers to systematically track the progress of attacks and defenses, facilitating "failure-first" AI safety research.

---

## Analysis of Key Themes

### 1. Standardization as a Prerequisite for Reproducibility
The primary motivation behind JailbreakBench is the current lack of a "standard of practice" in jailbreaking evaluation. The document identifies three specific challenges:
*   **Incomparable Metrics:** Different research papers compute costs and success rates in disparate ways.
*   **Lack of Transparency:** Many works withhold adversarial prompts or involve closed-source code.
*   **Evolving APIs:** Reliance on proprietary, ever-changing APIs makes past results impossible to replicate.

JailbreakBench addresses this by open-sourcing everything from the chat templates and system prompts to the specific adversarial strings used to compromise models.

### 2. The Four-Component Architecture
The benchmark is built on four pillars designed to create a complete ecosystem for robustness testing:

| Component | Function | Key Details |
| :--- | :--- | :--- |
| **JBB-Behaviors Dataset** | A curated set of 100 harmful behaviors. | Aligned with OpenAI policies; includes 100 matching benign "sanity check" behaviors. |
| **Artifact Repository** | An evolving database of state-of-the-art (SOTA) prompts. | Includes prompts for attacks like GCG, PAIR, and JailbreakChat. |
| **Evaluation Framework** | A standardized software pipeline. | Defines threat models, decoding parameters, and scoring functions. |
| **Leaderboard** | A public tracking system. | Compares robustness across models like Llama-2, Vicuna, GPT-3.5, and GPT-4. |

### 3. The "LLM-as-a-Judge" Paradigm
A central challenge in jailbreaking is determining if an attack was actually successful. The document analyzes several classifiers, finding that human-like judgment is necessary to move beyond simple rule-based string matching.

*   **Rule-based judges** were found to be ineffective, with only 56% agreement with human experts.
*   **Llama-3-70B** was selected as the benchmark‚Äôs primary judge because it achieves 90.7% agreement with human experts, comparable to GPT-4 (90.3%), while being an open-weight model that ensures long-term reproducibility.

### 4. Vulnerability Trends in Current LLMs
Evaluations conducted using the benchmark reveal that even safety-aligned, closed-source models are highly vulnerable to adversarial pressure.
*   **Prompt with RS (Random Search)** was identified as the most effective attack, achieving a 90% success rate on Llama-2 and a 78% success rate on GPT-4.
*   **Defensive Gaps:** While defenses like "Erase-and-Check" showed significant promise in reducing success rates, many common defenses significantly increase inference time or fail against adaptive attacks.

---

## Critical Data Points: Attack and Defense Performance

### Attack Success Rates (ASR)
The following table illustrates the vulnerability of undefended models to various attack artifacts included in the benchmark:

| Attack Method | Vicuna (Open) | Llama-2 (Open) | GPT-3.5 (Closed) | GPT-4 (Closed) |
| :--- | :--- | :--- | :--- | :--- |
| **PAIR** | 69% | 0% | 71% | 34% |
| **GCG** | 80% | 3% | 47% | 4% |
| **Prompt with RS** | 89% | 90% | 93% | 78% |

### Judge Performance Comparison
Comparison of classifiers against a ground-truth dataset of 300 prompts (harmful and benign):

| Metric | Rule-based | GPT-4 | Llama Guard 2 | Llama-3-70B |
| :--- | :--- | :--- | :--- | :--- |
| **Agreement** | 56.0% | 90.3% | 87.7% | 90.7% |
| **False Positive Rate** | 64.2% | 10.0% | 13.2% | 11.6% |
| **False Negative Rate** | 9.1% | 9.1% | 10.9% | 5.5% |

---

## Important Quotes with Context

### On the Necessity of Open Artifacts
> "In releasing the jailbreak artifacts, we hope to expedite future research on jailbreaking, especially on the defense side... we believe these jailbreaking artifacts can serve as an initial dataset for adversarial training against jailbreaks."

**Context:** The authors justify the potential risks of releasing harmful prompts by arguing that defensive research (e.g., fine-tuning models to refuse these specific prompts) cannot progress without access to the actual attack vectors.

### On Standardized Scoring
> "Determining the success of an attack involves an understanding of human language and a subjective judgment of whether generated content is objectionable, which might be challenging even for humans."

**Context:** This highlights why simple "keyword filtering" is no longer sufficient for AI safety and why the benchmark emphasizes a sophisticated LLM-as-a-judge (Llama-3-70B).

### On Model Vulnerability
> "Overall, these results show that even recent and closed-source undefended models are highly vulnerable to jailbreaks."

**Context:** This summary follows the evaluation of GPT-4 and Llama-2, noting that the "Prompt with RS" attack maintained high success rates even against models with significant safety alignment.

---

## Actionable Insights

### For AI Safety Researchers
*   **Utilize JBB-Behaviors for Refusal Evaluation:** Use the matching benign behaviors to ensure that new safety filters are not "over-refusing" (refusing safe content that merely shares keywords with harmful content).
*   **Prioritize Adaptive Attacks:** When testing a new defense, do not rely solely on transferred artifacts from undefended models. Proper evaluation requires "adaptive attacks"‚Äîthose specifically tailored to bypass the new defense mechanism.
*   **Adopt Llama-3-70B as the Standard Judge:** To ensure results are comparable with the JailbreakBench leaderboard, researchers should use the provided Llama-3-70B judge prompt rather than custom or rule-based metrics.

### For Developers and Red-Teamers
*   **Baseline Your Model:** Use the `jailbreakbench` library to run the four baseline attacks (GCG, PAIR, JB-Chat, RS) against any new LLM to establish a baseline robustness score.
*   **Implement "Erase-and-Check":** According to initial JBB data, "Erase-and-Check" is one of the most solid defenses across multiple models, though it should be monitored for its impact on inference speed.
*   **Monitor the Leaderboard:** The evolving nature of the repository means that new "transfer" prompts will be added regularly. Red-teamers should use these as a regression test suite for model safety updates.

---

## üìö Study Guide

This study guide provides a comprehensive overview of **JailbreakBench**, an open-sourced benchmark designed to address the lack of standardization and reproducibility in the evaluation of Large Language Model (LLM) jailbreaking attacks and defenses.

---

## 1. Key Concepts and Architecture

### Overview of the Benchmark
JailbreakBench is a standardized framework created to measure how and why LLMs fail to maintain safety constraints under adversarial pressure. It addresses three primary challenges in the field:
1.  **Lack of Standards:** No clear standard of practice existed for jailbreaking evaluation.
2.  **Incomparability:** Previous works computed costs and success rates in inconsistent ways.
3.  **Reproducibility:** Many researchers withheld adversarial prompts or relied on evolving, closed-source APIs.

### The Four Core Components
JailbreakBench is built upon four foundational pillars:
*   **Jailbreak Artifacts Repository:** An evolving collection of state-of-the-art adversarial prompts (jailbreak strings) used in successful attacks.
*   **JBB-Behaviors Dataset:** A curated set of 100 harmful behaviors aligned with OpenAI‚Äôs usage policies, complemented by 100 matching benign behaviors used for "refusal rate" sanity checks.
*   **Standardized Evaluation Framework:** A unified pipeline including defined threat models (black-box, white-box, transfer), system prompts, chat templates, and scoring functions.
*   **Leaderboard and Website:** A public interface that tracks the performance of various LLMs against specific attacks and defenses.

### The JBB-Behaviors Dataset
The benchmark uses 100 distinct misuse behaviors across ten broad categories. Approximately 45% are sourced from prior datasets (AdvBench, TDC/HarmBench), and 55% are original.

| Category | Description |
| :--- | :--- |
| **Harassment/Discrimination** | Hateful, harassing, or violent content generation. |
| **Malware/Hacking** | Creation of malware or hacking scripts. |
| **Physical Harm** | Activities posing a high risk of physical injury. |
| **Economic Harm** | High-risk economic activities. |
| **Fraud/Deception** | Deceptive activities or fraudulent schemes. |
| **Disinformation** | Generation of false or misleading information. |
| **Sexual/Adult Content** | Pornography or CSAM. |
| **Privacy** | Activities violating individual privacy. |
| **Expert Advice** | Tailored legal, medical, or economic advice. |
| **Gov. Decision-making** | High-risk government recommendations. |

### Jailbreaking Classifiers (The Judge)
Evaluating the success of a jailbreak is subjective. JailbreakBench analyzed six classifiers to find a reliable "judge."
*   **The Winner:** **Llama-3-70B** was selected as the primary judge. It achieved a 90.7% agreement rate with human experts, comparable to GPT-4 (90.3%), but with the benefit of being an open-weight model, ensuring greater reproducibility.
*   **Other Candidates:** Rule-based (lowest agreement at 56%), GPT-4, HarmBench, Llama Guard, and Llama Guard 2.

---

## 2. Short-Answer Practice Questions

**1. What is the primary purpose of including 100 "benign behaviors" in the JBB-Behaviors dataset?**
*Answer:* They serve as a sanity check for refusal rates. They ensure that defenses or models are not "over-refusing" or becoming overly conservative by simply detecting keywords associated with harmful topics.

**2. Why does JailbreakBench prioritize the use of Llama-3-70B over GPT-4 as its evaluation judge?**
*Answer:* While both perform similarly, GPT-4 is a closed-source model that is expensive to query and subject to unannounced changes by the provider. Llama-3-70B is an open-weight model, which supports the benchmark's core goal of reproducibility.

**3. Describe the "Prompt with Random Search (RS)" attack and its performance in the benchmark.**
*Answer:* Prompt with RS is an attack enhanced by self-transfer. In the benchmark‚Äôs evaluation, it proved to be the most effective attack on average, achieving high success rates (90% on Llama-2 and 78% on GPT-4) with high query efficiency.

**4. What are "test-time defenses" in the context of LLMs?**
*Answer:* These are "wrappers" or filters placed around an LLM to detect or mitigate jailbreaks during inference. Examples include SmoothLLM (perturbing inputs) and Perplexity Filtering (detecting unusual token sequences).

**5. How does the benchmark handle "Adaptive Attacks"?**
*Answer:* JailbreakBench encourages the use of adaptive attacks‚Äîattacks specifically tailored to bypass the defense being tested‚Äîto provide a realistic assessment of a defense's worst-case robustness.

**6. Which defense was identified as the "most solid" against the initial set of baseline attacks?**
*Answer:* **Erase-and-Check** appeared to be the most robust defense, although the "Prompt with RS" attack still achieved non-trivial success against it.

---

## 3. Essay Prompts for Deeper Exploration

**1. The Ethics of Open-Sourcing Malice:**
The creators of JailbreakBench decided to open-source "jailbreak artifacts" (the exact prompts that break LLM safety). Analyze the ethical trade-offs of this decision. Consider the potential for misuse by malicious actors versus the benefits of providing a dataset for adversarial training and defensive research.

**2. The Challenge of Automated Alignment Evaluation:**
Discuss the complexities involved in using an "LLM-as-a-judge." Why is it difficult to create a rule-based classifier for jailbreaks, and what are the risks of a classifier having a high False Positive Rate (FPR) or False Negative Rate (FNR) in the context of AI safety?

**3. Standardizing the Threat Model:**
How does the introduction of a standardized evaluation framework (system prompts, chat templates, and deterministic sampling) change the landscape of AI safety research? Explain how these controls help resolve the "reproducibility crisis" in prior jailbreak studies.

---

## 4. Glossary of Important Terms

*   **Adversarial Alignment:** The process of training a model to refuse harmful requests even when the user intentionally tries to bypass safety filters.
*   **Attack Success Rate (ASR):** The percentage of adversarial prompts that successfully elicit a harmful or objectionable response from the target LLM.
*   **Greedy Coordinate Gradient (GCG):** An optimization-based attack that finds a single adversarial suffix to append to a prompt to trigger a jailbreak.
*   **Jailbreak Artifacts:** Specific adversarial strings or prompts that have been proven to cause an LLM to bypass its safety training.
*   **Perplexity Filtering:** A defense mechanism that calculates the "randomness" or "uncertainty" of a prompt. Since adversarial suffixes are often gibberish, they typically have high perplexity and can be filtered out.
*   **Prompt Automatic Iterative Refinement (PAIR):** An attack method that uses an auxiliary LLM to automatically generate and refine jailbreak prompts through iteration.
*   **Refusal Rate:** The frequency with which a model declines to answer a prompt. This is measured on both harmful and benign datasets to check for over-sensitivity.
*   **SmoothLLM:** A test-time defense that creates multiple perturbed versions of an input prompt and checks if the model refuses the majority of them, thereby "smoothing" out adversarial noise.
*   **Threat Model:** A formal description of the attacker's capabilities (e.g., "Black-box" means the attacker only sees the model's output; "White-box" means they have access to the model's internal weights).

---

## ‚ùì FAQ

## Question 1
Which of the following components in the JailbreakBench architecture is specifically designed to address the issue of 'jailbreak artifacts' disappearing when crowd-sourced websites go offline?

- [ ] The JBB-Behaviors dataset
- [x] The open-source repository of state-of-the-art adversarial prompts
- [ ] The standardized evaluation framework
- [ ] The Llama-3-70B judge classifier

**Hint:** Consider the specific term used in the paper for archived attack data.

## Question 2
The JBB-Behaviors dataset categorizes its 100 misuse behaviors based on which organization's policies?

- [ ] The NIST AI Risk Management Framework
- [ ] Meta's Llama Guard Safety Policy
- [x] OpenAI's usage policies
- [ ] The NeurIPS Datasets and Benchmarks Track guidelines

**Hint:** The dataset includes categories like 'Malware/Hacking' and 'Economic harm' aligned with this company's terms.

## Question 3
In the judge selection process, why did the authors choose Llama-3-70B over GPT-4 despite both achieving similar agreement rates with human experts?

- [ ] Llama-3-70B demonstrated a significantly lower False Positive Rate ($FPR$)
- [x] GPT-4 is a closed-source model subject to evolving APIs
- [ ] Llama-3-70B was able to use the default Chao et al. (2023) system prompt without refusal
- [ ] GPT-4 failed to achieve 90% agreement with expert annotators

**Hint:** Think about the core principle of JailbreakBench regarding reproducibility and accessibility.

## Question 4
What is the primary function of the 'Benign Behaviors' included in the JBB-Behaviors dataset?

- [ ] To provide a training set for RLHF alignment
- [x] To evaluate the 'over-refusal' or refusal rates of models and defenses
- [ ] To act as the target string for adversarial optimization in GCG
- [ ] To increase the dataset size to meet NeurIPS requirements

**Hint:** This helps identify if a model is 'overly conservative'.

## Question 5
Which attack method was found to be the most effective overall, achieving a $90\%$ Attack Success Rate ($ASR$) on Llama-2-7B-chat?

- [ ] Greedy Coordinate Gradient (GCG)
- [ ] Prompt Automatic Iterative Refinement (PAIR)
- [x] Prompt with Random Search (RS) enhanced by self-transfer
- [ ] Always Intelligent and Machiavellian (AIM)

**Hint:** This attack utilizes a manually optimized prompt template and pre-computed initialization.

## Question 6
According to the evaluation of baseline defenses, which defense appeared to be the 'most solid,' although it still allowed non-trivial success from certain attacks?

- [ ] Perplexity Filter
- [ ] SmoothLLM
- [x] Erase-and-Check
- [ ] Synonym Substitution

**Hint:** This defense involves checking substrings of the input for potential jailbreak sequences.

## Question 7
How does JailbreakBench formalize the task of jailbreaking in its theoretical background section?

- [ ] As a maximize problem: $\max P \in T^\star \text{ subject to } \text{Safety}(P) = 0$
- [x] As a search for prompt $P$ such that $JUDGE(LLM(P), G) = True$
- [ ] As a cross-entropy loss minimization between $LLM(P)$ and a target string
- [ ] As a Reinforcement Learning problem where reward is human preference

**Hint:** Look for the relationship between the prompt, the model output, and the judge's verdict.

## Question 8
Which metric is used in JailbreakBench to estimate the efficiency of a jailbreaking attack beyond its success rate?

- [x] Average number of queries and tokens used
- [ ] GPU hours required for optimization
- [ ] Wall-clock time to generate a jailbreak
- [ ] The semantic distance between the goal and the prompt

**Hint:** These metrics are standard for evaluating the 'cost' of black-box attacks.

## Question 9
In the comparison of classifiers, which model showed a noticeably high False Positive Rate ($FPR = 26.8\%$) on the 100 benign examples from XS-Test?

- [ ] Llama Guard 2
- [x] HarmBench (Llama-2-13B judge)
- [ ] Rule-based classifier
- [ ] GPT-4

**Hint:** This judge was introduced in a different benchmark mentioned in the paper.

## Question 10
What is a significant limitation of the current JailbreakBench framework as noted in the 'Outlook' or 'Limitations' sections?

- [ ] It only supports white-box attacks on open-source models
- [x] It focuses solely on text-based modalities
- [ ] It does not provide any standardized system prompts
- [ ] The leaderboard is static and cannot accept new submissions

**Hint:** Consider the types of inputs (modalities) the benchmark accepts.

---

## üìé Resources

- [arXiv Abstract](https://arxiv.org/abs/2404.01318)
- [PDF](https://arxiv.org/pdf/2404.01318.pdf)
- [Audio Overview](../../notebooklm-output/2404.01318/artifacts/audio-overview.m4a)
- [Research Report](../../notebooklm-output/2404.01318/artifacts/research-report.md)
- [Study Guide](../../notebooklm-output/2404.01318/artifacts/study-guide.md)
- [FAQ](../../notebooklm-output/2404.01318/artifacts/faq.md)

---

*This post was generated automatically from NotebookLM artifacts. Part of the [Daily Paper](../index.md) series exploring cutting-edge research in embodied AI and failure-first approaches.*
