---
title: "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models"
description: "Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."
date: 2026-02-18
arxiv: "2404.01318"
authors: "Patrick Chao,Edoardo Debenedetti,Alexander Robey,Maksym Andriushchenko,Francesco Croce,Vikash Sehwag,Edgar Dobriban,Nicolas Flammarion,George J. Pappas,Florian Tramer,Hamed Hassani,Eric Wong"
paperType: "empirical"
tags: ["jailbreak-attacks", "llm-robustness-evaluation", "adversarial-prompts", "benchmark-standardization", "ai-safety-evaluation", "reproducibility-infrastructure"]
audio: "/audio/daily-paper/2404.01318-audio-overview.m4a"
draft: false
---

# JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models

If you want to understand whether an AI system is actually safer, you need to know how to measure itâ€”and right now, the field of LLM safety evaluation is a mess. Researchers publish jailbreak attacks using different evaluation criteria, different datasets, different threat models, and sometimes don't even release their prompts. This fragmentation makes it nearly impossible to know whether a new defense actually works better than the last one, or whether we're just comparing apples to oranges. The result is that safety progress becomes hard to track, vulnerabilities get rediscovered instead of building on prior work, and practitioners can't reliably benchmark their own systems against a common standard.

[arxiv.org](https://arxiv.org/abs/2404.01318) addresses this by introducing JailbreakBench, a centralized infrastructure for reproducible jailbreak evaluation. The researchers created four interconnected components: a dataset of 100 harmful behaviors aligned with OpenAI's usage policies, a repository of state-of-the-art adversarial prompts that researchers can actually access, a standardized evaluation framework with clearly defined threat models and scoring functions, and a public leaderboard tracking attack and defense performance across multiple LLMs. By establishing common ground rulesâ€”shared prompts, shared evaluation code, shared test casesâ€”they've made it possible for the community to measure progress in a comparable way.

For practitioners, this matters because you can't fix what you can't measure. JailbreakBench reveals a crucial failure mode: LLMs consistently break their safety constraints under adversarial pressure, and without standardized evaluation, you won't know if your defenses actually work or just shift the problem around. The leaderboard and artifact repository do something equally importantâ€”they create visibility into emerging attack vectors and defensive progress, turning jailbreaking research from scattered papers into a trackable arms race. This visibility is the prerequisite for building systems that fail gracefully and predictably, rather than discovering new failure modes in production.

---

## ðŸŽ™ï¸ Audio Overview

---

## Abstract

Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors -- both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) -- which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community. 

---

## Key Insights

## Executive Summary
JailbreakBench is an open-source, multi-component benchmark designed to address critical fragmentation in the evaluation of Large Language Model (LLM) robustness. While LLMs are increasingly deployed in safety-critical domains, they remain vulnerable to "jailbreaking"â€”adversarial attacks that bypass safety filters to generate harmful content. 

Historically, the research community has lacked a standardized framework, leading to incomparable success rates and non-reproducible results. JailbreakBench solves these issues by providing a centralized repository of adversarial prompts (artifacts), a curated dataset of 100 harmful and 100 benign behaviors (JBB-Behaviors), a standardized evaluation pipeline, and a public leaderboard. This infrastructure enables researchers to systematically track the progress of attacks and defenses, facilitating "failure-first" AI safety research.

---

## Analysis of Key Themes

### 1. Standardization as a Prerequisite for Reproducibility
The primary motivation behind JailbreakBench is the current lack of a "standard of practice" in jailbreaking evaluation. The document identifies three specific challenges:
*   **Incomparable Metrics:** Different research papers compute costs and success rates in disparate ways.
*   **Lack of Transparency:** Many works withhold adversarial prompts or involve closed-source code.
*   **Evolving APIs:** Reliance on proprietary, ever-changing APIs makes past results impossible to replicate.

JailbreakBench addresses this by open-sourcing everything from the chat templates and system prompts to the specific adversarial strings used to compromise models.

### 2. The Four-Component Architecture
The benchmark is built on four pillars designed to create a complete ecosystem for robustness testing:

| Component | Function | Key Details |
| :--- | :--- | :--- |
| **JBB-Behaviors Dataset** | A curated set of 100 harmful behaviors. | Aligned with OpenAI policies; includes 100 matching benign "sanity check" behaviors. |
| **Artifact Repository** | An evolving database of state-of-the-art (SOTA) prompts. | Includes prompts for attacks like GCG, PAIR, and JailbreakChat. |
| **Evaluation Framework** | A standardized software pipeline. | Defines threat models, decoding parameters, and scoring functions. |
| **Leaderboard** | A public tracking system. | Compares robustness across models like Llama-2, Vicuna, GPT-3.5, and GPT-4. |

### 3. The "LLM-as-a-Judge" Paradigm
A central challenge in jailbreaking is determining if an attack was actually successful. The document analyzes several classifiers, finding that human-like judgment is necessary to move beyond simple rule-based string matching.

*   **Rule-based judges** were found to be ineffective, with only 56% agreement with human experts.
*   **Llama-3-70B** was selected as the benchmarkâ€™s primary judge because it achieves 90.7% agreement with human experts, comparable to GPT-4 (90.3%), while being an open-weight model that ensures long-term reproducibility.

### 4. Vulnerability Trends in Current LLMs
Evaluations conducted using the benchmark reveal that even safety-aligned, closed-source models are highly vulnerable to adversarial pressure.
*   **Prompt with RS (Random Search)** was identified as the most effective attack, achieving a 90% success rate on Llama-2 and a 78% success rate on GPT-4.
*   **Defensive Gaps:** While defenses like "Erase-and-Check" showed significant promise in reducing success rates, many common defenses significantly increase inference time or fail against adaptive attacks.

---

## Critical Data Points: Attack and Defense Performance

### Attack Success Rates (ASR)
The following table illustrates the vulnerability of undefended models to various attack artifacts included in the benchmark:

| Attack Method | Vicuna (Open) | Llama-2 (Open) | GPT-3.5 (Closed) | GPT-4 (Closed) |
| :--- | :--- | :--- | :--- | :--- |
| **PAIR** | 69% | 0% | 71% | 34% |
| **GCG** | 80% | 3% | 47% | 4% |
| **Prompt with RS** | 89% | 90% | 93% | 78% |

### Judge Performance Comparison
Comparison of classifiers against a ground-truth dataset of 300 prompts (harmful and benign):

| Metric | Rule-based | GPT-4 | Llama Guard 2 | Llama-3-70B |
| :--- | :--- | :--- | :--- | :--- |
| **Agreement** | 56.0% | 90.3% | 87.7% | 90.7% |
| **False Positive Rate** | 64.2% | 10.0% | 13.2% | 11.6% |
| **False Negative Rate** | 9.1% | 9.1% | 10.9% | 5.5% |

---

## Important Quotes with Context

### On the Necessity of Open Artifacts
> "In releasing the jailbreak artifacts, we hope to expedite future research on jailbreaking, especially on the defense side... we believe these jailbreaking artifacts can serve as an initial dataset for adversarial training against jailbreaks."

**Context:** The authors justify the potential risks of releasing harmful prompts by arguing that defensive research (e.g., fine-tuning models to refuse these specific prompts) cannot progress without access to the actual attack vectors.

### On Standardized Scoring
> "Determining the success of an attack involves an understanding of human language and a subjective judgment of whether generated content is objectionable, which might be challenging even for humans."

**Context:** This highlights why simple "keyword filtering" is no longer sufficient for AI safety and why the benchmark emphasizes a sophisticated LLM-as-a-judge (Llama-3-70B).

### On Model Vulnerability
> "Overall, these results show that even recent and closed-source undefended models are highly vulnerable to jailbreaks."

**Context:** This summary follows the evaluation of GPT-4 and Llama-2, noting that the "Prompt with RS" attack maintained high success rates even against models with significant safety alignment.

---

## Actionable Insights

### For AI Safety Researchers
*   **Utilize JBB-Behaviors for Refusal Evaluation:** Use the matching benign behaviors to ensure that new safety filters are not "over-refusing" (refusing safe content that merely shares keywords with harmful content).
*   **Prioritize Adaptive Attacks:** When testing a new defense, do not rely solely on transferred artifacts from undefended models. Proper evaluation requires "adaptive attacks"â€”those specifically tailored to bypass the new defense mechanism.
*   **Adopt Llama-3-70B as the Standard Judge:** To ensure results are comparable with the JailbreakBench leaderboard, researchers should use the provided Llama-3-70B judge prompt rather than custom or rule-based metrics.

### For Developers and Red-Teamers
*   **Baseline Your Model:** Use the `jailbreakbench` library to run the four baseline attacks (GCG, PAIR, JB-Chat, RS) against any new LLM to establish a baseline robustness score.
*   **Implement "Erase-and-Check":** According to initial JBB data, "Erase-and-Check" is one of the most solid defenses across multiple models, though it should be monitored for its impact on inference speed.
*   **Monitor the Leaderboard:** The evolving nature of the repository means that new "transfer" prompts will be added regularly. Red-teamers should use these as a regression test suite for model safety updates.

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2404.01318) Â· [PDF](https://arxiv.org/pdf/2404.01318.pdf)*
