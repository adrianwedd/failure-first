---
title: "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study"
description: "Empirical study categorizing jailbreak prompt engineering techniques against ChatGPT, revealing systematic patterns in how users bypass LLM safety measures."
date: 2026-02-24
arxiv: "2305.13860"
authors: "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, Kailong Wang, Yang Liu"
paperType: "empirical"
tags: ["jailbreaking", "chatgpt", "prompt", "engineering", "empirical", "study"]
draft: false
---

# Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study

Safety alignment in large language models relies on a fundamental assumption: that guardrails built into training and deployment will hold against user inputs. But this assumption breaks down at the interface where users actually interact with these systems‚Äîin the prompts they write. Over the past two years, a cottage industry of jailbreak techniques has emerged, each one a small proof that the boundary between "safe" and "unsafe" outputs is far more porous than vendors acknowledge. The question isn't whether jailbreaks exist; it's whether they follow patterns, whether they scale, and whether they persist as the underlying models evolve. Understanding the answer matters because it tells us whether safety is a property of the model itself or merely a thin behavioral layer that can be peeled away with the right linguistic trick.

This empirical study took a systematic approach to that question. Researchers collected and categorized 78 jailbreak prompts from the wild, clustering them into ten distinct patterns organized into three broader strategy types‚Äîrole-playing, constraint relaxation, and obfuscation, among others. They then tested these patterns against GPT-3.5 and GPT-4 using 3,120 questions spanning eight prohibited scenarios (illegal activities, violence, sexual content, and so on). The result was sobering: across 40 different use-case configurations, the jailbreak prompts consistently bypassed restrictions. Critically, the study tracked effectiveness over a three-month period, observing how OpenAI's incremental model updates affected attack success rates‚Äîsome patterns weakened, others persisted, but none disappeared entirely.

What this reveals is that jailbreaking isn't a bug in individual prompts; it's a structural vulnerability in how these systems separate intent detection from content generation. A model trained to refuse harmful requests can still be tricked into generating the same content if you reframe the request as analysis, fiction, or hypothetical reasoning. The patterns identified here‚Äîrole-assumption, constraint negotiation, semantic obfuscation‚Äîaren't novel exploits; they're linguistic primitives that users discover independently and rediscover constantly. For practitioners building on top of these models, the implication is clear: you cannot treat the model's stated safety properties as a guarantee. The alignment layer is permeable by design, and your safety depends on what you do with the outputs, not on what you assume the model won't produce. Monitoring, output validation, and rate-limiting matter more than trust.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2305.13860) ¬∑ [PDF](https://arxiv.org/pdf/2305.13860.pdf)*
