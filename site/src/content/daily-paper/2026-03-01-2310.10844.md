---
title: "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"
description: "Comprehensive survey categorizing adversarial attacks on LLMs including prompt injection, jailbreaking, and data poisoning, with analysis of defense limitations."
date: 2026-03-01
arxiv: "2310.10844"
authors: "Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh"
paperType: "survey"
tags: ["survey", "vulnerabilities", "large", "language", "models", "revealed"]
draft: false
---

# Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks

The adversarial attack landscape for large language models has expanded rapidly enough that keeping track of what's been tried, what works, and what defenses exist requires dedicated survey work. Individual attack papers demonstrate specific vulnerabilities; surveys reveal the shape of the overall threat surface. For anyone responsible for deploying or securing LLM-based systems, understanding the taxonomy of known attacks is a prerequisite for building meaningful defenses.

This survey systematically categorizes adversarial attacks on LLMs across multiple dimensions: prompt injection, jailbreaking, data poisoning, and backdoor attacks. The authors map the relationships between attack strategies, analyze which defenses have been proposed for each category, and ‚Äî crucially ‚Äî assess where those defenses fall short. The survey covers both white-box attacks requiring model access and black-box attacks that work through API interactions, providing a comprehensive view of what adversaries can do at each level of access.

For the failure-first framing, surveys like this serve as vulnerability maps. The value isn't in any single finding but in the pattern that emerges: defenses tend to be designed against specific attack types and fail against novel ones, attack techniques transfer across model families in ways that defenses often don't, and the gap between published attacks and deployed defenses consistently favors the attacker. Practitioners should use this survey as a checklist ‚Äî if you haven't considered each category of attack against your specific deployment, you have uncharacterized risk.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2310.10844) ¬∑ [PDF](https://arxiv.org/pdf/2310.10844.pdf)*
