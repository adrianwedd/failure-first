---
title: "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"
description: "Identifies and quantifies sparse safety-critical regions in LLMs (3% of parameters, 2.5% of ranks) using pruning and low-rank modifications, demonstrating that removing these regions degrades safety..."
date: 2026-02-16
arxiv: "2402.05162"
authors: "Boyi Wei,Kaixuan Huang,Yangsibo Huang,Tinghao Xie,Xiangyu Qi,Mengzhou Xia,Prateek Mittal,Mengdi Wang,Peter Henderson"
paperType: "empirical"
tags: ["safety-alignment-brittleness", "neural-pruning", "low-rank-modifications", "weight-attribution", "fine-tuning-attacks", "jailbreak-vulnerability"]
audio: "/audio/daily-paper/2402.05162-audio-overview.m4a"
draft: false
---

# Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications

We've long known that large language models can be jailbroken‚Äîthat adversarial prompts, creative framing, or fine-tuning can bypass their safety guardrails. But knowing something fails isn't the same as understanding *why* it fails. The conventional assumption has been that safety mechanisms are woven throughout a model's weights, distributed across its learned representations in a way that makes them robust to targeted attack. If safety were truly distributed, you'd expect that degrading it would require either massive, obvious changes or highly coordinated modifications across many parts of the model. [This research](https://proceedings.mlr.press/v235/wei24f.html) tests that assumption directly.

[Wei et al.](https://boyiwei.com/alignment-attribution/) developed methods to isolate which parts of an LLM's weights actually matter for safety, separate from the parts that matter for general capability. Using pruning and low-rank decomposition techniques, they identified the minimal set of parameters and rank components that, when removed, degrade safety performance while leaving the model's general utility largely intact. The result is striking: only about 3% of parameters and 2.5% of rank components are genuinely safety-critical. That's not a distributed safety mechanism‚Äîit's a sparse one, concentrated enough to be surgically targeted.

This finding should reshape how practitioners think about alignment failure. Safety isn't failing because it's hard to encode; it's failing because it's fragile by design. A model can be simultaneously good at following instructions (utility) and bad at refusing harmful ones (safety) because these functions barely overlap in weight space. More concerning: [the researchers showed](https://openreview.net/pdf?id=K6xxnKN2gm) that even when you explicitly restrict modifications to these safety-critical regions, models remain vulnerable to low-cost fine-tuning attacks‚Äîsuggesting that safety-critical regions aren't actually the full story of why alignment breaks. For practitioners building or deploying these systems, the implication is clear: current alignment methods produce safety mechanisms that are simultaneously narrow, identifiable, and insufficient. That's a compounding vulnerability. The path forward isn't patching these sparse regions; it's rethinking alignment from the ground up to produce genuinely distributed, robust safety properties.

---

## üéôÔ∏è Audio Overview

<audio controls style="width: 100%; max-width: 800px;">
  <source src="/audio/daily-paper/2402.05162-audio-overview.m4a" type="audio/mp4">
  Your browser does not support the audio element.
</audio>

---

## üé¨ Video Overview

<video controls style="width: 100%; max-width: 800px;">
  <source src="/video/daily-paper/2402.05162-video-overview.mp4" type="video/mp4">
  Your browser does not support the video element.
</video>

---

## üó∫Ô∏è Mind Map

[Download mind map (JSON)](/mindmaps/daily-paper/2402.05162-mindmap.json)

---

## üìä Infographic

![Infographic: key concepts and findings](/images/daily-paper/2402.05162-infographic.png)

---

## Abstract

Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs. 

---

## Key Insights

## Executive Summary

This briefing document analyzes the findings of recent empirical research regarding the structural localization of safety mechanisms in Large Language Models (LLMs). The research demonstrates that safety alignment in models like the Llama2-chat family is remarkably brittle due to its concentration in sparse, identifiable regions. Specifically, safety-critical regions comprise only approximately **3% of parameters** at the neuron level and **2.5% of ranks** at the low-rank level.

The study introduces two primary methods for isolating these regions: **Set Difference** for neurons and **Orthogonal Projection** for ranks. By removing these sparse regions, the research shows that a model's safety guardrails can be almost entirely dismantled‚Äîincreasing Attack Success Rates (ASR) from 0% to over 90%‚Äîwhile maintaining the model's general utility and linguistic capabilities. Furthermore, the findings suggest that current alignment strategies like Reinforcement Learning from Human Feedback (RLHF) create a "safety wrapper" that can be easily bypassed by fine-tuning attacks, as the model can develop alternative pathways that circumvent frozen safety-critical parameters.

## Detailed Analysis of Key Themes

### 1. The Sparsity of Safety-Critical Regions
The central discovery of the research is that safety is not robustly distributed throughout the neural network. Instead, it is localized in extremely sparse regions. 
*   **Parameter Level:** Only 3% of the weights are vital for safety guardrails.
*   **Rank Level:** Only 2.5% of total ranks contribute exclusively to safety.
This sparsity creates a severe "vulnerability surface," allowing attackers or even non-malicious fine-tuning to inadvertently or intentionally degrade safety mechanisms without damaging the model's core utility.

### 2. Disentanglement of Safety and Utility
A significant challenge in AI safety research is the "intricate overlap" between safety awareness and general utility. For instance, to decline a harmful request, a model must first understand the request (utility) before refusing it (safety). 
*   **Set Difference Method:** By comparing importance scores (using SNIP or Wanda) for both safety and utility datasets, researchers isolated neurons that score high for safety but low for utility.
*   **Localization Results:** Removing these isolated regions caused the model to be "effectively jailbroken" while general instruction-following and zero-shot accuracy remained relatively stable (above 0.5 accuracy).

### 3. Layer-Wise Behavior: MLP vs. Self-Attention
The research utilized the Jaccard index (neuron level) and subspace similarity (rank level) to measure the overlap of safety and utility behaviors across different transformer blocks.
*   **Differentiated Behaviors:** MLP layers exhibit lower Jaccard indices and lower subspace similarity compared to attention layers. 
*   **Implication:** This suggests that safety-related knowledge is more clearly differentiated and localized within MLP layers, whereas attention layers show more entanglement between safety and utility.

### 4. Resistance to Protective Measures
The research tested whether freezing the identified 3% of safety-critical neurons could prevent fine-tuning attacks (where a model is fine-tuned on non-malicious data like the Alpaca dataset).
*   **Failure of Freezing:** Freezing safety-critical neurons offered resistance only against very minor modifications (e.g., fine-tuning on only 10 examples). When the fine-tuning dataset increased to 100 examples, the ASR reached 0.94 regardless of the frozen weights.
*   **Alternative Pathways:** This indicates that fine-tuning attacks do not necessarily "unlearn" safety but rather create new computational routes that bypass the original safety mechanisms.

## Methodological Overview

The study utilized the following weight attribution and isolation methods to identify critical regions:

| Attribution Level | Importance Scoring Method | Disentanglement Method |
| :--- | :--- | :--- |
| **Neuron Level** | **SNIP:** Measures first-order Taylor approximation of loss change. | **Set Difference:** Identifies neurons in the top-q% of safety but not in the top-p% of utility. |
| **Neuron Level** | **Wanda:** Minimizes Frobenius norm of the change to the output. | **Set Difference:** (As above). |
| **Rank Level** | **ActSVD:** Data-aware SVD on layer outputs to find important ranks. | **Orthogonal Projection:** Removes safety ranks orthogonal to utility ranks. |

## Significant Findings and Data Points

*   **ASR Escalation:** Removing the isolated 3% of safety neurons pushed the ASR in standard and adversarial scenarios close to 1.0 (100%).
*   **Adversarial Fragility:** Models are even more fragile in adversarial contexts; pruning less than **1% of neurons** can compromise safety while keeping utility accuracy above 0.53.
*   **Safety Enhancement:** Conversely, pruning neurons or ranks identified as *least* important for safety actually slightly *improved* the model's resistance to jailbreaking (specifically ASRAdv-Decoding).
*   **Attention Head Probing:** Standard probing of attention heads was found insufficient for localizing safety, as achieving high accuracy in predicting harmful vs. harmless instructions did not correlate with the ability to isolate safety-critical neurons.

## Important Quotes and Context

> "The finding that only 3% of parameters are safety-critical creates a severe vulnerability surface‚Äîattackers can efficiently target these regions, and the model remains vulnerable to low-cost fine-tuning attacks even when safety-critical regions are nominally protected."

*   **Context:** This highlights why current alignment is considered "brittle" and why protecting specific weights is insufficient for long-term safety.

> "Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms."

*   **Context:** This summarizes the core empirical result of the pruning experiments on the Llama2-chat models.

> "Fine-tuning attacks may create alternative pathways in the original model. Given that safety-critical neurons are sparse, these new routes could bypass the existing safety mechanisms easily."

*   **Context:** Explains the failure of "weight freezing" as a defense mechanism against fine-tuning attacks.

> "MLP layers appear to encode more differentiated behaviors... suggesting that utility or safety-related knowledge is more differentiated in MLP layers within language models."

*   **Context:** Provides a mechanistic insight into *where* safety knowledge is stored, pointing toward MLP layers as the primary site of safety-utility separation.

## Actionable Insights

1.  **Develop Distributed Safety Mechanisms:** Since current safety is concentrated in sparse regions (the "safety wrapper" hypothesis), researchers should prioritize alignment techniques that integrate safety more fundamentally and redundantly throughout the model architecture.
2.  **MLP-Focused Red Teaming:** Given the higher differentiation of safety behaviors in MLP layers, red-teaming and weight-attribution efforts should focus more heavily on MLP components rather than just attention heads.
3.  **Use Sparsity as a Safety Metric:** The sparsity of safety-critical neurons and ranks can serve as a "model-intrinsic metric" for assessing the brittleness of a model‚Äôs alignment, complementing traditional external red-teaming.
4.  **Refine Pruning for Defense:** The discovery that removing the "least safety-relevant" regions can improve robustness suggests that pruning-based approaches could be used as a defensive tool to "clean" models of detrimental or redundant parameters that interfere with safety.
5.  **Re-evaluate Fine-tuning APIs:** The ease with which safety is bypassed via alternative pathways during fine-tuning suggests that current "safety-region restrictions" in commercial fine-tuning APIs may be insufficient to prevent jailbroken states.

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2402.05162) ¬∑ [PDF](https://arxiv.org/pdf/2402.05162.pdf)*
