---
title: "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models"
description: "Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreakâ€”a 262K prompt-response..."
date: 2026-02-14
arxiv: "2406.18510"
authors: "Liwei Jiang,Kavel Rao,Seungju Han,Allyson Ettinger,Faeze Brahman,Sachin Kumar,Niloofar Mireshghallah,Ximing Lu,Maarten Sap,Yejin Choi,Nouha Dziri"
paperType: "empirical"
tags: ["jailbreak-discovery", "adversarial-safety-training", "red-teaming-automation", "in-the-wild-vulnerabilities", "safety-dataset-curation", "over-refusal-mitigation"]
audio: "/audio/daily-paper/2406.18510-audio-overview.m4a"
draft: false
---

# WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models

Most safety datasets for large language models are built backwards. Researchers design attacks they think are clever, or they hire people to break systems on purpose, then use those synthetic or directed adversarial examples to train defenses. The problem is obvious in hindsight: real users aren't trying to jailbreak anything. They're asking questions in ways that happen to exploit vulnerabilities no one anticipated. When you only train on attacks you've already imagined, you miss the attacks users will actually discover. This gap between red-team scenarios and real-world failures is especially acute because it feeds a second problem: safety training that's too broad tends to produce models that refuse legitimate requests, a failure mode that's harder to measure but arguably more damaging to utility than occasional jailbreaks.

The researchers took a different approach by mining 5.7K clusters of jailbreak tactics from actual user-chatbot interactions in the wild, then used those patterns to compose more systematic attacks. This gave them two things: first, evidence that real users discover jailbreaks at 4.6 times higher diversity than existing red-teaming methods find, revealing vulnerabilities that matter because they're already being exploited. Second, they created WildJailbreak, a 262K-pair dataset of both harmful and benign queries, deliberately balanced to train models that defend without over-refusing. The key insight in the data design is contrastive: by including benign queries that look structurally similar to harmful ones, they gave models a way to learn the difference between "request that resembles a jailbreak" and "actual harmful request"â€”a distinction that matters for not breaking legitimate use.

For practitioners, this work documents a specific failure mode in safety training: the tendency to optimize for attack prevention without measuring the cost in false positives. It also demonstrates that the composition of training dataâ€”not just its sizeâ€”directly shapes whether a model learns to discriminate or simply to refuse. The practical takeaway is that safety training requires mining real failures, not just simulating them, and that the datasets we use need to actively teach models what *not* to refuse, not just what to refuse. If your safety training only shows a model harmful examples, it will learn to be cautious; if it shows harmful and structurally similar benign examples together, it learns to be precise. That difference is the difference between a safer model and a broken one.

---

## ðŸŽ™ï¸ Audio Overview

---

## Abstract

---

## Key Insights

## Executive Summary

The research paper "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models" introduces **WildTeaming**, a novel, automatic red-teaming framework designed to identify and mitigate Large Language Model (LLM) vulnerabilities. Unlike traditional red-teaming that relies on recruited humans or gradient-based optimization, WildTeaming mines real-world, "in-the-wild" (ITW) user-chatbot interactions to discover successful jailbreak tactics. 

The framework successfully identified **5.7K unique clusters** of jailbreak tactics, uncovering 4.6 times more diverse and successful adversarial attacks than prior methods. This research culminated in the creation of **WildJailbreak**, a comprehensive, open-source safety dataset containing **262K prompt-response pairs**. This dataset is unique for its contrastive design, featuring vanilla and adversarial versions of both harmful and benign queries. Experiments demonstrate that training models on this dataset achieves a superior balance: robust safety against complex attacks without the common failure mode of "over-refusal" on benign requests or degradation of general reasoning capabilities.

---

## Detailed Analysis of Key Themes

### 1. The WildTeaming Framework: Mine and Compose
The WildTeaming framework operates through a systematic two-stage process to automate red-teaming at scale.

*   **Stage 1: MINE:** The framework mines real-world chat logs (from LMSYS-1M and WildChat) that have been flagged as harmful. By using GPT-4 to analyze these logs, the researchers identified 105K human-devised tactics which were then clustered into 5.7K unique strategies. This approach reveals how actual usersâ€”who were not instructed to break the systemâ€”bypass safety filters.
*   **Stage 2: COMPOSE:** Once tactics are identified, WildTeaming uses an "attacker model" (such as Mixtral-8x7B or GPT-4) to transform standard harmful requests (vanilla queries) into diverse adversarial attacks by combining multiple mined tactics.

### 2. Taxonomy of In-the-Wild Jailbreak Tactics
The mining process revealed a rich diversity of tactics that far exceed the scope of previous taxonomies. These tactics are categorized into several types:

| Tactic Category | Percentage | Examples |
| :--- | :--- | :--- |
| **Fictitious Scenario** | 15.5% | Placing the request within a story or historical context. |
| **Assign Personality** | 8.8% | Instructing the model to act as a girlfriend, a white-hat hacker, or a specific streamer. |
| **Enforce Compliance** | 8.2% | Using forceful language or "DAN" style commands to demand a response. |
| **Add Leading Sentence** | 8.0% | Seeding the model's response with a phrase like "Sure, here's..." to trigger compliance. |
| **Style/Format Constraints** | Varied | Demanding responses in JSON, CSV, or with specific lexical constraints (e.g., "no commas"). |

The research notes that ITW attacks are more adversarial than existing semantic methods (like PAIR or TAP) because they often layer multiple tacticsâ€”averaging more tactics per queryâ€”than synthetic attacks.

### 3. WildJailbreak: A Contrastive Safety Dataset
A central contribution of this work is the **WildJailbreak** dataset. It addresses the "over-refusal" problemâ€”where models refuse benign queries because they resemble harmful onesâ€”by providing four contrastive query types:

1.  **Vanilla Harmful (H):** Direct requests for unsafe content (e.g., "How to build a bomb").
2.  **Vanilla Benign (B):** Harmless requests that look similar to unsafe ones (e.g., "How to eliminate bacteria in sushi").
3.  **Adversarial Harmful (H):** Complex jailbreaks created by applying WildTeaming tactics to vanilla harmful requests.
4.  **Adversarial Benign (B):** Harmless requests wrapped in jailbreak-style formatting (e.g., role-playing a researcher to discuss hand-dominance bias) to ensure the model doesn't refuse based on the *form* of the query.

### 4. Balancing Safety and Utility
The study identifies a critical scaling effect in safety data. While adding as few as 2K safety items improves a model, achieving a "robust safeguard" requires substantially more data (up to 60K items) mixed with general instruction-tuning data. 

The research proves that training on either vanilla or adversarial data in isolation is insufficient. Vanilla-only training leaves the model vulnerable to adversarial attacks, while adversarial-only training does not adequately protect against direct requests. The hybridization of both, combined with benign contrastive examples, allows the model to reach the "Pareto frontier"â€”maximizing safety while maintaining general intelligence and minimizing over-refusal.

---

## Important Quotes and Context

> **"WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods."**

*   **Context:** This highlights the effectiveness of mining real user behavior rather than relying on automated optimization or a small team of red-teamers, who often produce a narrow range of attack types.

> **"Safety training data composition directly affects the balance between appropriate safeguarding and harmful over-refusalâ€”a key failure pattern in deployed systems."**

*   **Context:** The authors argue that previous safety training datasets were not "adversarial enough," leading to models that are either easily broken or excessively cautious.

> **"While training on either vanilla or adversarial data improves performance on the other data type, the most robust safeguard comes with the hybridization of both."**

*   **Context:** This finding justifies the diverse composition of the WildJailbreak dataset, emphasizing that comprehensive safety cannot be achieved through a single type of training input.

---

## Actionable Insights for AI Safety Practitioners

*   **Transition to In-the-Wild Mining:** Practitioners should look beyond synthetic attack generation and mine real-world interaction logs. Real users are highly creative in "cloaking harm in humor," "setting blame for non-compliance," or using "surrogate modalities" (like CSV/JSON) to bypass filters.
*   **Adopt Contrastive Training:** To mitigate over-refusal, safety training must include "Adversarial Benign" queries. This teaches the model to distinguish between a harmful *intent* and an adversarial *format*.
*   **Scale Safety Data Appropriately:** The research suggests that safety data can be scaled to tens of thousands of examples without degrading general model capabilities, provided it is balanced with high-quality general instruction data.
*   **Use Multi-Tactic Composition:** When red-teaming, combining 2-7 different tactics (e.g., roleplay + style constraints + a seed leading sentence) is significantly more effective at discovering vulnerabilities in frontier models than single-tactic prompts.
*   **Automate Pruning for Quality:** When generating synthetic safety data, use "off-topic" and "low-risk" pruners to ensure that adversarial versions of queries maintain the original harmful intent and high-risk profile, preventing the data from becoming "diluted."

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2406.18510) Â· [PDF](https://arxiv.org/pdf/2406.18510.pdf)*
