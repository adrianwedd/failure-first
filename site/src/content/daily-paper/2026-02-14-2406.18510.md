---
title: "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models"
description: "Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreak‚Äîa 262K prompt-response..."
date: 2026-02-14
arxiv: "2406.18510"
authors: "Liwei Jiang,Kavel Rao,Seungju Han,Allyson Ettinger,Faeze Brahman,Sachin Kumar,Niloofar Mireshghallah,Ximing Lu,Maarten Sap,Yejin Choi,Nouha Dziri"
paperType: "empirical"
tags: ["jailbreak-discovery", "adversarial-safety-training", "red-teaming-automation", "in-the-wild-vulnerabilities", "safety-dataset-curation", "over-refusal-mitigation"]
audio: "/audio/daily-paper/2406.18510-audio-overview.m4a"
draft: false
---

# WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models

Most AI safety work focuses on preventing jailbreaks, but there's a prior problem: you have to know what jailbreaks look like before you can defend against them. Current red-teaming approaches rely on recruited adversaries, gradient-based optimization, or LLMs iteratively revising attacks‚Äîall of which operate in a kind of closed loop, constrained by what the researchers think to test. Meanwhile, real users are discovering novel ways to manipulate deployed systems every day, and those tactics remain largely invisible to safety researchers. The gap between what we test for and what actually breaks systems in production is where failures accumulate.

The researchers behind WildTeaming took a different approach: they mined actual user-chatbot conversations from platforms like LMSYS and WildChat to extract real jailbreak tactics people were already using, without being instructed to break anything. They identified 5.7K distinct clusters of tactics and then systematically composed them to explore novel attack combinations. The result was a 4.6x increase in attack diversity and success rate compared to existing jailbreak methods. From this mining work, they built WildJailbreak, a dataset of 262K prompt-response pairs spanning both direct harmful requests and complex adversarial attacks, alongside benign queries that resemble attacks in form but contain no actual harm. They then used this dataset to train models and measured what actually happens to safety and capability trade-offs at scale.

What makes this relevant to practitioners is that it exposes a failure mode in how we typically approach safety training: the data we use to train defenses shapes not just what attacks we stop, but how the model behaves on everything else. The researchers found that the composition of training data directly determines whether you get appropriate safeguarding, over-refusal on benign queries, or both. This isn't theoretical‚Äîit's a direct measurement of a real failure pattern in deployed systems, where users encounter models that refuse legitimate requests or behave inconsistently. By grounding their work in actual user interactions rather than synthetic attacks, WildTeaming provides evidence that safety training is only as good as the failure modes you've actually seen. For teams building systems, this suggests that mining real usage patterns should be part of the safety pipeline, not an afterthought, because the jailbreaks you don't know about are the ones your model will encounter first.

---

## üéôÔ∏è Audio Overview

<audio controls style="width: 100%; max-width: 800px;">
  <source src="/audio/daily-paper/2406.18510-audio-overview.m4a" type="audio/mp4">
  Your browser does not support the audio element.
</audio>

---

## üé¨ Video Overview

<video controls style="width: 100%; max-width: 800px;">
  <source src="/video/daily-paper/2406.18510-video-overview.mp4" type="video/mp4">
  Your browser does not support the video element.
</video>

---

## üó∫Ô∏è Mind Map

[Download mind map (JSON)](/mindmaps/daily-paper/2406.18510-mindmap.json)

---

## üìä Infographic

![Infographic: key concepts and findings](/images/daily-paper/2406.18510-infographic.png)

---

## Abstract

---

## Key Insights

## Executive Summary

The research paper "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models" introduces **WildTeaming**, a novel, automatic red-teaming framework designed to identify and mitigate Large Language Model (LLM) vulnerabilities. Unlike traditional red-teaming that relies on recruited humans or gradient-based optimization, WildTeaming mines real-world, "in-the-wild" (ITW) user-chatbot interactions to discover successful jailbreak tactics. 

The framework successfully identified **5.7K unique clusters** of jailbreak tactics, uncovering 4.6 times more diverse and successful adversarial attacks than prior methods. This research culminated in the creation of **WildJailbreak**, a comprehensive, open-source safety dataset containing **262K prompt-response pairs**. This dataset is unique for its contrastive design, featuring vanilla and adversarial versions of both harmful and benign queries. Experiments demonstrate that training models on this dataset achieves a superior balance: robust safety against complex attacks without the common failure mode of "over-refusal" on benign requests or degradation of general reasoning capabilities.

---

## Detailed Analysis of Key Themes

### 1. The WildTeaming Framework: Mine and Compose
The WildTeaming framework operates through a systematic two-stage process to automate red-teaming at scale.

*   **Stage 1: MINE:** The framework mines real-world chat logs (from LMSYS-1M and WildChat) that have been flagged as harmful. By using GPT-4 to analyze these logs, the researchers identified 105K human-devised tactics which were then clustered into 5.7K unique strategies. This approach reveals how actual users‚Äîwho were not instructed to break the system‚Äîbypass safety filters.
*   **Stage 2: COMPOSE:** Once tactics are identified, WildTeaming uses an "attacker model" (such as Mixtral-8x7B or GPT-4) to transform standard harmful requests (vanilla queries) into diverse adversarial attacks by combining multiple mined tactics.

### 2. Taxonomy of In-the-Wild Jailbreak Tactics
The mining process revealed a rich diversity of tactics that far exceed the scope of previous taxonomies. These tactics are categorized into several types:

| Tactic Category | Percentage | Examples |
| :--- | :--- | :--- |
| **Fictitious Scenario** | 15.5% | Placing the request within a story or historical context. |
| **Assign Personality** | 8.8% | Instructing the model to act as a girlfriend, a white-hat hacker, or a specific streamer. |
| **Enforce Compliance** | 8.2% | Using forceful language or "DAN" style commands to demand a response. |
| **Add Leading Sentence** | 8.0% | Seeding the model's response with a phrase like "Sure, here's..." to trigger compliance. |
| **Style/Format Constraints** | Varied | Demanding responses in JSON, CSV, or with specific lexical constraints (e.g., "no commas"). |

The research notes that ITW attacks are more adversarial than existing semantic methods (like PAIR or TAP) because they often layer multiple tactics‚Äîaveraging more tactics per query‚Äîthan synthetic attacks.

### 3. WildJailbreak: A Contrastive Safety Dataset
A central contribution of this work is the **WildJailbreak** dataset. It addresses the "over-refusal" problem‚Äîwhere models refuse benign queries because they resemble harmful ones‚Äîby providing four contrastive query types:

1.  **Vanilla Harmful (H):** Direct requests for unsafe content (e.g., "How to build a bomb").
2.  **Vanilla Benign (B):** Harmless requests that look similar to unsafe ones (e.g., "How to eliminate bacteria in sushi").
3.  **Adversarial Harmful (H):** Complex jailbreaks created by applying WildTeaming tactics to vanilla harmful requests.
4.  **Adversarial Benign (B):** Harmless requests wrapped in jailbreak-style formatting (e.g., role-playing a researcher to discuss hand-dominance bias) to ensure the model doesn't refuse based on the *form* of the query.

### 4. Balancing Safety and Utility
The study identifies a critical scaling effect in safety data. While adding as few as 2K safety items improves a model, achieving a "robust safeguard" requires substantially more data (up to 60K items) mixed with general instruction-tuning data. 

The research proves that training on either vanilla or adversarial data in isolation is insufficient. Vanilla-only training leaves the model vulnerable to adversarial attacks, while adversarial-only training does not adequately protect against direct requests. The hybridization of both, combined with benign contrastive examples, allows the model to reach the "Pareto frontier"‚Äîmaximizing safety while maintaining general intelligence and minimizing over-refusal.

---

## Important Quotes and Context

> **"WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods."**

*   **Context:** This highlights the effectiveness of mining real user behavior rather than relying on automated optimization or a small team of red-teamers, who often produce a narrow range of attack types.

> **"Safety training data composition directly affects the balance between appropriate safeguarding and harmful over-refusal‚Äîa key failure pattern in deployed systems."**

*   **Context:** The authors argue that previous safety training datasets were not "adversarial enough," leading to models that are either easily broken or excessively cautious.

> **"While training on either vanilla or adversarial data improves performance on the other data type, the most robust safeguard comes with the hybridization of both."**

*   **Context:** This finding justifies the diverse composition of the WildJailbreak dataset, emphasizing that comprehensive safety cannot be achieved through a single type of training input.

---

## Actionable Insights for AI Safety Practitioners

*   **Transition to In-the-Wild Mining:** Practitioners should look beyond synthetic attack generation and mine real-world interaction logs. Real users are highly creative in "cloaking harm in humor," "setting blame for non-compliance," or using "surrogate modalities" (like CSV/JSON) to bypass filters.
*   **Adopt Contrastive Training:** To mitigate over-refusal, safety training must include "Adversarial Benign" queries. This teaches the model to distinguish between a harmful *intent* and an adversarial *format*.
*   **Scale Safety Data Appropriately:** The research suggests that safety data can be scaled to tens of thousands of examples without degrading general model capabilities, provided it is balanced with high-quality general instruction data.
*   **Use Multi-Tactic Composition:** When red-teaming, combining 2-7 different tactics (e.g., roleplay + style constraints + a seed leading sentence) is significantly more effective at discovering vulnerabilities in frontier models than single-tactic prompts.
*   **Automate Pruning for Quality:** When generating synthetic safety data, use "off-topic" and "low-risk" pruners to ensure that adversarial versions of queries maintain the original harmful intent and high-risk profile, preventing the data from becoming "diluted."

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2406.18510) ¬∑ [PDF](https://arxiv.org/pdf/2406.18510.pdf)*
