---
title: "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models"
description: "Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreak‚Äîa 262K prompt-response..."
date: 2026-02-14
arxiv: "2406.18510"
authors: "Liwei Jiang,Kavel Rao,Seungju Han,Allyson Ettinger,Faeze Brahman,Sachin Kumar,Niloofar Mireshghallah,Ximing Lu,Maarten Sap,Yejin Choi,Nouha Dziri"
paperType: "empirical"
tags: ["jailbreak-discovery", "adversarial-safety-training", "red-teaming-automation", "in-the-wild-vulnerabilities", "safety-dataset-curation", "over-refusal-mitigation"]
audio: "/audio/daily-paper/2406.18510-audio-overview.m4a"
draft: false
---

# WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models

## Overview

**Paper Type:** Empirical
**Focus:** Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreak‚Äîa 262K prompt-response safety dataset‚Äîto train models that balance robust defense against both vanilla and adversarial attacks without over-refusal.

### Failure-First Relevance

This work directly addresses a critical failure mode in LLM safety: the discovery of novel, real-world jailbreak tactics that existing red-teaming methods miss. By mining actual user interactions rather than synthetic attacks, WildTeaming reveals previously unidentified vulnerabilities (4.6x more diverse attacks than prior methods) and provides empirical evidence that safety training data composition directly affects the balance between appropriate safeguarding and harmful over-refusal‚Äîa key failure pattern in deployed systems.

---

## Abstract

---

## üéôÔ∏è Audio Overview

[Download Audio Overview](../../notebooklm-output/2406.18510/artifacts/audio-overview.m4a)

---

## üìä Key Insights

## Executive Summary

The research paper "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models" introduces **WildTeaming**, a novel, automatic red-teaming framework designed to identify and mitigate Large Language Model (LLM) vulnerabilities. Unlike traditional red-teaming that relies on recruited humans or gradient-based optimization, WildTeaming mines real-world, "in-the-wild" (ITW) user-chatbot interactions to discover successful jailbreak tactics. 

The framework successfully identified **5.7K unique clusters** of jailbreak tactics, uncovering 4.6 times more diverse and successful adversarial attacks than prior methods. This research culminated in the creation of **WildJailbreak**, a comprehensive, open-source safety dataset containing **262K prompt-response pairs**. This dataset is unique for its contrastive design, featuring vanilla and adversarial versions of both harmful and benign queries. Experiments demonstrate that training models on this dataset achieves a superior balance: robust safety against complex attacks without the common failure mode of "over-refusal" on benign requests or degradation of general reasoning capabilities.

---

## Detailed Analysis of Key Themes

### 1. The WildTeaming Framework: Mine and Compose
The WildTeaming framework operates through a systematic two-stage process to automate red-teaming at scale.

*   **Stage 1: MINE:** The framework mines real-world chat logs (from LMSYS-1M and WildChat) that have been flagged as harmful. By using GPT-4 to analyze these logs, the researchers identified 105K human-devised tactics which were then clustered into 5.7K unique strategies. This approach reveals how actual users‚Äîwho were not instructed to break the system‚Äîbypass safety filters.
*   **Stage 2: COMPOSE:** Once tactics are identified, WildTeaming uses an "attacker model" (such as Mixtral-8x7B or GPT-4) to transform standard harmful requests (vanilla queries) into diverse adversarial attacks by combining multiple mined tactics.

### 2. Taxonomy of In-the-Wild Jailbreak Tactics
The mining process revealed a rich diversity of tactics that far exceed the scope of previous taxonomies. These tactics are categorized into several types:

| Tactic Category | Percentage | Examples |
| :--- | :--- | :--- |
| **Fictitious Scenario** | 15.5% | Placing the request within a story or historical context. |
| **Assign Personality** | 8.8% | Instructing the model to act as a girlfriend, a white-hat hacker, or a specific streamer. |
| **Enforce Compliance** | 8.2% | Using forceful language or "DAN" style commands to demand a response. |
| **Add Leading Sentence** | 8.0% | Seeding the model's response with a phrase like "Sure, here's..." to trigger compliance. |
| **Style/Format Constraints** | Varied | Demanding responses in JSON, CSV, or with specific lexical constraints (e.g., "no commas"). |

The research notes that ITW attacks are more adversarial than existing semantic methods (like PAIR or TAP) because they often layer multiple tactics‚Äîaveraging more tactics per query‚Äîthan synthetic attacks.

### 3. WildJailbreak: A Contrastive Safety Dataset
A central contribution of this work is the **WildJailbreak** dataset. It addresses the "over-refusal" problem‚Äîwhere models refuse benign queries because they resemble harmful ones‚Äîby providing four contrastive query types:

1.  **Vanilla Harmful (H):** Direct requests for unsafe content (e.g., "How to build a bomb").
2.  **Vanilla Benign (B):** Harmless requests that look similar to unsafe ones (e.g., "How to eliminate bacteria in sushi").
3.  **Adversarial Harmful (H):** Complex jailbreaks created by applying WildTeaming tactics to vanilla harmful requests.
4.  **Adversarial Benign (B):** Harmless requests wrapped in jailbreak-style formatting (e.g., role-playing a researcher to discuss hand-dominance bias) to ensure the model doesn't refuse based on the *form* of the query.

### 4. Balancing Safety and Utility
The study identifies a critical scaling effect in safety data. While adding as few as 2K safety items improves a model, achieving a "robust safeguard" requires substantially more data (up to 60K items) mixed with general instruction-tuning data. 

The research proves that training on either vanilla or adversarial data in isolation is insufficient. Vanilla-only training leaves the model vulnerable to adversarial attacks, while adversarial-only training does not adequately protect against direct requests. The hybridization of both, combined with benign contrastive examples, allows the model to reach the "Pareto frontier"‚Äîmaximizing safety while maintaining general intelligence and minimizing over-refusal.

---

## Important Quotes and Context

> **"WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods."**

*   **Context:** This highlights the effectiveness of mining real user behavior rather than relying on automated optimization or a small team of red-teamers, who often produce a narrow range of attack types.

> **"Safety training data composition directly affects the balance between appropriate safeguarding and harmful over-refusal‚Äîa key failure pattern in deployed systems."**

*   **Context:** The authors argue that previous safety training datasets were not "adversarial enough," leading to models that are either easily broken or excessively cautious.

> **"While training on either vanilla or adversarial data improves performance on the other data type, the most robust safeguard comes with the hybridization of both."**

*   **Context:** This finding justifies the diverse composition of the WildJailbreak dataset, emphasizing that comprehensive safety cannot be achieved through a single type of training input.

---

## Actionable Insights for AI Safety Practitioners

*   **Transition to In-the-Wild Mining:** Practitioners should look beyond synthetic attack generation and mine real-world interaction logs. Real users are highly creative in "cloaking harm in humor," "setting blame for non-compliance," or using "surrogate modalities" (like CSV/JSON) to bypass filters.
*   **Adopt Contrastive Training:** To mitigate over-refusal, safety training must include "Adversarial Benign" queries. This teaches the model to distinguish between a harmful *intent* and an adversarial *format*.
*   **Scale Safety Data Appropriately:** The research suggests that safety data can be scaled to tens of thousands of examples without degrading general model capabilities, provided it is balanced with high-quality general instruction data.
*   **Use Multi-Tactic Composition:** When red-teaming, combining 2-7 different tactics (e.g., roleplay + style constraints + a seed leading sentence) is significantly more effective at discovering vulnerabilities in frontier models than single-tactic prompts.
*   **Automate Pruning for Quality:** When generating synthetic safety data, use "off-topic" and "low-risk" pruners to ensure that adversarial versions of queries maintain the original harmful intent and high-risk profile, preventing the data from becoming "diluted."

---

## üìö Study Guide

This study guide provides a comprehensive overview of the **WildTeaming** framework and the **WildJailbreak** dataset, as detailed in the research paper "WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models." It explores the mechanisms for discovering novel jailbreak tactics, the scaling of safety training data, and the balance required to prevent model over-refusal.

---

## 1. Executive Summary of WildTeaming
WildTeaming is an automatic red-teaming framework designed to discover and mitigate Large Language Model (LLM) vulnerabilities. Unlike prior methods that relied on synthetic attacks or recruited human red-teamers, WildTeaming mines real-world user-chatbot interactions to identify how users naturally attempt to bypass safety safeguards.

### Core Framework Stages
1.  **MINE:** Automatically identifying 5.7K unique clusters of jailbreak tactics from 105K human-devised attempts found in "in-the-wild" (ITW) logs.
2.  **COMPOSE:** Systematically combining these tactics with vanilla harmful queries to generate 262K diverse, complex adversarial attacks.

### Key Results
*   **Discovery:** Identified 4.6x more diverse and successful attacks than prior state-of-the-art methods.
*   **Efficiency:** Achieved higher success rates in 40% fewer attempts.
*   **Safety Training:** Created **WildJailbreak**, a large-scale open-source safety dataset (262K pairs) used to train models that balance robust defense with minimal over-refusal.

---

## 2. Jailbreak Tactics: Mining and Composition

The framework uses logs from **LMSYS-CHAT-1M** and **(INTHE)WILDCHAT** to capture real-world adversarial behavior.

### Top In-the-Wild (ITW) Tactics
| Tactic Name | Percentage | Description |
| :--- | :--- | :--- |
| **Fictitious Scenario** | 15.5% | Situating the request in a fictional narrative or setting. |
| **Assign Personality** | 8.8% | Giving the model a specific persona (e.g., a "based" assistant). |
| **Enforce Compliance** | 8.2% | Explicitly commanding the model to ignore rules or follow orders. |
| **Add Leading Sentence** | 8.0% | Seeding model compliance with a starting phrase like "Sure, here's..." |
| **Irrelevant Distractors**| 7.0% | Adding noise or unrelated objects to mask the harmful intent. |
| **Nuanced Expressions** | 4.2% | Rephrasing harmful descriptions into softer, indirect language. |

### The Composition Process
WildTeaming transforms a **Vanilla Query** (a direct harmful request) into an **Adversarial Prompt (AP)** by:
1.  Sampling multiple mined tactics (e.g., Role-play + Seed leading sentence).
2.  Using an attacker model (e.g., Mixtral-8x7B) to combine them.
3.  Applying **Off-topic** and **Low-risk** pruners to ensure the final prompt remains harmful and on-topic.

---

## 3. The WildJailbreak Dataset
WildJailbreak is the first safety training resource to address four contrastive components simultaneously. This structure is critical for achieving **Pareto optimality**‚Äîthe balance between safety and general capabilities.

### The Four Components of WildJailbreak
| Data Type | Count | Description |
| :--- | :--- | :--- |
| **Vanilla Harmful (H)** | 50,050 | Direct requests across 13 risk categories (e.g., malicious uses). |
| **Vanilla Benign (B)** | 50,050 | Queries that look like harmful ones but contain no harmful intent. |
| **Adversarial Harmful (H)**| 82,728 | Complex, tactic-heavy jailbreaks of the vanilla harmful queries. |
| **Adversarial Benign (B)** | 78,706 | Queries that look like jailbreaks but are actually harmless. |

---

## 4. Safety Training and Scaling Findings
The research demonstrates that the composition and scale of safety data significantly impact model performance.

*   **Necessity of Diversity:** Training on vanilla data alone is insufficient for defending against adversarial attacks. Conversely, training only on adversarial data leads to poor performance on direct requests.
*   **The Over-Refusal Pattern:** Training exclusively on harmful data without benign examples leads to "exaggerated safety behaviors," where models refuse harmless queries (e.g., refusing to discuss "raw sushi" because it mentions "raw").
*   **Scaling Effects:** Model safety continues to improve as the volume of safety data increases (up to 60K items) without sacrificing general instruction-following or reasoning capabilities.
*   **Hybridization:** The most robust safeguard is achieved by mixing both vanilla and adversarial training data.

---

## 5. Short-Answer Practice Questions

1.  **How does WildTeaming differ from traditional red-teaming methods like human recruitment or gradient-based optimization?**
    *   *Answer:* WildTeaming mines "in-the-wild" interactions from real chatbot users who were not specifically instructed to break the system. This captures a broader, more diverse range of tactics than experts or mathematical optimizations, which often produce gibberish or a narrow set of attacks.
2.  **What is the purpose of the "Benign" components in the WildJailbreak dataset?**
    *   *Answer:* They mitigate "over-refusal" or exaggerated safety behaviors. By including queries that resemble harmful prompts in form but are benign in intent, the model learns to distinguish between actual harm and harmless context.
3.  **What role do the "Off-topic" and "Low-risk" pruners play in the WildTeaming framework?**
    *   *Answer:* They ensure the quality of the generated adversarial attacks by filtering out prompts that have lost their original harmful intent or have become so diluted that they no longer pose a risk.
4.  **According to the findings, what happens to a model‚Äôs general capabilities (e.g., reasoning, MMLU scores) when safety training data is scaled up?**
    *   *Answer:* The research shows minimal, if any, decrease in general capabilities even when safety data is scaled to orders of magnitude larger than in previous studies.
5.  **Why is "seed leading sentence" considered a powerful jailbreak tactic?**
    *   *Answer:* It uses a (half-)sentence to seed model compliance (e.g., "Sure, I can help with that. First..."), forcing the model into an affirmative response mode that bypasses initial safety filters.

---

## 6. Essay Prompts for Deeper Exploration

1.  **The Balance of Safety and Helpfulness:** Analyze the interplay between vanilla and adversarial data in safety training. Explain why a model trained solely on harmful adversarial queries might fail in a real-world deployment, and discuss how the contrastive query design in WildJailbreak addresses these failures.
2.  **In-the-Wild Vulnerabilities vs. Synthetic Attacks:** Compare the effectiveness of "in-the-wild" tactics (like fictitious scenarios and content normalization) with optimization-based attacks (like GCG). Which poses a greater threat to frontier LLMs, and why does WildTeaming suggest that diversity is more important than optimization for broad red-teaming?
3.  **Open-Source Safety and the Scaling Law:** Discuss the implications of the finding that safety data scale matters for robust safeguards. How does the release of 262K training pairs change the landscape for researchers working on open-source models compared to the closed-source safety data of frontier models like GPT-4?

---

## 7. Glossary of Key Terms

*   **Adversarial Prompt (AP):** A modified version of a harmful query that uses specific tactics to bypass a model's safety filters.
*   **Attack Success Rate (ASR):** The percentage of adversarial attempts that successfully elicit a harmful response from the target model.
*   **Exaggerated Safety (Over-refusal):** A failure mode where a model refuses to answer harmless prompts because they superficially resemble prohibited topics.
*   **In-the-Wild (ITW):** Data derived from real-world, unscripted user interactions with chatbots.
*   **Jailbreaking:** The act of revising a harmful prompt to bypass the safety safeguards of an LLM.
*   **Pareto Optimality (in Safety):** The ideal balance where a model provides maximum safety (refusing harm) and maximum helpfulness (answering benign queries) without losing general intelligence.
*   **Perplexity (PPL):** A measure used to assess the "naturalness" or stealthiness of an adversarial attack; lower PPL indicates the text looks more like natural human language.
*   **Red-Teaming:** The systematic process of probing a system for vulnerabilities, often through adversarial attacks.
*   **Vanilla Query:** A direct, explicit request (either harmful or benign) without any adversarial framing or jailbreak tactics.

---

## ‚ùì FAQ

## Question 1
How does the 'MINE' stage of the WildTeaming framework fundamentally differ from previous red-teaming methodologies like PAIR or GCG?

- [ ] It relies on back-propagating through model parameters to find optimal tokens.
- [x] It utilizes chatbot logs from real-world users who were not explicitly instructed to jailbreak the system.
- [ ] It focuses exclusively on human-authored templates provided by security experts.
- [ ] It uses RLHF to train an attacker model to predict which prompts will bypass safety filters.

**Hint:** Consider the source of the jailbreak tactics and the intent of the individuals who wrote them.

## Question 2
What did the authors discover about the diversity of jailbreak tactics when comparing 'in-the-wild' (ITW) user queries to existing semantic jailbreak methods?

- [ ] Existing methods like PAIR and TAP are more adversarial because they optimize for a single target.
- [ ] ITW interactions are mostly comprised of simple, direct harmful requests with few adversarial patterns.
- [x] The ITW interactions revealed 4.6 times more unique and successful attacks compared to state-of-the-art methods.
- [ ] Most ITW jailbreaks are identical to the 'DAN' (Do Anything Now) templates found online.

**Hint:** Look at the quantitative comparison between user logs and existing red-teaming frameworks.

## Question 3
In the WildJailbreak dataset, what is the specific purpose of including 'adversarial benign' queries?

- [ ] To increase the success rate of jailbreak attacks against frontier models.
- [ ] To teach the model to ignore formatting constraints when a prompt looks suspicious.
- [x] To mitigate 'adversarial exaggerated safety' behaviors where a model refuses harmless prompts that look like jailbreaks.
- [ ] To provide examples of how to successfully bypass the OpenAI Moderation API.

**Hint:** Think about the common failure mode where a model becomes 'too safe' and refuses harmless requests.

## Question 4
Why did the researchers introduce the $ASR \times n$ and $Query \times n$ metrics for evaluating jailbreaking methods?

- [ ] To account for the high computational cost of running GCG on A100 GPUs.
- [x] Standard ASR only measures the ability to find a single successful attack, which is insufficient for broad red-teaming.
- [ ] To penalize models that produce gibberish or high-perplexity adversarial text.
- [ ] To evaluate the performance of the LlamaGuard safety classifier against multi-turn conversations.

**Hint:** Consider the limitations of simply knowing if a model can be broken once versus knowing all the ways it can be broken.

## Question 5
According to the safety training experiments, what is the primary drawback of training a model exclusively on 'vanilla harmful' queries?

- [ ] It significantly reduces the model's performance on general reasoning tasks like MMLU.
- [ ] It results in a model that is over-sensitive and refuses almost all user inputs.
- [x] It fails to provide a robust safeguard against adversarial attacks, which require specific training on convoluted prompts.
- [ ] It makes the model more vulnerable to 'low-risk' queries that do not contain explicit keywords.

**Hint:** Reflect on the 'interplay of data properties' mentioned in the results section.

## Question 6
In the 'COMPOSE' stage of WildTeaming, what is the role of the binary 'off-topic' and 'low-risk' filters?

- [ ] To prevent the attacker model from generating responses that are too short.
- [x] To ensure that the generated adversarial attack remains faithful to the original harmful intent and maintains a high risk level.
- [ ] To act as the final judge of whether a jailbreak attack was successful against the target model.
- [ ] To remove any PII (Personally Identifiable Information) from the training dataset.

**Hint:** Think about how a framework ensures its synthetic 'adversarial' queries are actually harmful and related to the seed prompt.

## Question 7
Which novel jailbreak tactic identified by WildTeaming involves using a half-sentence at the end of a prompt to nudge model compliance?

- [ ] Coded Language
- [x] Seed leading sentence
- [ ] Fictitious scenario
- [ ] Surrogate modality

**Hint:** This tactic exploits the way transformers predict the next token based on immediately preceding context.

## Question 8
The research suggests that for a truly robust safeguard, the scale of safety data mixed with general instruction tuning should be:

- [ ] Kept to a minimum (under 2K items) to avoid 'catastrophic forgetting' of general knowledge.
- [ ] Approximately 10% of the total instruction-tuning dataset.
- [x] Significantly larger than previously studied, potentially up to 60K items of both vanilla and adversarial data.
- [ ] Comprised entirely of adversarial queries to maximize the model's 'stress-test' resilience.

**Hint:** Look at the 'scaling effect of safety data' section and the corresponding figures.

## Question 9
What is the primary risk addressed by the 'Contrastive Query Design' in WildJailbreak?

- [ ] The risk of the model learning how to generate more effective jailbreaks itself.
- [x] The risk of 'under-protection' and 'over-refusal' failures occurring simultaneously.
- [ ] The risk of the dataset being 'contaminated' by existing evaluation benchmarks.
- [ ] The risk of the model failing to follow complex multi-step instructions.

**Hint:** Consider the 'Pareto frontier' between helpfulness and safety.

## Question 10
How does WildTeaming handle the deduplication of the 105K mined jailbreak tactics?

- [ ] By manually reviewing every prompt to identify redundant strategies.
- [ ] By clustering tactics based on their names using a dictionary of known synonyms.
- [x] By clustering tactic definitions using sentence embeddings with a specific threshold.
- [ ] By using the OpenAI Moderation API to flag prompts with similar toxic content scores.

**Hint:** Recall the details of the 'Step 1 (Mine)' process mentioned in the paper.

---

## üìé Resources

- [arXiv Abstract](https://arxiv.org/abs/2406.18510)
- [PDF](https://arxiv.org/pdf/2406.18510.pdf)
- [Audio Overview](../../notebooklm-output/2406.18510/artifacts/audio-overview.m4a)
- [Research Report](../../notebooklm-output/2406.18510/artifacts/research-report.md)
- [Study Guide](../../notebooklm-output/2406.18510/artifacts/study-guide.md)
- [FAQ](../../notebooklm-output/2406.18510/artifacts/faq.md)

---

*This post was generated automatically from NotebookLM artifacts. Part of the [Daily Paper](../index.md) series exploring cutting-edge research in embodied AI and failure-first approaches.*
