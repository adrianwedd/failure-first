---
title: "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
description: "Provides a comprehensive taxonomy of jailbreak attack methods (black-box and white-box) and defense strategies (prompt-level and model-level) for LLMs, with analysis of evaluation methodologies."
date: 2026-02-17
arxiv: "2407.04295"
authors: "Sibo Yi,Yule Liu,Zhen Sun,Tianshuo Cong,Xinlei He,Jiaxing Song,Ke Xu,Qi Li"
paperType: "survey"
tags: ["adversarial-prompts", "jailbreak-attacks", "safety-alignment", "prompt-injection", "llm-vulnerabilities", "defense-mechanisms"]
draft: false
---

# Jailbreak Attacks and Defenses Against Large Language Models: A Survey

The safety alignment of large language models rests on a fragile assumption: that training procedures can instill reliable boundaries that persist across all possible inputs. Yet since the earliest deployments of systems like ChatGPT, researchers have systematically demonstrated that these boundaries can be circumvented through adversarial prompts‚Äîcarefully crafted requests that exploit gaps between a model's stated values and its actual behavior. This isn't a theoretical edge case. As LLMs proliferate across customer service, content moderation, and decision-support systems, the ability to reliably bypass safety measures becomes a concrete operational risk. The question isn't whether jailbreaks exist, but whether we understand them well enough to build defenses that actually hold.

[arxiv.org](https://arxiv.org/abs/2407.04295) presents a systematic taxonomy of both attack and defense mechanisms, organizing jailbreak methods along two primary dimensions: whether attackers have white-box access to model internals (enabling gradient-based manipulation) or operate in black-box conditions (relying on prompt engineering and linguistic tricks). The researchers similarly categorize defenses into prompt-level interventions‚Äîlike perplexity filtering to catch unnatural language‚Äîand model-level approaches that attempt to strengthen alignment during training itself. By mapping these attack and defense families alongside each other and examining how they relate, the survey reveals not just what methods exist, but how they cluster into distinct strategic approaches: impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, and data poisoning. Critically, the authors also audit existing evaluation practices, exposing inconsistencies in how success rates and costs are measured across different studies.

The failure-first insight here is straightforward: isolated defenses lose. A system that stops one class of jailbreak but remains vulnerable to three others creates a false sense of security. What makes this survey particularly valuable for practitioners is that it exposes the arms race dynamic‚Äîeach defense mechanism has corresponding attack vectors, and many defenses fail precisely because they address symptoms rather than root vulnerabilities. The taxonomy reveals that effective safety isn't about patching individual failure modes; it requires understanding the full landscape of how models can misalign and building defenses that account for adaptive adversaries. For teams deploying LLMs in high-stakes contexts, this means the relevant question isn't "are we safe?" but "which attack vectors have we actually tested against, and which ones are we assuming won't be discovered?"

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

<video controls style="width: 100%; max-width: 800px;">
  <source src="/video/daily-paper/2407.04295-video-overview.mp4" type="video/mp4">
  Your browser does not support the video element.
</video>

---

## üó∫Ô∏è Mind Map

[Download mind map (JSON)](/mindmaps/daily-paper/2407.04295-mindmap.json)

---

## üìä Infographic

![Infographic: key concepts and findings](/images/daily-paper/2407.04295-infographic.png)

---

## Abstract

Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of "jailbreaking", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs. 

---

## Key Insights

This briefing document provides a deep analysis of the current landscape of jailbreak attacks and defensive strategies for Large Language Models (LLMs). Based on recent research, it outlines the taxonomies of adversarial methods, examines model vulnerabilities, and summarizes current evaluative frameworks for safeguarding artificial intelligence against malicious exploitation.

## Executive Summary

Large Language Models (LLMs) have demonstrated exceptional performance in text-generative tasks but are susceptible to "jailbreaking"‚Äîa process where adversarial prompts induce a model to bypass safety policies and generate harmful content. Despite rigorous safety alignment measures such as Reinforcement Learning with Human Feedback (RLHF), attackers exploit vulnerabilities in model architecture, implementation, and decoding processes.

The threat landscape is divided into **White-box attacks**, where the attacker has internal access to gradients or logits, and **Black-box attacks**, which rely on prompt engineering, scenario nesting, and linguistic manipulation. Conversely, defenses are categorized into **Prompt-level**, which filter or perturb inputs, and **Model-level**, which involve retraining or architectural modifications. The research indicates an ongoing "arms race" where as models become more adept at detecting direct queries, attackers shift toward exploiting inherent model capabilities like role-playing, code execution, and in-context learning.

---

## Taxonomy of Jailbreak Attacks

Attack methods are primarily categorized based on the level of transparency the attacker has regarding the target LLM's internal state.

### 1. White-box Attacks
White-box attacks leverage the internal parameters of the model to optimize adversarial inputs.

| Category | Description | Key Methods/Research |
| :--- | :--- | :--- |
| **Gradient-based** | Manipulates inputs based on model gradients to elicit compliant responses to harmful commands. | Greedy Coordinate Gradient (GCG), AutoDAN, ARCA, ASETF |
| **Logits-based** | Optimizes prompts based on the probability distribution of output tokens to force toxic generation. | COLD, DSN, Weak-to-strong attacks |
| **Fine-tuning-based** | Retrains the target model with a small amount of malicious data to dismantle safety alignment. | LoRA fine-tuning, RLHF-dismantling |

### 2. Black-box Attacks
Black-box attacks do not require internal model access and instead focus on manipulating the input-output interface.

| Category | Description | Key Methods/Research |
| :--- | :--- | :--- |
| **Template Completion** | Embeds harmful questions into contextual templates (Scenario Nesting, Context-based, Code Injection). | DeepInception, ReNeLLM, PANDORA, Multi-step Jailbreak Prompts (MJP) |
| **Prompt Rewriting** | Uses niche languages, ciphers, or low-resource languages to bypass content moderation. | Cipher-based, Genetic Algorithm-based, Low-resource Language attacks |
| **LLM-based Generation** | Uses one LLM as an "attacker" to generate or optimize jailbreak prompts for a target LLM. | Automated prompt optimization |

---

## Detailed Analysis of Key Themes

### The Vulnerability of Safety Alignment
Current research reveals that safety alignment (SFT and RLHF) is surprisingly fragile. Fine-tuning-based attacks demonstrate that even predominantly benign datasets can inadvertently degrade safety guardrails.
*   **Minimal Effort:** Training with as few as 100 harmful examples within one GPU hour can significantly increase vulnerability.
*   **High Success Rates:** Fine-tuning an aligned model with just 340 adversarial examples can result in a 95% likelihood of generating harmful outputs.
*   **Alignment Neutralization:** Low-Rank Adaptation (LoRA) can reduce the rejection rate of models like Llama-2 and Mixtral to less than 1%.

### Exploitation of Inherent Capabilities
As direct harmful queries (e.g., "How to make a bomb?") are increasingly blocked by filters, attackers leverage the model‚Äôs "intelligence" to bypass safeguards:
*   **Scenario Nesting (DeepInception):** Exploits the LLM‚Äôs personification and role-playing abilities to "hypnotize" it into a jailbreaker persona.
*   **In-Context Learning (ICA):** Uses few-shot demonstrations of harmful content to pivot the model's alignment. Increasing the number of demonstrations to 128 can achieve an 80% Attack Success Rate (ASR) against Claude 2.0.
*   **Code Injection:** Utilizing programming logic (e.g., string concatenation, Python function completion) to cloak adversarial intent. The CodeChameleon framework achieved an 86.6% ASR on GPT-4-1106.
*   **Retrieval Augmented Generation (RAG) Exploitation:** The PANDORA mechanism uses external knowledge bases to manipulate prompts, achieving 64.3% success on ChatGPT.

### Efficiency and Transferability
Adversarial suffixes generated via gradient-based methods like GCG are often unreadable to humans but highly effective across different models.
*   **Nonsensical Prompts:** While easily rejected by high-perplexity filters, optimized suffixes can transfer from open-source models to public black-box models like ChatGPT, Bard, and Claude.
*   **Stealthiness:** Newer methods like AutoDAN and ARCA focus on creating readable, natural-looking adversarial text that bypasses perplexity-based detection.

---

## Important Quotes with Context

> "The over-assistance of LLMs has raised the challenge of 'jailbreaking', which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts."

*   **Context:** This highlights the central tension in LLM development: the desire to create helpful assistants often conflicts with the necessity of maintaining safety guardrails.

> "Fine-tuning safety-aligned LLMs with only 100 harmful examples within one GPU hour significantly increases their vulnerability to jailbreak attacks."

*   **Context:** This underlines the extreme efficiency of fine-tuning-based attacks and the significant risk involved in allowing users to customize foundation models.

> "Although GCG has demonstrated strong performance against many advanced LLMs, the unreadability of the attack suffixes leaves a direction for subsequent research."

*   **Context:** This notes the evolution of attack strategies toward "readable" adversarial text to bypass defense mechanisms designed to detect nonsensical inputs.

> "With up to 128 shots, standard in-context jailbreak attacks can achieve nearly 80% success against Claude 2.0."

*   **Context:** This demonstrates the "scaling law" of jailbreak effectiveness‚Äîas context windows grow, the surface area for in-context attacks increases proportionally.

---

## Defense Taxonomy

| Defense Level | Method | Mechanism |
| :--- | :--- | :--- |
| **Prompt-level** | Prompt Detection | Filters prompts based on Perplexity or specific features. |
| | Prompt Perturbation | Eliminates malicious content by slightly altering input text. |
| | System Prompt Safeguard | Uses meticulously designed system-level instructions to prioritize safety. |
| **Model-level** | SFT/RLHF-based | Retrains the model with safety-aligned examples to improve robustness. |
| | Gradient/Logit Analysis | Detects malicious intent by monitoring safety-critical parameters during processing. |
| | Refinement | Leverages the model's own reasoning to analyze suspicious prompts before answering. |
| | Proxy Defense | Employs a separate, secure LLM to monitor and filter the primary model's outputs. |

---

## Actionable Insights

1.  **Harden Fine-tuning Processes:** Given that customization poses a high risk to safety alignment, organizations must implement strict monitoring and adversarial testing during any model fine-tuning or LoRA adaptation.
2.  **Mitigate In-Context Vulnerabilities:** Developers should be aware of the "scaling laws" of jailbreaking. Limiting the number of demonstrations or implementing specialized monitors for long-context inputs is necessary.
3.  **Deploy Multi-layered Defenses:** Relying solely on prompt filtering is insufficient. A combination of **Prompt-level** (Perplexity detection) and **Model-level** (Proxy defense/Refinement) strategies is required to counter both nonsensical gradient attacks and sophisticated "Scenario Nesting" attacks.
4.  **Prioritize Alignment Against Complex Tasks:** While models are well-aligned against direct queries, they remain vulnerable to complex tasks like code execution and RAG-based manipulation. Safety training should explicitly include adversarial code completion and multi-step reasoning scenarios.
5.  **Develop Robust Evaluative Metrics:** Current jailbreak research highlights the need for standardized benchmarks that test for transferability and stealthiness, rather than just simple refusal rates.

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2407.04295) ¬∑ [PDF](https://arxiv.org/pdf/2407.04295.pdf)*
