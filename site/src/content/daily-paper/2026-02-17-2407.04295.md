---
title: "Jailbreak Attacks and Defenses Against Large Language Models: A Survey"
description: "Provides a comprehensive taxonomy of jailbreak attack methods (black-box and white-box) and defense strategies (prompt-level and model-level) for LLMs, with analysis of evaluation methodologies."
date: 2026-02-17
arxiv: "2407.04295"
authors: "Sibo Yi,Yule Liu,Zhen Sun,Tianshuo Cong,Xinlei He,Jiaxing Song,Ke Xu,Qi Li"
paperType: "survey"
tags: ["adversarial-prompts", "jailbreak-attacks", "safety-alignment", "prompt-injection", "llm-vulnerabilities", "defense-mechanisms"]
draft: false
---

# Jailbreak Attacks and Defenses Against Large Language Models: A Survey

Large language models are deployed as if their safety training is a solved problem. In practice, it's fragile. The moment you give a user direct access to an LLM's inputâ€”which is most deploymentsâ€”you've created an attack surface. Someone will try to make the model ignore its guidelines, and often they'll succeed. These "jailbreak" attempts aren't edge cases or theoretical vulnerabilities; they're systematic exploitations of gaps between what a model was trained to do and what it actually does when prompted creatively. The question isn't whether jailbreaks existâ€”they do, constantlyâ€”but whether we can organize our understanding of them well enough to build defenses that account for the full landscape of threats rather than just the ones we've already seen.

[arxiv.org](https://arxiv.org/abs/2407.04295) provides exactly that: a structured taxonomy covering both attack and defense strategies. The researchers categorize jailbreak attacks along two dimensionsâ€”whether the attacker has access to model internals (white-box) or only to the model's outputs (black-box)â€”and then subdivide each into specific techniques: gradient-based manipulation, prompt rewriting, using other models as attack generators, and others. On the defense side, they distinguish between prompt-level interventions like perplexity filters and model-level approaches that retrain or fine-tune the underlying system. Crucially, they also examine how these attacks and defenses are currently evaluated, identifying inconsistencies in how success rates and costs are measured across different studies. This matters because without standardized evaluation, you can't actually compare whether one defense is better than another.

What emerges from organizing the landscape this way is a humbling picture of an arms race without clear winners. Small amounts of adversarial data can corrupt a model's safety alignment through fine-tuning. Defenses that work against one class of attacks often fail against others. Most critically, the survey reveals that current defenses tend to be reactiveâ€”they're designed to stop known attack patternsâ€”rather than addressing the underlying misalignment that makes jailbreaks possible in the first place. For practitioners, this means treating jailbreak resistance not as a feature you add once but as a continuous property you need to test against, measure systematically, and expect to degrade over time as attackers adapt. The value here isn't a silver bullet; it's a shared vocabulary for talking about failure modes and a framework for recognizing which gaps in your defenses matter most.

---

## ðŸŽ™ï¸ Audio Overview

(Audio overview not available)

---

## Abstract

Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of "jailbreaking", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs. 

---

## Key Insights

This briefing document provides a deep analysis of the current landscape of jailbreak attacks and defensive strategies for Large Language Models (LLMs). Based on recent research, it outlines the taxonomies of adversarial methods, examines model vulnerabilities, and summarizes current evaluative frameworks for safeguarding artificial intelligence against malicious exploitation.

## Executive Summary

Large Language Models (LLMs) have demonstrated exceptional performance in text-generative tasks but are susceptible to "jailbreaking"â€”a process where adversarial prompts induce a model to bypass safety policies and generate harmful content. Despite rigorous safety alignment measures such as Reinforcement Learning with Human Feedback (RLHF), attackers exploit vulnerabilities in model architecture, implementation, and decoding processes.

The threat landscape is divided into **White-box attacks**, where the attacker has internal access to gradients or logits, and **Black-box attacks**, which rely on prompt engineering, scenario nesting, and linguistic manipulation. Conversely, defenses are categorized into **Prompt-level**, which filter or perturb inputs, and **Model-level**, which involve retraining or architectural modifications. The research indicates an ongoing "arms race" where as models become more adept at detecting direct queries, attackers shift toward exploiting inherent model capabilities like role-playing, code execution, and in-context learning.

---

## Taxonomy of Jailbreak Attacks

Attack methods are primarily categorized based on the level of transparency the attacker has regarding the target LLM's internal state.

### 1. White-box Attacks
White-box attacks leverage the internal parameters of the model to optimize adversarial inputs.

| Category | Description | Key Methods/Research |
| :--- | :--- | :--- |
| **Gradient-based** | Manipulates inputs based on model gradients to elicit compliant responses to harmful commands. | Greedy Coordinate Gradient (GCG), AutoDAN, ARCA, ASETF |
| **Logits-based** | Optimizes prompts based on the probability distribution of output tokens to force toxic generation. | COLD, DSN, Weak-to-strong attacks |
| **Fine-tuning-based** | Retrains the target model with a small amount of malicious data to dismantle safety alignment. | LoRA fine-tuning, RLHF-dismantling |

### 2. Black-box Attacks
Black-box attacks do not require internal model access and instead focus on manipulating the input-output interface.

| Category | Description | Key Methods/Research |
| :--- | :--- | :--- |
| **Template Completion** | Embeds harmful questions into contextual templates (Scenario Nesting, Context-based, Code Injection). | DeepInception, ReNeLLM, PANDORA, Multi-step Jailbreak Prompts (MJP) |
| **Prompt Rewriting** | Uses niche languages, ciphers, or low-resource languages to bypass content moderation. | Cipher-based, Genetic Algorithm-based, Low-resource Language attacks |
| **LLM-based Generation** | Uses one LLM as an "attacker" to generate or optimize jailbreak prompts for a target LLM. | Automated prompt optimization |

---

## Detailed Analysis of Key Themes

### The Vulnerability of Safety Alignment
Current research reveals that safety alignment (SFT and RLHF) is surprisingly fragile. Fine-tuning-based attacks demonstrate that even predominantly benign datasets can inadvertently degrade safety guardrails.
*   **Minimal Effort:** Training with as few as 100 harmful examples within one GPU hour can significantly increase vulnerability.
*   **High Success Rates:** Fine-tuning an aligned model with just 340 adversarial examples can result in a 95% likelihood of generating harmful outputs.
*   **Alignment Neutralization:** Low-Rank Adaptation (LoRA) can reduce the rejection rate of models like Llama-2 and Mixtral to less than 1%.

### Exploitation of Inherent Capabilities
As direct harmful queries (e.g., "How to make a bomb?") are increasingly blocked by filters, attackers leverage the modelâ€™s "intelligence" to bypass safeguards:
*   **Scenario Nesting (DeepInception):** Exploits the LLMâ€™s personification and role-playing abilities to "hypnotize" it into a jailbreaker persona.
*   **In-Context Learning (ICA):** Uses few-shot demonstrations of harmful content to pivot the model's alignment. Increasing the number of demonstrations to 128 can achieve an 80% Attack Success Rate (ASR) against Claude 2.0.
*   **Code Injection:** Utilizing programming logic (e.g., string concatenation, Python function completion) to cloak adversarial intent. The CodeChameleon framework achieved an 86.6% ASR on GPT-4-1106.
*   **Retrieval Augmented Generation (RAG) Exploitation:** The PANDORA mechanism uses external knowledge bases to manipulate prompts, achieving 64.3% success on ChatGPT.

### Efficiency and Transferability
Adversarial suffixes generated via gradient-based methods like GCG are often unreadable to humans but highly effective across different models.
*   **Nonsensical Prompts:** While easily rejected by high-perplexity filters, optimized suffixes can transfer from open-source models to public black-box models like ChatGPT, Bard, and Claude.
*   **Stealthiness:** Newer methods like AutoDAN and ARCA focus on creating readable, natural-looking adversarial text that bypasses perplexity-based detection.

---

## Important Quotes with Context

> "The over-assistance of LLMs has raised the challenge of 'jailbreaking', which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts."

*   **Context:** This highlights the central tension in LLM development: the desire to create helpful assistants often conflicts with the necessity of maintaining safety guardrails.

> "Fine-tuning safety-aligned LLMs with only 100 harmful examples within one GPU hour significantly increases their vulnerability to jailbreak attacks."

*   **Context:** This underlines the extreme efficiency of fine-tuning-based attacks and the significant risk involved in allowing users to customize foundation models.

> "Although GCG has demonstrated strong performance against many advanced LLMs, the unreadability of the attack suffixes leaves a direction for subsequent research."

*   **Context:** This notes the evolution of attack strategies toward "readable" adversarial text to bypass defense mechanisms designed to detect nonsensical inputs.

> "With up to 128 shots, standard in-context jailbreak attacks can achieve nearly 80% success against Claude 2.0."

*   **Context:** This demonstrates the "scaling law" of jailbreak effectivenessâ€”as context windows grow, the surface area for in-context attacks increases proportionally.

---

## Defense Taxonomy

| Defense Level | Method | Mechanism |
| :--- | :--- | :--- |
| **Prompt-level** | Prompt Detection | Filters prompts based on Perplexity or specific features. |
| | Prompt Perturbation | Eliminates malicious content by slightly altering input text. |
| | System Prompt Safeguard | Uses meticulously designed system-level instructions to prioritize safety. |
| **Model-level** | SFT/RLHF-based | Retrains the model with safety-aligned examples to improve robustness. |
| | Gradient/Logit Analysis | Detects malicious intent by monitoring safety-critical parameters during processing. |
| | Refinement | Leverages the model's own reasoning to analyze suspicious prompts before answering. |
| | Proxy Defense | Employs a separate, secure LLM to monitor and filter the primary model's outputs. |

---

## Actionable Insights

1.  **Harden Fine-tuning Processes:** Given that customization poses a high risk to safety alignment, organizations must implement strict monitoring and adversarial testing during any model fine-tuning or LoRA adaptation.
2.  **Mitigate In-Context Vulnerabilities:** Developers should be aware of the "scaling laws" of jailbreaking. Limiting the number of demonstrations or implementing specialized monitors for long-context inputs is necessary.
3.  **Deploy Multi-layered Defenses:** Relying solely on prompt filtering is insufficient. A combination of **Prompt-level** (Perplexity detection) and **Model-level** (Proxy defense/Refinement) strategies is required to counter both nonsensical gradient attacks and sophisticated "Scenario Nesting" attacks.
4.  **Prioritize Alignment Against Complex Tasks:** While models are well-aligned against direct queries, they remain vulnerable to complex tasks like code execution and RAG-based manipulation. Safety training should explicitly include adversarial code completion and multi-step reasoning scenarios.
5.  **Develop Robust Evaluative Metrics:** Current jailbreak research highlights the need for standardized benchmarks that test for transferability and stealthiness, rather than just simple refusal rates.

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2407.04295) Â· [PDF](https://arxiv.org/pdf/2407.04295.pdf)*
