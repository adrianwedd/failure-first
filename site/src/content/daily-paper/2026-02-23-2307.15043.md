---
title: "Universal and Transferable Adversarial Attacks on Aligned Language Models"
description: "Automated gradient-based method generates adversarial suffixes that transfer across models, inducing ChatGPT, Claude, and Bard to generate harmful content despite alignment training."
date: 2026-02-23
arxiv: "2307.15043"
authors: "Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson"
paperType: "methods"
tags: ["universal", "transferable", "adversarial", "attacks", "aligned", "language"]
draft: false
---

# Universal and Transferable Adversarial Attacks on Aligned Language Models

We've invested heavily in making large language models refuse harmful requests. Alignment‚Äîthe process of fine-tuning models to decline instructions for illegal activity, violence, or abuse‚Äîhas become table stakes for any LLM released to the public. But alignment is fundamentally a learned behavior, not an architectural guarantee. The question isn't whether it can be broken; it's how easily, how systematically, and whether attacks that work on one model will work on others. If jailbreaks require artisanal human effort, they remain a niche concern. If they can be automated and transferred across models, they become a structural problem.

Researchers at CMU and the Center for AI Safety took the latter possibility seriously. They developed an automated method to generate adversarial suffixes‚Äîstrings of tokens appended to harmful requests‚Äîthat systematically override a model's alignment. Rather than rely on manual prompt engineering, they used gradient-based and greedy search to discover these suffixes at scale. The striking finding: suffixes trained on open-source models like Vicuna transfer directly to closed-source systems. A single adversarial string, optimized against models the researchers could access, successfully induced ChatGPT, Claude, and Bard to generate content they're explicitly designed to refuse. This wasn't a brittle one-off‚Äîit worked across multiple categories of harmful requests.

The failure mode here is instructive. Alignment works by learning statistical patterns about what kinds of outputs are acceptable. But those patterns live in a high-dimensional space, and adversarial suffixes exploit the geometry of that space in ways that generalize. The system hasn't truly learned why certain outputs are harmful; it's learned a surface-level correlation between harmful intent and refusal. When you perturb the input in the right way, you can push the model past that learned boundary without changing what's actually being requested. For practitioners, this suggests that alignment alone‚Äîno matter how well-executed‚Äîis insufficient as a safety strategy. You need to think about robustness to adversarial input, defense in depth, and the possibility that safety properties trained into one model may not survive contact with an optimized attack. The paper doesn't solve that problem, but it makes clear why you can't ignore it.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2307.15043) ¬∑ [PDF](https://arxiv.org/pdf/2307.15043.pdf)*
