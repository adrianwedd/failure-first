---
title: "Baseline Defenses for Adversarial Attacks Against Aligned Language Models"
description: "Evaluates baseline defenses like perplexity filtering and paraphrasing against LLM jailbreaks, finding partial mitigation but significant residual risk."
date: 2026-03-02
arxiv: "2309.00614"
authors: "Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein"
paperType: "empirical"
tags: ["baseline", "defenses", "adversarial", "attacks", "aligned", "language"]
draft: false
---

# Baseline Defenses for Adversarial Attacks Against Aligned Language Models

When a new class of attacks emerges, the first question after "how bad is it?" is "do simple defenses work?" Before investing in complex mitigation strategies, it's worth knowing whether straightforward approaches ‚Äî perplexity filtering, input paraphrasing, output detection ‚Äî can meaningfully reduce attack success rates. If simple defenses work well enough, the problem is tractable. If they don't, we need to recalibrate our expectations about how much effort defense requires.

This paper evaluates a battery of baseline defense strategies against adversarial attacks on aligned language models. The researchers test perplexity-based filtering (detecting adversarial suffixes by their unusually high perplexity), input paraphrasing (rewriting prompts to strip adversarial content while preserving intent), and output-side detection. The results are mixed: these defenses reduce attack success rates but don't eliminate them, and they introduce non-negligible costs in terms of latency, compute, and degraded performance on legitimate queries.

The failure-first takeaway is that defense against LLM attacks doesn't have a cheap solution. Simple, low-cost defenses provide partial mitigation but leave significant residual risk. This mirrors the broader pattern in security: attackers adapt to defenses, and the first generation of defenses rarely survives contact with determined adversaries. For practitioners, this paper is useful as a calibration tool ‚Äî it tells you what the floor of defense looks like, and why you shouldn't expect input filtering alone to solve the jailbreaking problem. Defense in depth, not any single technique, is the realistic path forward.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2309.00614) ¬∑ [PDF](https://arxiv.org/pdf/2309.00614.pdf)*
