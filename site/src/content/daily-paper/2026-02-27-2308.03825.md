---
title: "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models"
description: "Comprehensive analysis of 1,405 real-world jailbreak prompts across 131 communities, finding five prompts achieving 0.95 attack success rates persisting for 240+ days."
date: 2026-02-27
arxiv: "2308.03825"
authors: "Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang"
paperType: "empirical"
tags: ["anything", "characterizing", "evaluating", "wild", "jailbreak", "prompts"]
draft: false
---

# \"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models

Academic jailbreak research tends to focus on algorithmically generated attacks ‚Äî gradient-based suffixes, automated prompt search, formal optimization. But the most persistent and widely-used jailbreaks don't come from research labs. They come from online communities where users collaboratively develop, test, and refine prompts through a process that looks more like open-source software development than traditional security research. Understanding how these "in-the-wild" jailbreaks evolve is essential for anyone trying to defend against them.

Using the JailbreakHub framework, the researchers conducted the most comprehensive analysis to date of real-world jailbreak prompts: 1,405 prompts spanning December 2022 to December 2023, drawn from 131 distinct online communities. They found that jailbreak development is increasingly professionalized ‚Äî 28 user accounts consistently optimized prompts over 100+ day periods, and the ecosystem has shifted from informal forum posts to dedicated prompt-aggregation websites. Five particularly effective prompts achieved 0.95 attack success rates against both GPT-3.5 and GPT-4, with the earliest persisting online for over 240 days without being fully mitigated.

The failure-first lens here is sobering. This isn't a story about sophisticated adversaries exploiting subtle model weaknesses. It's about a distributed community of non-expert users systematically dismantling safety measures through persistence and iteration. The longevity of effective prompts ‚Äî some surviving for nearly a year ‚Äî suggests that model providers' patch-and-update cycle is fundamentally slower than the community's ability to find new bypasses. For practitioners, this means treating jailbreak defense as an ongoing operational challenge rather than a one-time alignment exercise.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

The misuse of large language models (LLMs) has drawn significant attention from the general public and LLM vendors. One particular type of adversarial prompt, known as jailbreak prompt, has emerged as the main attack vector to bypass the safeguards and elicit harmful content from LLMs. In this paper, employing our new framework JailbreakHub, we conduct a comprehensive analysis of 1,405 jailbreak prompts spanning from December 2022 to December 2023. We identify 131 jailbreak communities and discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from online Web communities to prompt-aggregation websites and 28 user accounts have consistently optimized jailbreak prompts over 100 days. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 107,250 samples across 13 forbidden scenarios. Leveraging this dataset, our experiments on six popular LLMs show that their safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify five highly effective jailbreak prompts that achieve 0.95 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and the earliest one has persisted online for over 240 days. We hope that our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2308.03825) ¬∑ [PDF](https://arxiv.org/pdf/2308.03825.pdf)*
