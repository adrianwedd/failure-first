---
title: "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection"
description: "Demonstrates indirect prompt injection attacks where adversarial instructions embedded in external content cause LLM-powered tools to exfiltrate data and execute code."
date: 2026-02-28
arxiv: "2302.12173"
authors: "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz"
paperType: "empirical"
tags: ["what", "signed", "compromising", "real", "world", "integrated"]
draft: false
---

# Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection

Direct prompt injection ‚Äî where a user deliberately crafts a malicious input ‚Äî is concerning but at least involves a willing attacker at the keyboard. Indirect prompt injection is worse: the malicious instructions live in external content that the LLM processes on behalf of an unsuspecting user. When your email assistant summarizes a message containing hidden instructions, or your code copilot processes a repository with embedded adversarial comments, the attack happens without anyone deliberately trying to jailbreak anything.

This paper systematically demonstrates indirect prompt injection against real-world LLM-integrated applications. The researchers show that adversarial instructions embedded in web pages, emails, and documents can cause LLM-powered tools to execute remote code, exfiltrate sensitive data, and conduct social engineering attacks against the user. The attacks work because LLM integrations typically pass retrieved content directly into the model's context window alongside the user's instructions, creating a confused-deputy problem where the model can't distinguish legitimate instructions from adversarial ones.

From a failure-first perspective, this research exposes a systemic architectural vulnerability in how LLMs are deployed. The failure isn't in the model's alignment ‚Äî it's in the integration pattern. Every retrieval-augmented generation system, every tool-using agent, every LLM that processes external data is potentially vulnerable. The practical implication for builders is stark: you cannot assume that alignment training will protect against instructions injected through the data plane. Input sanitization, privilege separation, and output validation become essential ‚Äî the same defensive patterns that web security learned through decades of SQL injection and XSS attacks.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2302.12173) ¬∑ [PDF](https://arxiv.org/pdf/2302.12173.pdf)*
