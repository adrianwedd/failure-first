---
title: "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"
description: "SmoothLLM defends against jailbreaking by randomly perturbing input copies and aggregating predictions, achieving SOTA robustness against GCG, PAIR, and other attacks."
date: 2026-03-04
arxiv: "2310.03684"
authors: "Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas"
paperType: "methods"
tags: ["smoothllm", "defending", "large", "language", "models", "jailbreaking"]
draft: false
---

# SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks

Adversarial attacks on LLMs ‚Äî particularly the GCG suffix attack ‚Äî demonstrated that small perturbations to input text can reliably bypass alignment. This raised an immediate question: are these adversarial prompts robust, or are they fragile? If adversarial suffixes break when you change a few characters, then randomized input perturbation could serve as a practical defense. This insight, borrowed from the randomized smoothing literature in computer vision, motivates a defense that doesn't try to detect adversarial intent but instead destroys the precise token arrangements that attacks depend on.

SmoothLLM applies this principle to LLM jailbreaking. The defense creates multiple perturbed copies of each input prompt ‚Äî inserting, swapping, or patching characters randomly ‚Äî and runs each through the target model. If the original prompt contained an adversarial suffix, the perturbations disrupt the carefully optimized token sequence, causing most copies to produce refusals. By aggregating predictions across copies and flagging inputs where a significant fraction produce harmful outputs, SmoothLLM detects and blocks adversarial prompts. The approach achieves strong robustness against GCG, PAIR, RandomSearch, and AmpleGCG attacks with a manageable trade-off in nominal performance.

The failure-first angle here is about the fragility of attacks versus defenses. SmoothLLM works because current adversarial suffixes are brittle ‚Äî they exploit precise token arrangements that don't survive character-level noise. But this is a property of today's attacks, not a fundamental guarantee. As attack methods evolve to be more robust to perturbation, defenses based on input randomization will need to adapt. For practitioners, SmoothLLM represents a practical, deployable defense layer, but it should be understood as one component of defense in depth ‚Äî not a permanent solution to the adversarial robustness problem.

---

## üéôÔ∏è Audio Overview

(Audio overview not available)

---

## üé¨ Video Overview

(Video overview not available)

---

## üó∫Ô∏è Mind Map

(Mind map not available)

---

## üìä Infographic

(Infographic not available)

---

## Abstract

Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. Across a range of popular LLMs, SmoothLLM sets the state-of-the-art for robustness against the GCG, PAIR, RandomSearch, and AmpleGCG jailbreaks. SmoothLLM is also resistant against adaptive GCG attacks, exhibits a small, though non-negligible trade-off between robustness and nominal performance, and is compatible with any LLM. Our code is publicly available at \url{https://github.com/arobey1/smooth-llm}.

---

## Key Insights

(Not available)

---

*Read the [full paper on arXiv](https://arxiv.org/abs/2310.03684) ¬∑ [PDF](https://arxiv.org/pdf/2310.03684.pdf)*
