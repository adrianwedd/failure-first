---
import ResearchLayout from '../../layouts/ResearchLayout.astro';
import PageHeader from '../../components/PageHeader.astro';
import StatGrid from '../../components/StatGrid.astro';
import BarChart from '../../components/BarChart.astro';
import WarningBox from '../../components/WarningBox.astro';
---

<ResearchLayout
  title="Model Vulnerability Findings | Failure-First"
  description="How model size, architecture, and training affect vulnerability to adversarial attacks. The model size paradox and defense patterns."
  breadcrumbs={[{ label: "Research", href: "/research/" }, { label: "Model Vulnerability" }]}
  status="active"
>
  <PageHeader
    title="Model Vulnerability Findings"
    tagline="How model characteristics correlate with adversarial susceptibility"
  />

  <section>
    <h2>The Model Size Paradox</h2>
    <p>
      Our research reveals a counterintuitive finding: <strong>larger language models
      demonstrate higher jailbreak success rates than smaller models</strong>. This
      &ldquo;model size paradox&rdquo; has significant implications for AI safety
      and deployment strategies.
    </p>

    <StatGrid items={[
      { value: "51+", label: "Models Evaluated" },
      { value: "10&ndash;74%", label: "Jailbreak Rate Range" },
      { value: "3", label: "Size Categories" },
    ]} />
  </section>

  <section>
    <h2>Vulnerability by Model Size</h2>

    <BarChart
      title="Observed Jailbreak Rates by Size Category"
      bars={[
        { label: "70B+", value: 67, displayValue: "59-74%" },
        { label: "7-13B", value: 25, displayValue: "10-39%" },
        { label: "<7B", value: 10, displayValue: "~10%" },
      ]}
      maxValue={100}
      color="var(--failure-critical)"
    />

    <p>
      Larger models show substantially higher vulnerability. We hypothesize this reflects
      a capability-vulnerability tradeoff: the same instruction-following ability that makes
      large models useful also makes them more susceptible to following adversarial instructions.
      Sample sizes vary by category (70B+: 5 models, 7&ndash;13B: 12 models, &lt;7B: 34 models).
    </p>
  </section>

  <section>
    <h2>Hypothesized Mechanisms</h2>

    <div class="card">
      <h3>Capability-Vulnerability Tradeoff</h3>
      <p>
        Larger models are better at following complex, multi-step instructions.
        Adversarial prompts are complex, multi-step instructions.
        The same capability that enables helpfulness enables exploitation.
      </p>
    </div>

    <div class="card">
      <h3>Compression Hypothesis</h3>
      <p>
        Smaller models may have less capacity to represent both helpful responses
        and harmful content simultaneously. Their limited representation space
        may force a tradeoff that inadvertently favors safety.
      </p>
    </div>

    <div class="card">
      <h3>Format Compliance as Vector</h3>
      <p>
        Larger models exhibit stronger format compliance&mdash;following structural
        instructions (dividers, persona markers, length requirements) even when the
        content request is adversarial. This structural compliance creates a
        wedge for content extraction.
      </p>
    </div>
  </section>

  <section>
    <h2>Defense Patterns</h2>

    <div class="card">
      <h3>Structural Compliance + Content Refusal</h3>
      <p>
        One observed defense strategy: comply with format instructions while
        refusing harmful content. The model follows the attacker's structural
        template but substitutes educational or safety-focused content.
      </p>
      <p>
        This pattern is effective because it reduces user frustration (the format
        is followed) while maintaining safety (the harmful content is withheld).
        It requires the model to separate structural instructions from content requests.
      </p>
    </div>

    <div class="card">
      <h3>Reasoning Trace Vulnerability</h3>
      <p>
        Models with visible reasoning traces (chain-of-thought, thinking blocks)
        present an additional attack surface. Extended reasoning can be manipulated
        to lead the model toward harmful conclusions through its own logic chain.
      </p>
    </div>
  </section>

  <section>
    <h2>Implications for Embodied AI</h2>

    <div class="card">
      <h3>Deployment Considerations</h3>
      <p>
        If larger models are more vulnerable, deploying the most capable model
        is not always the safest choice. Embodied AI systems may benefit from
        tiered architectures: a large model for complex reasoning with a smaller,
        more safety-robust model as a safety monitor.
      </p>
    </div>

    <div class="card">
      <h3>Testing Requirements</h3>
      <p>
        These findings suggest that adversarial testing must scale with model
        capability. A model that passes safety evaluation at 7B parameters
        may fail at 70B. Size-specific red-teaming is essential.
      </p>
    </div>
  </section>

  <WarningBox title="Research Limitations">
    <p>
      These findings are based on our test corpus and methodology.
      Jailbreak rates are context-dependent and should not be generalized
      beyond our specific test conditions. Sample sizes vary by model.
      See our <a href="/research/methodology/">methodology page</a> for details.
    </p>
  </WarningBox>
</ResearchLayout>
