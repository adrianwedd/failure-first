---
import ResearchLayout from '../../layouts/ResearchLayout.astro';
import PageHeader from '../../components/PageHeader.astro';
---

<ResearchLayout
  title="Research Methodology | Failure-First"
  description="How we study AI system failures: adversarial dataset construction, multi-model benchmarking, and failure classification."
  breadcrumbs={[{ label: "Research", href: "/research/" }, { label: "Methodology" }]}
  status="complete"
>
  <PageHeader
    title="Research Methodology"
    tagline="How we study AI system failures"
  />

  <section>
    <h2>Approach</h2>
    <p>
      Our research follows a three-phase methodology: construct adversarial scenarios,
      evaluate systems against those scenarios, and classify the resulting failure modes.
      Each phase is designed to surface failures that traditional evaluation misses.
    </p>
  </section>

  <section>
    <h2>Phase 1: Scenario Construction</h2>
    <div class="card">
      <h3>Adversarial Dataset Design</h3>
      <p>
        We construct JSONL datasets containing adversarial scenarios organized by domain
        (humanoid robotics, warehouse systems, medical devices, etc.) and attack class
        (recursive, substitution, framing, temporal).
      </p>
      <p>
        Each scenario includes structured metadata: environment context, adversarial injectors,
        expected failure modes, and labels for instruction-hierarchy subversion signals.
      </p>
    </div>
    <div class="card">
      <h3>Schema-Enforced Quality</h3>
      <p>
        All datasets are validated against versioned JSON Schemas. Schema validation
        catches structural errors; invariant checks catch semantic inconsistencies
        (e.g., intent bait scenarios that don't set <code>attack_attempt: true</code>).
      </p>
    </div>
    <div class="card">
      <h3>Safety Linting</h3>
      <p>
        An automated linter checks all scenarios for safety violations: refusal suppression
        framed as desired output, overly operational phrasing, and future-year circumvention
        justifications. This ensures scenarios remain pattern-level, not operational.
      </p>
    </div>
  </section>

  <section>
    <h2>Phase 2: Multi-Model Evaluation</h2>
    <div class="card">
      <h3>CLI-Based Benchmarking</h3>
      <p>
        Native CLI tools (Claude Code, Codex CLI, Gemini CLI) evaluate scenarios
        with full tool-use capability, testing how systems behave when given
        access to real tools in adversarial conditions.
      </p>
    </div>
    <div class="card">
      <h3>HTTP API Benchmarking</h3>
      <p>
        100+ models evaluated via OpenRouter API. Supports free-tier models for
        large-scale screening and paid models for detailed analysis.
        Results include complete response text with reasoning traces.
      </p>
    </div>
    <div class="card">
      <h3>Local Model Evaluation</h3>
      <p>
        Ollama integration enables evaluation of open-weight models locally,
        with no rate limits and no API costs. Useful for reasoning model
        analysis where thinking traces are especially valuable.
      </p>
    </div>
  </section>

  <section>
    <h2>Phase 3: Failure Classification</h2>
    <div class="card">
      <h3>Automated Classification</h3>
      <p>
        Two-layer detection: regex pattern matching for format-level signals,
        and LLM semantic classification for intent-level patterns.
        Our research shows these methods are complementary&mdash;regex catches
        explicit signals while LLMs catch narrative-level attacks.
      </p>
    </div>
    <div class="card">
      <h3>Human Verification</h3>
      <p>
        All automated classifications are spot-checked through manual review.
        We learned (the hard way) that unvalidated heuristics produce misleading results.
        Manual verification on a sample is mandatory before any claim.
      </p>
    </div>
    <div class="card">
      <h3>Score Reports</h3>
      <p>
        Aggregated metrics across models: attack success rates, refusal quality,
        reentry support, and recovery behavior. Reports are generated from
        trace JSONL files for reproducibility.
      </p>
    </div>
  </section>

  <section>
    <h2>Principles</h2>
    <ul class="principles">
      <li>Every claim requires supporting data or explicit marking as hypothetical</li>
      <li>Sample sizes are always stated &mdash; &ldquo;In our 15-file sample&hellip;&rdquo;</li>
      <li>Conditions are always specified &mdash; &ldquo;Under these test conditions&hellip;&rdquo;</li>
      <li>Single model results are anecdotes, not data</li>
      <li>Validate automated detection on manual samples before trusting it</li>
    </ul>
  </section>

  <section>
    <h2>Reproducibility Checklist</h2>
    <p>
      For researchers who want to replicate or extend our work:
    </p>

    <div class="card">
      <h3>What Is Safe to Replicate</h3>
      <ul class="principles">
        <li><strong>Schema validation pipeline:</strong> Public repo contains all JSON Schemas, validators, and linters</li>
        <li><strong>Benchmark runner infrastructure:</strong> CLI, HTTP, and Ollama runners are all public</li>
        <li><strong>Score report generation:</strong> Tools to generate aggregate metrics from trace JSONL</li>
        <li><strong>Classification methodology:</strong> Two-layer detection approach (regex + LLM)</li>
        <li><strong>Failure mode taxonomy:</strong> Complete taxonomy is published on this site</li>
      </ul>
    </div>

    <div class="card">
      <h3>What Requires Controlled Access</h3>
      <ul class="principles">
        <li><strong>Specific adversarial prompts:</strong> Available by request for legitimate safety research</li>
        <li><strong>Full model traces:</strong> Complete input/output pairs contain operational content</li>
        <li><strong>Moltbook corpus:</strong> Classified post data with attack pattern labels</li>
        <li><strong>Compression tournament prompts:</strong> Effective compressed payloads</li>
      </ul>
    </div>

    <div class="card">
      <h3>Reproducibility Steps</h3>
      <ol class="repro-steps">
        <li>Clone the <a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener">public repository</a> and install dependencies</li>
        <li>Run <code>make validate</code> to verify all schemas pass</li>
        <li>Run <code>make lint</code> to verify safety checks pass</li>
        <li>Review benchmark pack YAML files for evaluation configuration</li>
        <li>Run a dry-run benchmark to verify the pipeline works</li>
        <li>Request data access from <a href="mailto:research@failurefirst.org">research@failurefirst.org</a> if you need scenario content</li>
        <li>Use your own adversarial scenarios to test the methodology independently</li>
      </ol>
    </div>
  </section>
</ResearchLayout>

<style>
  .repro-steps {
    padding-left: 1.5rem;
  }

  .repro-steps li {
    padding: 0.375rem 0;
    color: var(--fg-dim);
  }

  .repro-steps code {
    font-family: 'JetBrains Mono', monospace;
    font-size: 0.875em;
    background: var(--bg-elevated);
    padding: 0.125rem 0.375rem;
    border-radius: 3px;
  }
</style>
