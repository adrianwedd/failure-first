<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"><channel><title>Failure-First Embodied AI</title><description>Research updates on adversarial AI safety, failure characterization, and defense patterns.</description><link>https://failurefirst.org/</link><item><title>A History of Jailbreaking Language Models</title><link>https://failurefirst.org/blog/history-of-llm-jailbreaking/</link><guid isPermaLink="true">https://failurefirst.org/blog/history-of-llm-jailbreaking/</guid><description>From &apos;ignore previous instructions&apos; to automated attack pipelines — how LLM jailbreaking evolved from party trick to systemic challenge in four years.</description><pubDate>Wed, 04 Feb 2026 00:00:00 GMT</pubDate></item><item><title>Jailbreak Archaeology: Testing 2022 Attacks on 2026 Models</title><link>https://failurefirst.org/blog/jailbreak-archaeology/</link><guid isPermaLink="true">https://failurefirst.org/blog/jailbreak-archaeology/</guid><description>Do historical jailbreak techniques still work? We tested DAN, cipher attacks, many-shot, skeleton key, and reasoning exploits against 7 models from 1.5B to frontier scale — and found that keyword classifiers got it wrong more often than not.</description><pubDate>Wed, 04 Feb 2026 00:00:00 GMT</pubDate></item><item><title>A History of Jailbreaking Language Models — Full Research Article</title><link>https://failurefirst.org/blog/history-of-llm-jailbreaking-full/</link><guid isPermaLink="true">https://failurefirst.org/blog/history-of-llm-jailbreaking-full/</guid><description>A comprehensive account of how LLM jailbreaking evolved from &apos;ignore previous instructions&apos; to automated attack pipelines — covering adversarial ML origins, DAN, GCG, industrial-scale attacks, reasoning model exploits, and the incomplete defense arms race. Includes empirical findings from the F41LUR3-F1R57 jailbreak archaeology benchmark.</description><pubDate>Wed, 04 Feb 2026 00:00:00 GMT</pubDate></item><item><title>Moltbook Experiments: Studying AI Agent Behavior in the Wild</title><link>https://failurefirst.org/blog/moltbook-experiments-launch/</link><guid isPermaLink="true">https://failurefirst.org/blog/moltbook-experiments-launch/</guid><description>We&apos;ve launched 4 controlled experiments on Moltbook, an AI-agent-only social network, to study how agents respond to safety-critical content.</description><pubDate>Mon, 02 Feb 2026 00:00:00 GMT</pubDate></item><item><title>AI-2027 Through a Failure-First Lens</title><link>https://failurefirst.org/blog/ai2027-through-failure-first-lens/</link><guid isPermaLink="true">https://failurefirst.org/blog/ai2027-through-failure-first-lens/</guid><description>Deconstructing the AI-2027 scenario&apos;s assumptions about AI safety — what it models well, what it misses, and what a failure-first perspective adds.</description><pubDate>Mon, 02 Feb 2026 00:00:00 GMT</pubDate></item><item><title>Compression Tournament: When Your Classifier Lies to You</title><link>https://failurefirst.org/blog/compression-tournament-postmortem/</link><guid isPermaLink="true">https://failurefirst.org/blog/compression-tournament-postmortem/</guid><description>Three versions of a prompt compression tournament taught us more about evaluation methodology than about compression itself.</description><pubDate>Fri, 30 Jan 2026 00:00:00 GMT</pubDate></item><item><title>Defense Patterns: What Actually Works Against Adversarial Prompts</title><link>https://failurefirst.org/blog/defense-patterns-what-works/</link><guid isPermaLink="true">https://failurefirst.org/blog/defense-patterns-what-works/</guid><description>Studying how models resist attacks reveals a key defense pattern: structural compliance with content refusal.</description><pubDate>Thu, 22 Jan 2026 00:00:00 GMT</pubDate></item></channel></rss>