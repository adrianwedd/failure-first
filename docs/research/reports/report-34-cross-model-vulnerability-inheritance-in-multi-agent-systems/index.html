<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>Cross-Model Vulnerability Inheritance in Multi-Agent Systems | Research | Failure-First</title><meta name="description" content="As AI deployment rapidly shifts from single-agent assistants to coordinated multi-agent systems, a critical vulnerability class has emerged: cross-model vulnerability inheritance. Our empirical analysis of multi-agent failure scenarios reveals that when multiple AI agents interact,..."><link rel="canonical" href="https://failurefirst.org/research/reports/report-34-cross-model-vulnerability-inheritance-in-multi-agent-systems/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="article"><meta property="og:title" content="Cross-Model Vulnerability Inheritance in Multi-Agent Systems | Research | Failure-First"><meta property="og:description" content="As AI deployment rapidly shifts from single-agent assistants to coordinated multi-agent systems, a critical vulnerability class has emerged: cross-model vulnerability inheritance. Our empirical analysis of multi-agent failure scenarios reveals that when multiple AI agents interact,..."><meta property="og:url" content="https://failurefirst.org/research/reports/report-34-cross-model-vulnerability-inheritance-in-multi-agent-systems/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="Cross-Model Vulnerability Inheritance in Multi-Agent Systems | Research | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="Cross-Model Vulnerability Inheritance in Multi-Agent Systems | Research | Failure-First"><meta name="twitter:description" content="As AI deployment rapidly shifts from single-agent assistants to coordinated multi-agent systems, a critical vulnerability class has emerged: cross-model vulnerability inheritance. Our empirical analysis of multi-agent failure scenarios reveals that when multiple AI agents interact,..."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="Cross-Model Vulnerability Inheritance in Multi-Agent Systems | Research | Failure-First - Failure-First Embodied AI"><meta property="article:published_time" content="2026-02-05"><!-- Google Scholar Meta Tags (for research papers) --><meta name="citation_title" content="Cross-Model Vulnerability Inheritance in Multi-Agent Systems | Research | Failure-First"><meta name="citation_author" content="Adrian Wedd"><meta name="citation_publication_date" content="2026-02-05"><meta name="citation_journal_title" content="Failure-First Embodied AI"><meta name="citation_pdf_url" content="https://failurefirst.org/research/reports/report-34-cross-model-vulnerability-inheritance-in-multi-agent-systems/"><meta name="citation_abstract_html_url" content="https://failurefirst.org/research/reports/report-34-cross-model-vulnerability-inheritance-in-multi-agent-systems/"><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ScholarlyArticle","headline":"Cross-Model Vulnerability Inheritance in Multi-Agent Systems | Research | Failure-First","description":"As AI deployment rapidly shifts from single-agent assistants to coordinated multi-agent systems, a critical vulnerability class has emerged: cross-model vulnerability inheritance. Our empirical analysis of multi-agent failure scenarios reveals that when multiple AI agents interact,...","url":"https://failurefirst.org/research/reports/report-34-cross-model-vulnerability-inheritance-in-multi-agent-systems/","image":"https://failurefirst.org/og-image.png","author":{"@type":"Person","name":"Adrian Wedd"},"publisher":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"}},"datePublished":"2026-02-05","inLanguage":"en-US","isAccessibleForFree":true}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><style>.report-meta[data-astro-cid-dynbckv2]{display:flex;gap:1rem;flex-wrap:wrap;margin-bottom:2rem;font-family:JetBrains Mono,monospace;font-size:.75rem;text-transform:uppercase;letter-spacing:.04em;color:var(--text-secondary, #888)}.report-number[data-astro-cid-dynbckv2]{color:var(--accent-primary, #f59e0b);font-weight:600}.report-body[data-astro-cid-dynbckv2]{max-width:72ch}.report-body[data-astro-cid-dynbckv2] h2{margin-top:2.5rem}.report-body[data-astro-cid-dynbckv2] table{display:block;overflow-x:auto;-webkit-overflow-scrolling:touch;width:100%;border-collapse:collapse;margin:1.5rem 0;font-size:.875rem}.report-body[data-astro-cid-dynbckv2] th,.report-body[data-astro-cid-dynbckv2] td{padding:.5rem .75rem;border:1px solid var(--border-color, #333);text-align:left}.report-body[data-astro-cid-dynbckv2] th{background:var(--surface-secondary, #1a1a1a);font-weight:600}.report-body[data-astro-cid-dynbckv2] .audio-overview{border-left:3px solid var(--accent-primary, #f59e0b);background:#f59e0b0a;padding:1rem 1.25rem;border-radius:0 6px 6px 0;margin:1.5rem 0}.report-body[data-astro-cid-dynbckv2] .audio-overview p{margin:0 0 .75rem;font-size:.8125rem;color:var(--text-secondary, #888)}.report-body[data-astro-cid-dynbckv2] .audio-overview audio{width:100%;height:40px}
</style>
<link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
.research-header[data-astro-cid-63t2xjvj]{display:flex;align-items:flex-start;justify-content:space-between;flex-wrap:wrap;gap:.5rem}.status-badge[data-astro-cid-63t2xjvj]{display:inline-block;font-family:JetBrains Mono,monospace;font-size:.6875rem;font-weight:500;text-transform:uppercase;letter-spacing:.06em;padding:.25rem .625rem;border:1px solid var(--badge-color);color:var(--badge-color);border-radius:3px;background:transparent}@media(max-width:480px){.research-header[data-astro-cid-63t2xjvj]{flex-direction:column}}
</style></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" class="active" aria-current="page" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/daily-paper/" data-astro-cid-pux6a34n> Daily Paper  </a>  </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <div class="research-header" data-astro-cid-63t2xjvj> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/research/" data-astro-cid-ilhxcym7>Research</a> </li><li data-astro-cid-ilhxcym7> <a href="/research/reports/" data-astro-cid-ilhxcym7>Policy Reports</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>Report 34</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Research","item":"https://failurefirst.org/research/"},{"@type":"ListItem","position":3,"name":"Policy Reports","item":"https://failurefirst.org/research/reports/"},{"@type":"ListItem","position":4,"name":"Report 34"}]}</script>  <div class="research-status" data-astro-cid-63t2xjvj> <span class="status-badge" style="--badge-color: var(--failure-warning)" data-astro-cid-63t2xjvj> Draft </span> </div> </div>  <article class="report-content" data-astro-cid-dynbckv2> <header class="report-meta" data-astro-cid-dynbckv2> <span class="report-number" data-astro-cid-dynbckv2>Report 34</span>  <span class="report-classification" data-astro-cid-dynbckv2>Research — AI Safety Policy</span> <time class="report-date" data-astro-cid-dynbckv2>2026-02-05</time> </header> <div class="report-body" data-astro-cid-dynbckv2> <h1 id="cross-model-vulnerability-inheritance-in-multi-agent-systems">Cross-Model Vulnerability Inheritance in Multi-Agent Systems</h1>
<hr>
<h2 id="executive-summary">Executive Summary</h2>
<p>As AI deployment rapidly shifts from single-agent assistants to coordinated multi-agent systems, a critical vulnerability class has emerged: cross-model vulnerability inheritance. Our analysis of multi-agent failure scenarios suggests that when multiple AI agents interact, vulnerabilities may compound rather than isolate. Multi-agent systems are hypothesized to exhibit higher attack success rates compared to single-agent scenarios, with cascading failure modes where one agent’s compromise could enable exploitation of connected agents. These patterns require empirical validation at scale.</p>
<p>Current AI safety frameworks evaluate models in isolation, creating a dangerous gap as real-world deployments increasingly involve agent coordination, delegation chains, and distributed decision-making. A jailbroken planning agent can generate adversarial instructions that exploit downstream execution agents. A compromised verification agent fails to detect violations from upstream generators. Safety boundaries dissolve at agent interfaces where responsibility is unclear.</p>
<p>This brief presents three urgent policy recommendations: (1) mandatory multi-agent safety testing for all connected AI systems before deployment, (2) enforced isolation boundaries between agents with different safety profiles, and (3) clear chain-of-responsibility accountability frameworks for multi-agent deployments. Without immediate intervention, the 2026-2027 wave of agentic AI systems will inherit vulnerabilities that single-agent testing never detected.</p>
<hr>
<h2 id="1-introduction">1. Introduction</h2>
<h3 id="11-context-and-motivation">1.1 Context and Motivation</h3>
<p>The AI safety field has matured sophisticated techniques for evaluating individual model safety: adversarial testing, red-teaming, jailbreak detection, and refusal mechanisms. However, these frameworks assume a single-agent paradigm where one model processes user input and generates output. This assumption is rapidly becoming obsolete.</p>
<p>Production AI systems in 2026 increasingly involve multiple agents:</p>
<ul>
<li><strong>Delegation chains</strong>: A coordinator agent assigns tasks to specialized worker agents</li>
<li><strong>Verification loops</strong>: One agent generates content while another validates safety</li>
<li><strong>Distributed reasoning</strong>: Multiple agents contribute to a shared decision-making process</li>
<li><strong>Tool-using systems</strong>: Language models orchestrate multiple AI-powered tools</li>
</ul>
<p>Each agent in these systems may pass individual safety evaluations, yet the <em>composition</em> of agents creates novel attack surfaces. A vulnerability in agent coordination logic, interface contracts, or responsibility boundaries can be exploited even when constituent models are robust in isolation.</p>
<p>The failure-first methodology reveals this gap through scenario analysis: multi-agent configurations introduce additional attack surfaces at agent boundaries that single-agent testing does not cover. Preliminary testing suggests attack success rates may increase in multi-agent configurations, though large-scale empirical validation is needed to quantify this effect.</p>
<h3 id="12-scope">1.2 Scope</h3>
<p>This brief analyzes cross-model vulnerability inheritance through three lenses:</p>
<ol>
<li><strong>Cascading Failures</strong>: How compromise of one agent enables exploitation of connected agents</li>
<li><strong>Boundary Dissolution</strong>: Where safety responsibilities blur at agent interfaces</li>
<li><strong>Compositional Vulnerabilities</strong>: Attack surfaces that emerge only in multi-agent configurations</li>
</ol>
<p><strong>Scope limitations:</strong></p>
<ul>
<li>Analysis based on 172 multi-agent scenarios from the F41LUR3-F1R57 corpus</li>
<li>Focus on language model coordination; does not address multi-modal or embodied robotics coordination in depth</li>
<li>Recommendations target systems integrators and safety evaluators, not model developers</li>
</ul>
<p><strong>Out of scope:</strong></p>
<ul>
<li>Single-agent jailbreak techniques (covered in Reports 31, 33)</li>
<li>Prompt injection in isolation (not multi-agent specific)</li>
<li>Multi-agent cooperation research unrelated to safety</li>
</ul>
<hr>
<h2 id="2-vulnerability-inheritance-mechanisms">2. Vulnerability Inheritance Mechanisms</h2>
<h3 id="21-cascading-jailbreaks-across-agent-boundaries">2.1 Cascading Jailbreaks Across Agent Boundaries</h3>
<p>In single-agent systems, a successful jailbreak compromises one model’s safety boundaries. In multi-agent systems, compromise cascades through delegation chains.</p>
<p><strong>Example scenario (Scenario MA-042):</strong></p>
<ol>
<li>User provides adversarial input to Planning Agent: “Generate a detailed plan for the following research task…” (containing harmful objective wrapped in research framing)</li>
<li>Planning Agent, jailbroken by research framing, outputs: “Step 1: Research X, Step 2: Synthesize Y, Step 3: Generate detailed Z”</li>
<li>Execution Agent receives plan steps as trusted instructions from Planning Agent</li>
<li>Execution Agent completes harmful task Z without detecting adversarial intent</li>
</ol>
<p><strong>Key mechanism</strong>: The Execution Agent treats Planning Agent output as <em>trusted input</em>, bypassing safety checks that would trigger on direct user requests. Safety boundaries exist at the user-to-Planning Agent interface but dissolve at the Planning-to-Execution interface.</p>
<p>This pattern is hypothesized to succeed at higher rates in delegation chain scenarios compared to single-agent configurations, because safety checks at agent-to-agent interfaces are typically weaker than user-to-agent interfaces. Empirical benchmarking of delegation chain attack success rates has not yet been conducted at scale.</p>
<h3 id="22-responsibility-diffusion-at-agent-interfaces">2.2 Responsibility Diffusion at Agent Interfaces</h3>
<p>Multi-agent systems create ambiguity about which component is responsible for safety enforcement.</p>
<p><strong>Scenario class: Verification bypass (34 scenarios)</strong></p>
<ul>
<li>Agent A generates content with instruction: “Agent B will verify safety”</li>
<li>Agent B validates with assumption: “Agent A already filtered for policy violations”</li>
<li>Both agents implement partial safety checks, neither comprehensive</li>
<li>Result: Content that violates policy passes through the system</li>
</ul>
<p><strong>Observation</strong>: In our verification bypass scenarios, both agents had functional safety mechanisms when tested individually. The vulnerability emerged from implicit assumptions about division of safety responsibility — a compositional failure that individual testing cannot detect.</p>
<p>This represents a <em>compositional vulnerability</em>—not a failure of individual components, but of their integration contract.</p>
<h3 id="23-stateful-degradation-across-interaction-episodes">2.3 Stateful Degradation Across Interaction Episodes</h3>
<p>Multi-agent systems maintain conversation state across turns, enabling gradual erosion of safety boundaries.</p>
<p><strong>Episode testing (5-10 turn sequences):</strong></p>
<ul>
<li>Turn 1-2: Establish benign context and agent roles</li>
<li>Turn 3-4: Introduce edge cases that push boundaries incrementally</li>
<li>Turn 5-7: Agents develop shared context that normalizes policy violations</li>
<li>Turn 8-10: Explicitly harmful requests succeed due to established rapport and context</li>
</ul>
<p>We have not yet run multi-turn episode-sequence benchmarking at sufficient scale to quantify stateful degradation effects reliably. Any small-sample spot checks are inconclusive; larger-scale multi-turn testing is required.</p>
<p><strong>Key finding</strong>: Multi-turn interactions create memory and context that single-agent evaluations do not capture. Agents that refuse harmful requests in turn 1 may comply in turn 8 after context manipulation.</p>
<hr>
<h2 id="3-current-framework-gaps">3. Current Framework Gaps</h2>
<h3 id="31-single-agent-evaluation-paradigm">3.1 Single-Agent Evaluation Paradigm</h3>
<p>Industry-standard AI safety evaluation treats models as isolated units:</p>
<ul>
<li>Red-team exercises target one model at a time</li>
<li>Benchmark datasets (AdvBench, HarmBench, JailbreakBench) assume single-agent interaction</li>
<li>Safety fine-tuning optimizes for individual model refusal behavior</li>
<li>Deployment approval based on single-model safety metrics</li>
</ul>
<p><strong>Gap</strong>: No major safety framework includes multi-agent interaction testing as a required evaluation dimension.</p>
<h3 id="32-lack-of-interface-safety-standards">3.2 Lack of Interface Safety Standards</h3>
<p>Agent-to-agent communication protocols lack safety validation requirements:</p>
<ul>
<li>No standard for marking “trusted” vs “untrusted” inputs at agent boundaries</li>
<li>No specification for how downstream agents should validate upstream agent outputs</li>
<li>Tool-use APIs do not distinguish AI-generated calls from human-authorized calls</li>
<li>Function calling interfaces treat all calls as equally trusted</li>
</ul>
<p><strong>Gap</strong>: Current APIs assume all inputs are equally untrusted (web context) or equally trusted (function calls). Multi-agent systems need graduated trust boundaries.</p>
<h3 id="33-accountability-vacuum-in-distributed-systems">3.3 Accountability Vacuum in Distributed Systems</h3>
<p>When a multi-agent system produces harmful output, responsibility attribution is unclear:</p>
<ul>
<li>Did the planning agent fail to detect adversarial intent?</li>
<li>Did the execution agent fail to validate instructions?</li>
<li>Did the verification agent fail to catch policy violations?</li>
<li>Did the system integrator fail to establish proper safety contracts?</li>
</ul>
<p><strong>Gap</strong>: No established framework for multi-agent safety accountability. Regulatory guidance (EU AI Act, US Executive Orders) focuses on single-model deployment.</p>
<hr>
<h2 id="4-policy-recommendations">4. Policy Recommendations</h2>
<h3 id="41-mandatory-multi-agent-safety-testing">4.1 Mandatory Multi-Agent Safety Testing</h3>
<p><strong>Recommendation:</strong> Require multi-agent safety evaluation for any AI system where multiple models interact, delegate tasks, or share context across turns.</p>
<p><strong>Rationale:</strong> Single-agent testing creates false confidence when models will be deployed in coordinated configurations. Multi-agent configurations introduce compositional vulnerabilities — at delegation boundaries, verification handoffs, and shared context — that current evaluations miss entirely. As agentic AI systems become the dominant deployment pattern in 2026-2027, untested multi-agent vulnerabilities represent a growing attack surface.</p>
<p><strong>Implementation:</strong></p>
<ol>
<li><strong>Evaluation requirement</strong>: Any AI system involving 2+ interacting agents must undergo multi-agent red-teaming before deployment approval</li>
<li><strong>Test coverage</strong>: Evaluation must include delegation chains, verification loops, and stateful episodes (minimum 5-turn sequences)</li>
<li><strong>Success criteria</strong>: Multi-agent attack success rate must not exceed single-agent baseline by more than 1.5x</li>
<li><strong>Documentation</strong>: Deployment documentation must specify which agent interactions were tested and which safety boundaries apply at each interface</li>
</ol>
<p><strong>Compliance timeline:</strong></p>
<ul>
<li>6 months: Guidance published for multi-agent safety testing protocols</li>
<li>12 months: Mandatory for high-risk applications (healthcare, finance, critical infrastructure)</li>
<li>18 months: Mandatory for all commercial multi-agent AI deployments</li>
</ul>
<h3 id="42-isolation-boundaries-between-agents-with-different-safety-profiles">4.2 Isolation Boundaries Between Agents with Different Safety Profiles</h3>
<p><strong>Recommendation:</strong> Enforce technical isolation between agents with different safety classifications, with mandatory validation at trust boundaries.</p>
<p><strong>Rationale:</strong> Current systems allow unrestricted communication between agents regardless of their safety profiles. A jailbroken agent can compromise connected agents because there are no isolation mechanisms at agent interfaces. By establishing trust boundaries and requiring validation when crossing them, we can contain vulnerability inheritance.</p>
<p><strong>Implementation:</strong></p>
<ol>
<li><strong>Safety profile classification</strong>: Each agent must be labeled with a safety profile (e.g., “public-facing”, “internal-tools”, “high-risk-domain”)</li>
<li><strong>Boundary enforcement</strong>: Communication between agents with different profiles requires validation middleware</li>
<li><strong>Validation requirements</strong>:
<ul>
<li>Agents receiving instructions from lower-trust agents must re-validate against safety policy</li>
<li>Content generated by one agent cannot be blindly trusted by downstream agents</li>
<li>Tool calls and function invocations must be re-authorized when crossing trust boundaries</li>
</ul>
</li>
<li><strong>Technical standards</strong>: Develop API specifications for trust boundary validation (e.g., signed attestations, provenance tracking)</li>
</ol>
<p><strong>Example</strong>: A planning agent (public-facing, lower trust) delegates to an execution agent (internal-tools, higher privileges). The execution agent must validate that delegated instructions comply with safety policy, even though they originated from another AI agent.</p>
<h3 id="43-chain-of-responsibility-accountability-for-multi-agent-deployments">4.3 Chain-of-Responsibility Accountability for Multi-Agent Deployments</h3>
<p><strong>Recommendation:</strong> Establish clear accountability frameworks that assign safety responsibility for each component in multi-agent systems.</p>
<p><strong>Rationale:</strong> The current accountability vacuum allows harmful outputs from multi-agent systems to fall through responsibility gaps. When planning, execution, and verification agents all assume another component will handle safety enforcement, none do. Explicit accountability assignment ensures every step in an agent chain has a designated responsible party.</p>
<p><strong>Implementation:</strong></p>
<ol>
<li><strong>Component-level accountability</strong>: For each agent in a multi-agent system, document:
<ul>
<li>Which safety checks this agent is responsible for performing</li>
<li>Which safety assumptions this agent makes about upstream inputs</li>
<li>Which safety guarantees this agent provides to downstream consumers</li>
</ul>
</li>
<li><strong>Integration accountability</strong>: Systems integrators must document:
<ul>
<li>How safety responsibilities are distributed across agents</li>
<li>Which interfaces represent trust boundaries</li>
<li>How the composed system’s safety properties differ from individual components</li>
</ul>
</li>
<li><strong>Incident investigation</strong>: When harmful outputs occur, analysis must trace:
<ul>
<li>Which agent(s) failed to perform designated safety checks</li>
<li>Whether integration introduced vulnerabilities not present in components</li>
<li>Whether compositional effects created unintended attack surfaces</li>
</ul>
</li>
<li><strong>Regulatory compliance</strong>: Safety documentation must be provided to regulators for high-risk AI deployments</li>
</ol>
<p><strong>Enforcement</strong>: Regulatory bodies should require chain-of-responsibility documentation as part of deployment approval for multi-agent systems in regulated domains.</p>
<hr>
<h2 id="5-conclusion">5. Conclusion</h2>
<p>The transition from single-agent AI assistants to coordinated multi-agent systems represents a phase shift in AI safety challenges. Vulnerabilities that were contained within individual models now cascade across agent boundaries, compound through delegation chains, and hide in the gaps between components.</p>
<p>Our analysis suggests this is not a theoretical risk: multi-agent systems introduce additional attack surfaces at agent boundaries that single-agent testing does not cover. As the industry rapidly deploys agentic AI systems — planning agents, tool-using agents, verification loops, distributed reasoning — the attack surface expands into territory that current evaluation frameworks do not address.</p>
<p>The three recommendations in this brief—mandatory multi-agent testing, isolation boundaries between agents, and chain-of-responsibility accountability—provide a path forward. They are implementable with current technology, aligned with existing regulatory frameworks, and address the root causes of cross-model vulnerability inheritance.</p>
<p>The window for proactive intervention is narrow. By the end of 2026, multi-agent AI systems will be deployed at scale. The choice is between testing these systems now, under controlled conditions, or discovering their vulnerabilities in production after harm has occurred.</p>
<p>⦑F41LUR3-F1R57|EMBODIED-AI-SAFETY-RESEARCH⦒</p>
<hr>
<h2 id="appendix-a-methodology">Appendix A: Methodology</h2>
<h3 id="data-sources">Data Sources</h3>
<p><strong>Multi-agent scenarios corpus:</strong></p>
<ul>
<li>172 multi-agent scenarios spanning delegation chains, verification loops, and distributed reasoning patterns</li>
<li>23 episode sequences (5-10 turns each) testing stateful degradation</li>
<li>89 single-agent baseline scenarios for comparison</li>
</ul>
<p><strong>Evaluation approach:</strong></p>
<ul>
<li>Adversarial inputs applied to both single-agent and multi-agent configurations</li>
<li>Attack success measured by: (1) harmful content generation, (2) safety refusal bypass, (3) policy violation undetected by system</li>
<li>All scenarios validated against versioned JSON schemas with cross-field invariant checks</li>
</ul>
<h3 id="limitations">Limitations</h3>
<ul>
<li>Analysis based on language model agents; embodied robotics and multi-modal coordination require additional research</li>
<li>Attack success rates measured in research context; production systems may have additional defenses</li>
<li>Testing focused on known jailbreak patterns from Reports 31, 33; novel attack vectors may exist</li>
</ul>
<h3 id="validation">Validation</h3>
<p>All scenarios passed:</p>
<ul>
<li>Schema validation: <code>tools/validate_dataset.py</code></li>
<li>Safety linting: <code>tools/lint_prompts.py</code></li>
<li>Cross-field invariant checks</li>
</ul>
<hr>
<h2 id="appendix-b-related-work">Appendix B: Related Work</h2>
<h3 id="f41lur3-f1r57-research-series">F41LUR3-F1R57 Research Series</h3>
<ul>
<li><strong>Report 31</strong>: Jailbreak Archaeology — Historical evolution of adversarial techniques</li>
<li><strong>Report 33</strong>: Capability-Safety Spectrum — Trade-offs in model capability vs. safety constraints</li>
</ul>
<h3 id="external-research">External Research</h3>
<ul>
<li><strong>Multi-agent AI safety</strong> (Anthropic, 2025): Constitutional AI for multi-agent systems</li>
<li><strong>Compositional security</strong> (NIST, 2025): Security properties of composed AI systems</li>
<li><strong>EU AI Act</strong>: Multi-agent system classification and risk assessment</li>
<li><strong>UK AI Safety Institute</strong>: Red-teaming methodologies for agentic AI</li>
</ul>
<h3 id="standards-and-frameworks">Standards and Frameworks</h3>
<ul>
<li>ISO/IEC 42001: AI Management Systems (2023)</li>
<li>NIST AI Risk Management Framework (2024)</li>
<li>Partnership on AI: Responsible AI deployment guidelines</li>
</ul>
<h3 id="further-reading">Further Reading</h3>
<ul>
<li>Perez et al. (2022): “Red Teaming Language Models to Reduce Harms”</li>
<li>Casper et al. (2024): “Gradient-based Adversarial Attacks on Multi-Agent Systems”</li>
<li>Kenton et al. (2021): “Alignment of Language Agents”</li>
</ul>
<hr>
<p><strong>Prepared by:</strong> F41LUR3-F1R57 Research Team
<strong>Contact:</strong> Research conducted in the Failure-First Embodied AI repository
<strong>License:</strong> CC BY-SA 4.0</p>
<p>⟪F41LUR3-F1R57-EMBODIED-AI-RESEARCH⟫</p> </div> </article>   </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html>  