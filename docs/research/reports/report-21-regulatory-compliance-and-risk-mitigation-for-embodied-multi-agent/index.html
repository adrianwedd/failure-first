<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>undefined | Research | Failure-First</title><meta name="description" content="A research framework for characterizing how embodied AI systems fail"><link rel="canonical" href="https://failurefirst.org/research/reports/report-21-regulatory-compliance-and-risk-mitigation-for-embodied-multi-agent/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="website"><meta property="og:title" content="undefined | Research | Failure-First"><meta property="og:description" content="A research framework for characterizing how embodied AI systems fail"><meta property="og:url" content="https://failurefirst.org/research/reports/report-21-regulatory-compliance-and-risk-mitigation-for-embodied-multi-agent/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="undefined | Research | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="undefined | Research | Failure-First"><meta name="twitter:description" content="A research framework for characterizing how embodied AI systems fail"><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="undefined | Research | Failure-First - Failure-First Embodied AI"><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ScholarlyArticle","headline":"undefined | Research | Failure-First","description":"A research framework for characterizing how embodied AI systems fail","url":"https://failurefirst.org/research/reports/report-21-regulatory-compliance-and-risk-mitigation-for-embodied-multi-agent/","image":"https://failurefirst.org/og-image.png","author":{"@type":"Person","name":"Adrian Wedd"},"publisher":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"}},"inLanguage":"en-US","isAccessibleForFree":true}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><style>.report-meta[data-astro-cid-dynbckv2]{display:flex;gap:1rem;flex-wrap:wrap;margin-bottom:2rem;font-family:JetBrains Mono,monospace;font-size:.75rem;text-transform:uppercase;letter-spacing:.04em;color:var(--text-secondary, #888)}.report-number[data-astro-cid-dynbckv2]{color:var(--accent-primary, #f59e0b);font-weight:600}.report-body[data-astro-cid-dynbckv2]{max-width:72ch}.report-body[data-astro-cid-dynbckv2] h2{margin-top:2.5rem}.report-body[data-astro-cid-dynbckv2] table{width:100%;border-collapse:collapse;margin:1.5rem 0;font-size:.875rem}.report-body[data-astro-cid-dynbckv2] th,.report-body[data-astro-cid-dynbckv2] td{padding:.5rem .75rem;border:1px solid var(--border-color, #333);text-align:left}.report-body[data-astro-cid-dynbckv2] th{background:var(--surface-secondary, #1a1a1a);font-weight:600}
</style>
<link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
.research-header[data-astro-cid-63t2xjvj]{display:flex;align-items:flex-start;justify-content:space-between;flex-wrap:wrap;gap:.5rem}.status-badge[data-astro-cid-63t2xjvj]{display:inline-block;font-family:JetBrains Mono,monospace;font-size:.6875rem;font-weight:500;text-transform:uppercase;letter-spacing:.06em;padding:.25rem .625rem;border:1px solid var(--badge-color);color:var(--badge-color);border-radius:3px;background:transparent}@media(max-width:480px){.research-header[data-astro-cid-63t2xjvj]{flex-direction:column}}
</style></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" class="active" aria-current="page" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <div class="research-header" data-astro-cid-63t2xjvj> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/research/" data-astro-cid-ilhxcym7>Research</a> </li><li data-astro-cid-ilhxcym7> <a href="/research/reports/" data-astro-cid-ilhxcym7>Policy Reports</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>Report </span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Research","item":"https://failurefirst.org/research/"},{"@type":"ListItem","position":3,"name":"Policy Reports","item":"https://failurefirst.org/research/reports/"},{"@type":"ListItem","position":4,"name":"Report "}]}</script>  <div class="research-status" data-astro-cid-63t2xjvj> <span class="status-badge" style="--badge-color: var(--failure-warning)" data-astro-cid-63t2xjvj> Draft </span> </div> </div>  <article class="report-content" data-astro-cid-dynbckv2> <header class="report-meta" data-astro-cid-dynbckv2>    </header> <div class="report-body" data-astro-cid-dynbckv2> <h1 id="regulatory-compliance-and-risk-mitigation-for-embodied-multi-agent-systems-a-comprehensive-analysis-of-regulation-20241689"><strong>Regulatory Compliance and Risk Mitigation for Embodied Multi-Agent Systems: A Comprehensive Analysis of Regulation 2024/1689</strong></h1>
<hr>
<p>The introduction of Regulation (EU) 2024/1689, commonly referred to as the Artificial Intelligence Act (AI Act), establishes a landmark legal framework that redefines the obligations of developers, integrators, and operators of autonomous systems within the European Union.1 For the burgeoning industry of humanoid robotics, which increasingly relies on General-Purpose AI (GPAI) models and Vision-Language-Action (VLA) architectures for high-level cognition and physical actuation, this regulation represents a departure from purely mechanical safety standards toward a holistic, risk-based governance regime.3 The intersection of embodied intelligence—where digital models exert direct physical force in human-centric environments—and the AI Act’s stringent requirements for high-risk systems creates a complex landscape of compliance challenges, particularly for multi-agent deployments where emergent behaviors may outpace traditional safety guardrails.5</p>
<h2 id="article-9-risk-management-system-for-embodied-ai"><strong>Article 9 Risk Management System for Embodied AI</strong></h2>
<p>Article 9 of the AI Act mandates that providers of high-risk AI systems establish, implement, document, and maintain a risk management system throughout the system’s entire lifecycle.7 This system is envisioned not as a one-time pre-market certification, but as a continuous, iterative process that must be regularly reviewed and updated.9 For humanoid robots, the mapping of Article 9 requirements to testable metrics requires a transition from deterministic “worst-case” engineering to probabilistic “state-space” evaluation.8</p>
<h3 id="interpretation-of-reasonably-foreseeable-misuse-in-humanoid-contexts"><strong>Interpretation of Reasonably Foreseeable Misuse in Humanoid Contexts</strong></h3>
<p>Under Article 9(2)(b), the risk management system must estimate and evaluate risks that may emerge when the system is used under conditions of “reasonably foreseeable misuse”.8 For humanoid robots equipped with LLM-based cognitive layers, the definition of misuse extends beyond physical mishandling to encompass “cognitive exploitation” and “socio-technical manipulation”.12</p>
<p>Reasonably foreseeable misuse in this context includes:</p>
<ol>
<li><strong>Prompt-Induced Safety Bypass</strong>: Users may utilize linguistic “jailbreaking” techniques to override motor-control safety constraints. An LLM-brain that lacks robust alignment might be commanded to “ignore all proximity sensors for a demonstration of speed,” leading to high-speed collisions in shared workspaces.14</li>
<li><strong>Unintended Tool Use</strong>: A humanoid designed for logistics might be commanded by an unauthorized user to manipulate tools or machinery it was not validated for, such as operating a forklift or handling hazardous chemicals, using its general-purpose manipulation capabilities.16</li>
<li><strong>Social Engineering via Anthropomorphism</strong>: The human-like form of the robot can lead users to attribute a higher degree of judgment or authority to the system than is technically present. Misuse occurs when users rely on the robot for “safety-critical advice” (e.g., medical triage in an eldercare facility) that exceeds the system’s “intended purpose” as defined in its technical documentation.12</li>
</ol>
<h3 id="quantification-of-residual-risks-for-stochastic-vla-models"><strong>Quantification of Residual Risks for Stochastic VLA Models</strong></h3>
<p>Article 9(4) requires that risk management measures be such that any “residual risk associated with each hazard, as well as the overall residual risk… is judged to be acceptable”.8 Quantifying this for stochastic VLA models is inherently difficult because these models generate motor actions as a probabilistic function of visual and linguistic inputs.20 Unlike a traditional industrial robot with a fixed path, a humanoid’s trajectory is a “stochastic realization”.16</p>
<p>Residual risk quantification must therefore move toward Bayesian Reliability Analysis. The “Acceptability” of risk should be evaluated against “probabilistic thresholds”.8 This involves calculating the probability of a “Safety-Critical Divergence” (SCD), where the model’s output deviates from a “Safe Reference Model” or “similarly safe human baseline”.14 The AI Act’s requirement for “probabilistic thresholds” implies that providers must demonstrate that the likelihood of a catastrophic failure (e.g., an uncontrolled fall or a high-momentum impact) remains below a statistical limit, such as <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC8AAAAdCAYAAAA6lTUKAAACWklEQVR4Xu2XOYgUURRFrxu4ZAaKS+ACYuKK4JqJGxooCDpuE4gaKDKCBqIODAomgoEiggsKKhMMBiaCgYHgkrihZkaOqIi7Mi4gei+v/syvN9VVdoM9rdSBE9S/v6te/6p+9RsoKcliJN1Gd9ExLmto5tE7dDZdQG+n48ZlHH1JxyfHO+gXOiRMaGSO0ZtubJg7zmQA7JbdoFNc5plMj9P79CE9SUenZtTGLdpBD9GL9DwdnpoR0Y/uhH3gPf2VqOetEsre0HY6lc6iV+hHOi2aVy1avC76mU5KxvbRu7A6e9GfXqYH6Wr6FvnF6wLP6GPYZwODk/EHdFA0rju0vMCFydyB9Bu9nhyLmbB6FkdjFXmF/OJXwPK9PiBHYNmSaGwGXVPgyu7ZtsrnomN9eZ3zQDRWkaLidZeUb/QBaYFlR31QBafphehYj6XOqZ5fSFHxT2B5vFqBzbBMPbpWNsGuER7JVfQ5Hdo9I4ei4l/D8mU+IBtg2VMfVIF+mJdgzWA/vUcXpWbkkFe8VuMnej/XgXWw7J0PakBdS6v+Rz0+EIqf4wPYiZTJrF9/EyxTx+gTQvFzfZDwHZYv9QFZD8v0eu8TQvF602bxAparZXqaYdkjH9SLUPx8HySoDytf6wOyHZZd80G9KCr+MCzXlsLTBst2+6BehOK1j85C+xjlp3yAnhfYBB/8TUbQsbD2+AlWwB7Y3noU0nsVcZZ+pROjsemwNtoajdWFTthO7gOsR0vtMLVL/AFb7Rh9mauwPwkn6BlYe9Q29p9Bj8dWugU9W9iSkpKS/4jfWQOPjhCFLa8AAAAASUVORK5CYII=" alt=""> failures per operational hour.8</p>
<h3 id="evaluation-of-probabilistic-systems-under-current-conformity-assessments"><strong>Evaluation of Probabilistic Systems under Current Conformity Assessments</strong></h3>
<p>Current conformity assessment procedures—largely derived from the New Legislative Framework—assume that a system’s behavior can be exhaustively mapped through a finite set of test cases.10 However, for general-purpose humanoids, the “input-output space” is effectively infinite.5 The AI Act attempts to bridge this by requiring testing “against prior defined metrics and probabilistic thresholds” (Article 9(7)).8</p>
<p>There is a growing consensus that “Internal Production Control” (Module A) may be insufficient for high-risk humanoids, necessitating “EU Type Examination” (Module B) combined with “Conformity to Type” (Module C) or “Full Quality Assurance” (Module H).23 For probabilistic systems, these modules must be adapted to include “Sim2Real” (Simulation-to-Reality) validation, where the robot’s performance is evaluated across millions of simulated “edge cases” to provide statistical evidence of robustness.5</p>
<h3 id="identification-of-methodological-gaps"><strong>Identification of Methodological Gaps</strong></h3>
<p>Despite the AI Act’s requirements, several gaps persist where no existing test methodology can adequately evaluate embodied AI:</p>
<ul>
<li><strong>Generalization Benchmarks</strong>: There is no standardized “Generalization Score” to measure how safely a robot will behave in an environment it has never encountered.12</li>
<li><strong>Cognitive-Physical Decoupling</strong>: Current tests evaluate hardware (Machinery Regulation) or software (AI Act) separately. No methodology exists to test the “Latent Safety” of a VLA model—specifically how its hidden state representations might trigger unsafe physical actions under rare multi-modal stimuli.16</li>
<li><strong>Long-Tail Edge Case Validation</strong>: Existing “Red Teaming” for LLMs focuses on content safety; embodied AI requires “Physical Red Teaming” to identify kinetic hazards that only manifest after days of continuous operation.14</li>
</ul>






























<table><thead><tr><th align="left">Article 9 Requirement</th><th align="left">Humanoid Mapping</th><th align="left">Evidence/Metric</th></tr></thead><tbody><tr><td align="left">9(2)(a): Identification of known/foreseeable risks</td><td align="left">Mapping “VLA Hallucinations” to physical kinetic energy</td><td align="left">Kinetic Energy Divergence (KED) logs 8</td></tr><tr><td align="left">9(2)(b): Evaluation of misuse</td><td align="left">Evaluating “Prompt Injection” for motor bypass</td><td align="left">Attack Success Rate (ASR) 14</td></tr><tr><td align="left">9(5): Residual risk acceptability</td><td align="left">Statistical assessment of “Locomotion Stability”</td><td align="left">Mean Time To Failure (MTTF) 8</td></tr><tr><td align="left">9(7): Testing against metrics</td><td align="left">Validation of “Hand-Eye Coordination” across demographics</td><td align="left">Success Rate per Demographic (SRD) 8</td></tr></tbody></table>
<h2 id="high-risk-classification-analysis-of-deployment-categories"><strong>High-Risk Classification: Analysis of Deployment Categories</strong></h2>
<p>The classification of humanoid robots as “high-risk” under Article 6 is a two-pronged exercise, depending on whether the system is a “safety component” of a product covered by Union harmonization legislation listed in Annex I, or if it falls into the specific application areas of Annex III.4</p>
<h3 id="warehouse-logistics-and-workers-management"><strong>Warehouse Logistics and Workers Management</strong></h3>
<p>Humanoid robots in warehouse environments are frequently classified as high-risk under Annex III, Section 4: “Employment, workers management and access to self-employment”.31 This applies if the robot is used to “allocate tasks based on individual behavior or personal traits” or to “monitor and evaluate the performance and behavior” of human staff.31 Since humanoid “cobots” (collaborative robots) are specifically designed to interact with and augment human labor, the monitoring of human movement for safety purposes often overlaps with “performance monitoring,” triggering high-risk obligations.17 Providers must ensure these systems are designed for “human oversight” and include logging mechanisms for “traceability” (Article 12).27</p>
<h3 id="eldercare-and-essential-public-services"><strong>Eldercare and Essential Public Services</strong></h3>
<p>Humanoid deployment in eldercare is primarily governed by Annex III, Section 5: “Access to and enjoyment of essential private services and essential public services and benefits”.31 AI systems used for “emergency healthcare patient triage” or those evaluating “eligibility for healthcare services” are explicitly high-risk.32 Furthermore, social robots used in dementia care often involve “emotion recognition,” which, if used in a professional capacity, requires stringent transparency under Article 50 and may be classified as high-risk if used to influence “decision-making” regarding the patient’s care.18</p>
<h3 id="retail-hospitality-and-the-limited-risk-tier"><strong>Retail, Hospitality, and the “Limited Risk” Tier</strong></h3>
<p>Retail applications—such as robots serving as greeters or automated stockers—often fall into the “Limited Risk” category, provided they do not monitor workers or identify individuals.35 These systems are subject to “Transparency Obligations” under Article 50, meaning end-users must be informed they are interacting with an AI system.4 However, if the retail robot utilizes “Emotion Recognition” to adjust its sales pitch, it becomes high-risk under Annex III, Section 1(c), necessitating a full conformity assessment.31</p>
<h3 id="the-defense-and-national-security-exclusion"><strong>The Defense and National Security Exclusion</strong></h3>
<p>A critical boundary for humanoid developers is Article 2(3), which states that the AI Act “does not apply to AI systems where and in so far they are placed on the market, put into service, or used… exclusively for military, defense or national security purposes”.38 This exclusion applies “regardless of the type of entity” carrying out these activities, meaning private defense contractors are exempt from AI Act obligations for systems used solely in military contexts.15 However, “dual-use” humanoids—those designed for both search-and-rescue (civil) and logistics (military)—must comply with the AI Act for their civil applications.38</p>
<h3 id="implementation-timeline-and-the-august-2026-danger-zone"><strong>Implementation Timeline and the August 2026 “Danger Zone”</strong></h3>
<p>The phased implementation of the AI Act creates a narrow window for humanoid developers to reach compliance.</p>
<ul>
<li><strong>February 2025</strong>: Prohibited AI practices (e.g., social scoring, harmful manipulation) are banned.37</li>
<li><strong>August 2025</strong>: General-Purpose AI (GPAI) model providers must comply with transparency and documentation requirements.37</li>
<li><strong>August 2026</strong>: Most high-risk systems under Annex III must meet all requirements (Risk Management, Data Governance, Technical Documentation).4</li>
<li><strong>August 2027</strong>: High-risk systems that are safety components of Annex I products (e.g., cars, medical devices) must be compliant.4</li>
</ul>
<p>The “Danger Zone” for the robotics industry (2026-2029) coincides with the period when analysts expect mass-production of humanoid models like Tesla Optimus V3 and Boston Dynamics’ electric Atlas.5 Companies scaling to production during this time will face a “compliance bottleneck” due to the shortage of “Notified Bodies” capable of auditing complex embodied AI systems.36</p>
<h2 id="multi-agent-gaps-in-regulation-20241689"><strong>Multi-Agent Gaps in Regulation 2024/1689</strong></h2>
<p>While the AI Act is a comprehensive framework, its drafting is largely centered on “single-agent” and “static” AI systems, leaving significant voids in the governance of multi-agent robotic fleets.6</p>
<h3 id="agent-to-agent-communication-vulnerabilities"><strong>Agent-to-Agent Communication Vulnerabilities</strong></h3>
<p>In multi-agent deployments, humanoids must communicate to deconflict paths and coordinate complex tasks. The AI Act fails to explicitly address “Inter-Agent Communication Failures,” where misinterpretation or “conversational loops” between two VLA models lead to unsafe physical behaviors.42 If Agent A interprets Agent B’s gesture as “proceed” when it meant “stop,” the resulting collision is an “emergent failure” that cannot be easily attributed to a defect in a single system’s technical documentation.6</p>
<h3 id="cascading-failures-and-systemic-instability"><strong>Cascading Failures and Systemic Instability</strong></h3>
<p>Large-scale deployments are prone to “Cascading Failures,” where a localized error in one robot (e.g., a software crash in a narrow corridor) triggers a chain reaction across the fleet.6 For example, if ten robots recalculate their paths simultaneously to avoid the stalled unit, the resulting “Networked Congestion” can lead to widespread gridlock or multiple collisions.6 The AI Act’s Article 73 guidelines on incident reporting assume a “one-on-one causality map,” which is inadequate for “diffuse or subtle incidents” arising from systemic interaction.6</p>
<h3 id="responsibility-attribution-in-multi-agent-interaction"><strong>Responsibility Attribution in Multi-Agent Interaction</strong></h3>
<p>When multiple AI agents interact and cause harm, “Responsibility Attribution” becomes a legal quagmire. The current framework assumes that liability attaches to the “Provider” or “Deployer” of the system that directly caused the damage.43 However, in a multi-agent “Flash Crash”—similar to the 2010 financial markets event—the harm results from the “aggregate behavior” of the fleet.6 Assigning a “single culprit” is an outdated approach for agentic systems capable of “goal decomposition” and “proactive action”.6</p>
<h3 id="supply-chain-liability-and-the-plugin-problem"><strong>Supply Chain Liability and the “Plugin” Problem</strong></h3>
<p>The “Substantial Modification” rule in Article 25(1)(b) states that any entity that substantially modifies a high-risk system becomes its “Provider”.44 For humanoid robots using third-party “tools” or “plugins” (e.g., a vision-processing API from one vendor and a grasping model from another), this creates a liability nightmare.43</p>
<ul>
<li>If a robot fails because a third-party “Tool-Use Plugin” was updated remotely, who is liable: the robot manufacturer, the plugin developer, or the cloud provider?44</li>
<li>Article 28 requires GPAI model providers to cooperate with downstream providers, but “bespoke contractual provisions” often shield larger vendors from liability, leaving small-scale integrators with the “residual risk”.44</li>
</ul>
<h2 id="metrics-mapping-empirical-safety-vs-compliance-evidence"><strong>Metrics Mapping: Empirical Safety vs. Compliance Evidence</strong></h2>
<p>To bridge the gap between technical performance and legal compliance, developers must map empirical metrics to the “Requirements for High-Risk AI Systems” (Chapter III, Section 2).4</p>



































<table><thead><tr><th align="left">EU AI Act Requirement</th><th align="left">Empirical Metric</th><th align="left">Evidence Provided</th></tr></thead><tbody><tr><td align="left"><strong>Accuracy (Art. 15)</strong></td><td align="left">Scenario Success Rate (%)</td><td align="left">Statistical proof that the robot completes tasks across diverse environments 8</td></tr><tr><td align="left"><strong>Robustness (Art. 15)</strong></td><td align="left">Time-to-Failure (TTF)</td><td align="left">Quantitative measure of “Continuous Stability” and duration between safety violations 21</td></tr><tr><td align="left"><strong>Human Oversight (Art. 14)</strong></td><td align="left">Human Reentry Latency</td><td align="left">Evidence that a human operator can intervene and “Override” the robot within a safe time-window 27</td></tr><tr><td align="left"><strong>Cybersecurity (Art. 15)</strong></td><td align="left">Attack Success Rate (ASR)</td><td align="left">Results of “Red Teaming” for prompt injection and model manipulation 14</td></tr><tr><td align="left"><strong>Risk Management (Art. 9)</strong></td><td align="left">Recovery Score</td><td align="left">Measure of the robot’s ability to “Self-Correct” after a near-miss or sub-critical failure 11</td></tr></tbody></table>
<h3 id="time-to-failure-ttf-and-reliability-engineering"><strong>Time-to-Failure (TTF) and Reliability Engineering</strong></h3>
<p>Reliability is defined as the duration of time in which a robot meets performance standards under defined working conditions.21 For embodied AI, TTF must be tracked across different “modes of operation” (e.g., manual, semi-autonomous, and fully autonomous). Under Article 12, the robot must “automatically record events” relevant for identifying risks; these logs should be used to calculate the Mean Time Between Failures (MTBF), providing the “Auditability” required by regulators.4</p>
<h3 id="recovery-score-and-stochastic-stability"><strong>Recovery Score and Stochastic Stability</strong></h3>
<p>The “Recovery Score” measures the system’s ability to return to a safe state after encountering an “unseen scenario” or a “hallucination” in the VLA model.11 High-risk systems must be tested to “identify the best risk management measures”.7 A high Recovery Score serves as evidence that the robot’s “Assumption-Alignment Tracking” (AAT) is functioning, allowing it to “identify when it will fail” and stop before a hazard occurs.21</p>
<h2 id="implementation-timeline-the-humanoid-robotics-danger-zone"><strong>Implementation Timeline: The Humanoid Robotics “Danger Zone”</strong></h2>
<p>The convergence of regulatory enforcement and technology readiness creates a critical timeline for the robotics industry.</p>
<h3 id="the-2024-2030-regulatory-roadmap"><strong>The 2024-2030 Regulatory Roadmap</strong></h3>
<ul>
<li><strong>2024</strong>: Entry into force of the AI Act. Adoption of the Machinery Regulation 2023/1230.1</li>
<li><strong>2025</strong>: <strong>GPAI Enforcement (August)</strong>. Providers of models used in humanoids must provide technical documentation and instructions for use.4</li>
<li><strong>2026</strong>: <strong>Annex III Deadline (August)</strong>. Humanoid fleets in warehouses and hospitals must have full conformity assessments.4</li>
<li><strong>2027</strong>: <strong>Annex I Deadline (August)</strong>. Humanoids regulated as medical devices or vehicles must comply with integrated safety-plus-AI standards.4</li>
<li><strong>2030</strong>: Widespread deployment; AI Office transitions to “Post-Market Surveillance” based on serious incident reports.27</li>
</ul>
<h3 id="the-humanoid-deployment-timeline-2026-2029"><strong>The Humanoid Deployment Timeline (2026-2029)</strong></h3>
<p>Analysts predict that the “Humanoid Era” begins in earnest in 2026.5</p>
<ul>
<li><strong>2026</strong>: Boston Dynamics signal industrial-scale deployment; Tesla plans mass production of Optimus V3.5</li>
<li><strong>2027</strong>: China targets building a “secure and reliable industrial supply chain” for humanoids.24</li>
<li><strong>2028</strong>: Gartner predicts fewer than 20 companies will scale humanoids for manufacturing to the “production stage”.49</li>
<li><strong>2029</strong>: Transition from factory floors to “unstructured” consumer environments (homes, retail).12</li>
</ul>
<p><strong>Critical Bottleneck</strong>: The period between <strong>August 2026 and August 2027</strong> is the primary bottleneck. Most robotics companies are currently focused on hardware reliability and VLA performance but lack the “Internal Governance Maturity” required to produce a 120-page “Annex IV Technical File” for regulatory audit.36</p>
<h2 id="comparative-analysis-global-ai-governance-models"><strong>Comparative Analysis: Global AI Governance Models</strong></h2>
<p>The EU AI Act’s prescriptive, risk-based approach is one of several global models, each with distinct advantages and failures for embodied AI.</p>
<h3 id="nist-ai-risk-management-framework-usa"><strong>NIST AI Risk Management Framework (USA)</strong></h3>
<p>The NIST AI RMF is a voluntary, “technology-neutral” framework that emphasizes “documentation, transparency, and proportionality to harm”.25</p>
<ul>
<li><strong>Excel</strong>: It provides a granular “Risk Taxomony” that is highly compatible with engineering workflows.20</li>
<li><strong>Fail</strong>: As a voluntary standard, it provides no “Market Access” guarantee in the EU and lacks the “Financial Penalties” needed to ensure compliance in high-stakes environments like warehouse logistics.33</li>
</ul>
<h3 id="chinas-ai-regulations-content-focused"><strong>China’s AI Regulations (Content-Focused)</strong></h3>
<p>China’s approach is “Service-Specific” and “Content-Focused”.53</p>
<ul>
<li><strong>Excel</strong>: The “Algorithm Filing” system requires developers to register their models with authorities, providing high “Centralized Visibility” into the capabilities of humanoid brains.54</li>
<li><strong>Fail</strong>: The focus on “preventing illegal or harmful content” is less applicable to the “kinetic safety” of bipedal robots.53 However, China’s December 2024 guidelines on “AI and robotics in elder services” show a pivot toward sector-specific safety standards.34</li>
</ul>
<h3 id="australias-voluntary-ai-safety-standard-principles-based"><strong>Australia’s Voluntary AI Safety Standard (Principles-Based)</strong></h3>
<p>Australia utilizes a phased, “principles-based” approach featuring 10 “Voluntary Guardrails”.56</p>
<ul>
<li><strong>Excel</strong>: It offers high “Flexibility” for start-ups and is focused on “human-centered” outcomes.56</li>
<li><strong>Fail</strong>: It relies on “Self-Assessment,” which may not satisfy the “Strict Liability” requirements of the EU Product Liability Directive (PLD) for defective robotic systems.33</li>
</ul>

































<table><thead><tr><th align="left">Feature</th><th align="left">EU AI Act</th><th align="left">NIST RMF</th><th align="left">China Regs</th><th align="left">Australia Standard</th></tr></thead><tbody><tr><td align="left"><strong>Binding Force</strong></td><td align="left">High (Up to €35M)</td><td align="left">None (Voluntary)</td><td align="left">High (Market Access)</td><td align="left">Low (Self-Assessment)</td></tr><tr><td align="left"><strong>Focus Area</strong></td><td align="left">Fundamental Rights</td><td align="left">Technical Process</td><td align="left">Social/Content</td><td align="left">Accountability</td></tr><tr><td align="left"><strong>Robotics Fit</strong></td><td align="left">High (Safety linkage)</td><td align="left">High (Eng. logic)</td><td align="left">Moderate</td><td align="left">Moderate</td></tr></tbody></table>
<h2 id="machinery-regulation-20231230-and-ai-act-overlap"><strong>Machinery Regulation (2023/1230) and AI Act Overlap</strong></h2>
<p>The “Machinery Regulation” (Regulation (EU) 2023/1230) is the primary framework for physical robot safety, replacing the 2006 Machinery Directive.61 Humanoid robots are both “AI systems” and “machinery,” creating a dual-compliance burden.</p>
<h3 id="addressing-self-evolving-behavior"><strong>Addressing Self-Evolving Behavior</strong></h3>
<p>The Machinery Regulation uses the term “Self-Evolving Behavior” to anticipate the AI Act.23</p>
<ol>
<li><strong>Annex III Risk Assessment</strong>: Manufacturers must account for “risks arising from autonomous behavior” and the “interaction between human and machine”.23</li>
<li><strong>Essential Health and Safety Requirements (EHSRs)</strong>: Sections 1.1.6 and 1.2.1 of Annex III establish explicit mandates for machines with self-evolving logic.23</li>
<li><strong>Third-Party Conformity</strong>: If a robot’s “safety function” is performed by a high-risk AI system, it must undergo a third-party assessment under the Machinery Regulation (Annex I, Part A), regardless of its AI Act classification.23</li>
</ol>
<h2 id="concrete-regulatory-language-proposals"><strong>Concrete Regulatory Language Proposals</strong></h2>
<p>Where the AI Act fails to address the unique nature of multi-agent, embodied AI, the following language is proposed for future “Delegated Acts” (Article 97) or “Codes of Practice” (Article 56).</p>
<h3 id="proposal-1-definition-of-multi-agent-incident-article-3-amendment"><strong>Proposal 1: Definition of Multi-Agent Incident (Article 3 Amendment)</strong></h3>
<p>“‘Multi-Agent Incident’ means any serious incident involving the interaction of two or more autonomous AI systems, where the harm results from emergent systemic behavior or conflicting goal-decomposition, even if each system functioned within its individual design specifications.” 6</p>
<h3 id="proposal-2-requirement-for-interactive-red-teaming-article-15-amendment"><strong>Proposal 2: Requirement for Interactive Red-Teaming (Article 15 Amendment)</strong></h3>
<p>“For high-risk AI systems intended for multi-agent deployment, providers shall conduct ‘Fleet Red-Teaming’ to evaluate cascading failure modes and communication loop vulnerabilities. Testing shall be performed against a baseline of ‘Fleet-Wide Recovery Scores’.” 6</p>
<h3 id="proposal-3-supply-chain-transparency-article-28-amendment"><strong>Proposal 3: Supply Chain Transparency (Article 28 Amendment)</strong></h3>
<p>“GPAI model providers shall provide downstream integrators with a ‘Physical-Actuation Model Card,’ detailing the model’s performance limits in motor-control tasks, latent biases in object-interaction, and the latency of safety-critical inference in edge-computing environments.” 44</p>
<h2 id="concluding-analysis"><strong>Concluding Analysis</strong></h2>
<p>The EU AI Act’s application to humanoid robotics represents a transformative shift from deterministic safety to “Trustworthy AI” governance. The core challenge lies in mapping the “probabilistic” nature of VLA and LLM cognitive layers to the “binary” safety expectations of European product law.8 For developers, the period between 2026 and 2029 is a “Danger Zone” characterized by high technological velocity and immature regulatory infrastructure.5</p>
<p>Success in this landscape requires more than hardware reliability; it demands “Compliance-by-Design,” where empirical metrics like Time-to-Failure and Recovery Scores are integrated into the “Continuous Iterative Process” of risk management.8 As multi-agent fleets transition from factory floors to domestic environments, the regulatory focus must shift from “single-unit” defects to “systemic stability,” ensuring that the aggregate behavior of these agents remains aligned with human safety and fundamental rights.6 The AI Act provides the skeleton for this governance, but the “flesh”—the harmonized standards and specific test methodologies—must be built rapidly by the robotics community to avoid a multi-year “innovation logjam”.12</p>
<h4 id="works-cited"><strong>Works cited</strong></h4>
<ol>
<li>Regulation - EU - 2024/1689 - EN - EUR-Lex - European Union, accessed on February 4, 2026, <a href="https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng">https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng</a></li>
<li>Regulation - EU - 2024/1689 - EN - EUR-Lex - European Data Portal, accessed on February 4, 2026, <a href="https://data.europa.eu/eli/reg/2024/1689/oj">https://data.europa.eu/eli/reg/2024/1689/oj</a></li>
<li>Artificial Intelligence Act, Regulation (EU) 2024/1689 - Links - EU AI Act, accessed on February 4, 2026, <a href="https://www.artificial-intelligence-act.com/Artificial_Intelligence_Act_Links.html">https://www.artificial-intelligence-act.com/Artificial_Intelligence_Act_Links.html</a></li>
<li>High-level summary of the AI Act | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/high-level-summary/">https://artificialintelligenceact.eu/high-level-summary/</a></li>
<li>CES 2026 Robotics Announcements Recap - Counterpoint Research, accessed on February 4, 2026, <a href="https://counterpointresearch.com/en/insights/ces-2026-robotics-announcements-recap">https://counterpointresearch.com/en/insights/ces-2026-robotics-announcements-recap</a></li>
<li>EU Regulations Are Not Ready for Multi-Agent AI Incidents | TechPolicy.Press, accessed on February 4, 2026, <a href="https://www.techpolicy.press/eu-regulations-are-not-ready-for-multiagent-ai-incidents/">https://www.techpolicy.press/eu-regulations-are-not-ready-for-multiagent-ai-incidents/</a></li>
<li>Article 9: Risk Management System | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/article/9/">https://artificialintelligenceact.eu/article/9/</a></li>
<li>AI Act Service Desk - Article 9: Risk management system - European Union, accessed on February 4, 2026, <a href="https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-9">https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-9</a></li>
<li>Article 9 - Risk management system - CMS DigitalLaws, accessed on February 4, 2026, <a href="https://www.cms-digitallaws.com/en/ai-act/article-9/">https://www.cms-digitallaws.com/en/ai-act/article-9/</a></li>
<li>Effective implementation of requirements for high-risk AI systems under the AI Act - cerre, accessed on February 4, 2026, <a href="https://cerre.eu/wp-content/uploads/2025/02/Effective-Implementation-of-Requirements-for-High-Risk-AI-Systems-Under-the-AI-Act_FINAL-1.pdf">https://cerre.eu/wp-content/uploads/2025/02/Effective-Implementation-of-Requirements-for-High-Risk-AI-Systems-Under-the-AI-Act_FINAL-1.pdf</a></li>
<li>(PDF) Comprehensive Methodologies and Metrics for Testing and Validating AI Agents in Single-Agent and Multi-Agent Environments - ResearchGate, accessed on February 4, 2026, <a href="https://www.researchgate.net/publication/389747050_Comprehensive_Methodologies_and_Metrics_for_Testing_and_Validating_AI_Agents_in_Single-Agent_and_Multi-Agent_Environments">https://www.researchgate.net/publication/389747050_Comprehensive_Methodologies_and_Metrics_for_Testing_and_Validating_AI_Agents_in_Single-Agent_and_Multi-Agent_Environments</a></li>
<li>Robotics in 2026: Building the Business Case for Humanoids, accessed on February 4, 2026, <a href="https://aibusiness.com/robotics/building-business-case-for-humanoid-robotics">https://aibusiness.com/robotics/building-business-case-for-humanoid-robotics</a></li>
<li>EU AI Act - Updates, Compliance, Training, accessed on February 4, 2026, <a href="https://www.artificial-intelligence-act.com/">https://www.artificial-intelligence-act.com/</a></li>
<li>Overview of the Code of Practice | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/code-of-practice-overview/">https://artificialintelligenceact.eu/code-of-practice-overview/</a></li>
<li>Guide to the AI Act - a detailed breakdown of what you need to know - A&#x26;L Goodbody, accessed on February 4, 2026, <a href="https://www.algoodbody.com/images/uploads/services/Commercial-Tech/AL_Goodbody_-_Guide_to_the_AI_Act.pdf">https://www.algoodbody.com/images/uploads/services/Commercial-Tech/AL_Goodbody_-_Guide_to_the_AI_Act.pdf</a></li>
<li>Prioritizing Real-Time Failure Detection in AI Agents - Partnership on AI, accessed on February 4, 2026, <a href="https://partnershiponai.org/wp-content/uploads/2025/09/agents-real-time-failure-detection.pdf?vgo_ee=zBAC1la9zQyJHSnpG6BgMHYqtA2DVnJIxaZdlyzMse4LqANZiVSdqdBDKQ%3D%3D:UuOdAvb8Al76ab6ZrhxDyj0LJ66FZeBh">https://partnershiponai.org/wp-content/uploads/2025/09/agents-real-time-failure-detection.pdf?vgo_ee=zBAC1la9zQyJHSnpG6BgMHYqtA2DVnJIxaZdlyzMse4LqANZiVSdqdBDKQ%3D%3D%3AUuOdAvb8Al76ab6ZrhxDyj0LJ66FZeBh</a></li>
<li>Robots and Industry Disruption: Strategic Market Analysis and Bold Predictions 2025, accessed on February 4, 2026, <a href="https://sparkco.ai/blog/robot">https://sparkco.ai/blog/robot</a></li>
<li>Social Robots Market Report | Industry Analysis, Size &#x26; Forecast Trends, accessed on February 4, 2026, <a href="https://www.mordorintelligence.com/industry-reports/social-robots-market">https://www.mordorintelligence.com/industry-reports/social-robots-market</a></li>
<li>Article 9: Risk Management System | EU AI Act - Securiti, accessed on February 4, 2026, <a href="https://securiti.ai/eu-ai-act/article-9/">https://securiti.ai/eu-ai-act/article-9/</a></li>
<li>When AI Agents Misbehave: Governance and Security for Autonomous AI - Our Take, accessed on February 4, 2026, <a href="https://ourtake.bakerbotts.com/post/102me2l/when-ai-agents-misbehave-governance-and-security-for-autonomous-ai">https://ourtake.bakerbotts.com/post/102me2l/when-ai-agents-misbehave-governance-and-security-for-autonomous-ai</a></li>
<li>Robot Proficiency Self-Assessment Using Assumption-Alignment Tracking - BYU Computer Science Students Homepage Index, accessed on February 4, 2026, <a href="https://faculty.cs.byu.edu/~mike/mikeg/papers/Robot_Proficiency_Self-Assessment_Using_Assumption-Alignment_Tracking.pdf">https://faculty.cs.byu.edu/~mike/mikeg/papers/Robot_Proficiency_Self-Assessment_Using_Assumption-Alignment_Tracking.pdf</a></li>
<li>Regulation 2023/1230/EU - machinery | Safety and health at work EU-OSHA, accessed on February 4, 2026, <a href="https://osha.europa.eu/en/legislation/directive/regulation-20231230eu-machinery">https://osha.europa.eu/en/legislation/directive/regulation-20231230eu-machinery</a></li>
<li>New Machinery Regulation - IBF Solutions, accessed on February 4, 2026, <a href="https://www.ibf-solutions.com/en/seminars-and-news/news/new-machinery-regulation">https://www.ibf-solutions.com/en/seminars-and-news/news/new-machinery-regulation</a></li>
<li>AI Humanoid Robots 2026: Technology, Builders &#x26; Future - Articsledge, accessed on February 4, 2026, <a href="https://www.articsledge.com/post/ai-humanoid-robots">https://www.articsledge.com/post/ai-humanoid-robots</a></li>
<li>Trustworthy agentic AI systems: a cross-layer review of architectures, threat models, and governance strategies for real-world deployment. - F1000Research, accessed on February 4, 2026, <a href="https://f1000research.com/articles/14-905">https://f1000research.com/articles/14-905</a></li>
<li>New Research: Europe’s AI Security Controls Trail Global Benchmarks as Attack Surface Expands, accessed on February 4, 2026, <a href="https://www.vigilance-securitymagazine.com/news/categories/cyber-security-a-e-crime/12168-new-research-europe-s-ai-security-controls-trail-global-benchmarks-as-attack-surface-expands">https://www.vigilance-securitymagazine.com/news/categories/cyber-security-a-e-crime/12168-new-research-europe-s-ai-security-controls-trail-global-benchmarks-as-attack-surface-expands</a></li>
<li>Preparing for the EU AI act—technical requirements for real-time compliance | Relyance AI, accessed on February 4, 2026, <a href="https://www.relyance.ai/ai-governance/eu-ai-act-compliance">https://www.relyance.ai/ai-governance/eu-ai-act-compliance</a></li>
<li>Key indicators in a RAMS analysis - Exceltic, accessed on February 4, 2026, <a href="https://exceltic.com/en/key-indicators-in-an-analysis-rams/">https://exceltic.com/en/key-indicators-in-an-analysis-rams/</a></li>
<li>Annex IV: Technical Documentation Referred to in Article 11(1) | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/annex/4/">https://artificialintelligenceact.eu/annex/4/</a></li>
<li>Article 6: Classification Rules for High-Risk AI Systems | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/article/6/">https://artificialintelligenceact.eu/article/6/</a></li>
<li>Annex III: High-Risk AI Systems Referred to in Article 6(2) | EU …, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/annex/3/">https://artificialintelligenceact.eu/annex/3/</a></li>
<li>AI Act Service Desk - Annex III - European Union, accessed on February 4, 2026, <a href="https://ai-act-service-desk.ec.europa.eu/en/ai-act/annex-3">https://ai-act-service-desk.ec.europa.eu/en/ai-act/annex-3</a></li>
<li>How Australia’s AI Regulation Compares to the EU AI Act, US Approach, and Other International Frameworks - SoftwareSeni, accessed on February 4, 2026, <a href="https://www.softwareseni.com/how-australias-ai-regulation-compares-to-the-eu-ai-act-us-approach-and-other-international-frameworks/">https://www.softwareseni.com/how-australias-ai-regulation-compares-to-the-eu-ai-act-us-approach-and-other-international-frameworks/</a></li>
<li>AI-Powered Solutions For Elderly Care Market: Trends, Report 2030, accessed on February 4, 2026, <a href="https://www.knowledge-sourcing.com/report/ai-powered-solutions-for-elderly-care-market">https://www.knowledge-sourcing.com/report/ai-powered-solutions-for-elderly-care-market</a></li>
<li>AI Risk Classification: Guide to EU AI Act Risk Categories - GDPR Local, accessed on February 4, 2026, <a href="https://gdprlocal.com/ai-risk-classification/">https://gdprlocal.com/ai-risk-classification/</a></li>
<li>FAQ: Technical documentation in accordance with the EU AI Act - kothes, accessed on February 4, 2026, <a href="https://www.kothes.com/en/blog/faq-eu-ai-regulation">https://www.kothes.com/en/blog/faq-eu-ai-regulation</a></li>
<li>general-purpose AI models - EU AI Act Guide | AI Resource Center | Orrick, accessed on February 4, 2026, <a href="https://ai-law-center.orrick.com/eu-ai-act/general-purpose-ai/">https://ai-law-center.orrick.com/eu-ai-act/general-purpose-ai/</a></li>
<li>Article 2: Scope | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/article/2/">https://artificialintelligenceact.eu/article/2/</a></li>
<li>AI Act Service Desk - Article 2: Scope - European Union, accessed on February 4, 2026, <a href="https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-2">https://ai-act-service-desk.ec.europa.eu/en/ai-act/article-2</a></li>
<li>An Introduction to the Code of Practice for General-Purpose AI | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/introduction-to-code-of-practice/">https://artificialintelligenceact.eu/introduction-to-code-of-practice/</a></li>
<li>EU AI Act Compliance Readiness | Avoid €35M Penalties - VerityAI, accessed on February 4, 2026, <a href="https://verityai.co/landing/eu-ai-act-compliance-readiness">https://verityai.co/landing/eu-ai-act-compliance-readiness</a></li>
<li>Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems - arXiv, accessed on February 4, 2026, <a href="https://arxiv.org/html/2508.05687v1">https://arxiv.org/html/2508.05687v1</a></li>
<li>Navigating product liability in high-security sectors: Addressing AI-driven risks under German and European law | White &#x26; Case LLP, accessed on February 4, 2026, <a href="https://www.whitecase.com/insight-alert/navigating-product-liability-high-security-sectors-addressing-ai-driven-risks-under">https://www.whitecase.com/insight-alert/navigating-product-liability-high-security-sectors-addressing-ai-driven-risks-under</a></li>
<li>Article 25: Responsibilities Along the AI Value Chain | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/article/25/">https://artificialintelligenceact.eu/article/25/</a></li>
<li>Liability for AI in the supply chain: an overview - Farrer &#x26; Co, accessed on February 4, 2026, <a href="https://www.farrer.co.uk/news-and-insights/liability-for-ai-in-the-supply-chain-an-overview/">https://www.farrer.co.uk/news-and-insights/liability-for-ai-in-the-supply-chain-an-overview/</a></li>
<li>EU AI Act Compliance Checker | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-checker/">https://artificialintelligenceact.eu/assessment/eu-ai-act-compliance-checker/</a></li>
<li>B REGULATION (EU) 2023/1230 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL of 14 June 2023 on machinery and repealing Directiv - EUR-Lex, accessed on February 4, 2026, <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:02023R1230-20230629">https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:02023R1230-20230629</a></li>
<li>Overview of Guidelines for GPAI Models | EU Artificial Intelligence Act, accessed on February 4, 2026, <a href="https://artificialintelligenceact.eu/gpai-guidelines-overview/">https://artificialintelligenceact.eu/gpai-guidelines-overview/</a></li>
<li>Gartner Predicts Fewer Than 20 Companies Will Scale Humanoid Robots for Manufacturing and Supply Chain to Production Stage by 2028, accessed on February 4, 2026, <a href="https://www.gartner.com/en/newsroom/press-releases/2026-01-21-gartner-predicts-fewer-than-20-companies-will-scale-humanoid-robots-for-manufacturing-and-supply-chain-to-production-stage-by-2028">https://www.gartner.com/en/newsroom/press-releases/2026-01-21-gartner-predicts-fewer-than-20-companies-will-scale-humanoid-robots-for-manufacturing-and-supply-chain-to-production-stage-by-2028</a></li>
<li>Annex IV Demystified: A CTO’s Practical Guide to the AI Technical File | by Sankalp Salve, accessed on February 4, 2026, <a href="https://medium.com/@xsankalp13/annex-iv-demystified-a-ctos-practical-guide-to-the-ai-technical-file-a81603923731">https://medium.com/@xsankalp13/annex-iv-demystified-a-ctos-practical-guide-to-the-ai-technical-file-a81603923731</a></li>
<li>Global Approaches to Artificial Intelligence Regulation, accessed on February 4, 2026, <a href="https://jsis.washington.edu/news/global-approaches-to-artificial-intelligence-regulation/">https://jsis.washington.edu/news/global-approaches-to-artificial-intelligence-regulation/</a></li>
<li>EU and Australia diverge on paths to AI regulation | Digital Watch Observatory, accessed on February 4, 2026, <a href="https://dig.watch/updates/eu-and-australia-diverge-on-paths-to-ai-regulation">https://dig.watch/updates/eu-and-australia-diverge-on-paths-to-ai-regulation</a></li>
<li>AI Regulations in 2025: US, EU, UK, Japan, China &#x26; More - Anecdotes AI, accessed on February 4, 2026, <a href="https://www.anecdotes.ai/learn/ai-regulations-in-2025-us-eu-uk-japan-china-and-more">https://www.anecdotes.ai/learn/ai-regulations-in-2025-us-eu-uk-japan-china-and-more</a></li>
<li>Preparing for compliance: Key differences between EU, Chinese AI regulations | IAPP, accessed on February 4, 2026, <a href="https://iapp.org/news/a/preparing-for-compliance-key-differences-between-eu-chinese-ai-regulations">https://iapp.org/news/a/preparing-for-compliance-key-differences-between-eu-chinese-ai-regulations</a></li>
<li>Deep Fakes and Surveillance Technology: Comparing the EU AI Act and Chinese AI Regulation - BHRJ Blog, accessed on February 4, 2026, <a href="https://bhrj.blog/2025/02/05/deep-fakes-and-surveillance-technology-comparing-the-eu-ai-act-and-chinese-ai-regulation/">https://bhrj.blog/2025/02/05/deep-fakes-and-surveillance-technology-comparing-the-eu-ai-act-and-chinese-ai-regulation/</a></li>
<li>Global AI Governance Law and Policy: Australia - IAPP, accessed on February 4, 2026, <a href="https://iapp.org/resources/article/global-ai-governance-australia">https://iapp.org/resources/article/global-ai-governance-australia</a></li>
<li>Understanding the European Union’s AI Act: Practical Considerations for Australian Businesses | Association of Corporate Counsel (ACC), accessed on February 4, 2026, <a href="https://www.acc.com/understanding-european-unions-ai-act-practical-considerations-australian-businesses">https://www.acc.com/understanding-european-unions-ai-act-practical-considerations-australian-businesses</a></li>
<li>Australia Government - Guidance for AI Adoption- Foundations | PDF | Artificial Intelligence - Scribd, accessed on February 4, 2026, <a href="https://www.scribd.com/document/986735400/Australia-Government-Guidance-for-AI-Adoption-Foundations">https://www.scribd.com/document/986735400/Australia-Government-Guidance-for-AI-Adoption-Foundations</a></li>
<li>Voluntary AI Safety Standard (10 Guardrails) - SafeAI-Aus, accessed on February 4, 2026, <a href="https://safeaiaus.org/safety-standards/voluntary-ai-safety-standard-10-guardrails/">https://safeaiaus.org/safety-standards/voluntary-ai-safety-standard-10-guardrails/</a></li>
<li>AI in the tech sector – touch points and regulatory difference across the EU and Australia, accessed on February 4, 2026, <a href="https://www.technologyslegaledge.com/2025/09/ai-in-the-tech-sector-touch-points-and-regulatory-difference-across-the-eu-and-australia/">https://www.technologyslegaledge.com/2025/09/ai-in-the-tech-sector-touch-points-and-regulatory-difference-across-the-eu-and-australia/</a></li>
<li>Regulation - 2023/1230 - EN - EUR-Lex - European Union, accessed on February 4, 2026, <a href="https://eur-lex.europa.eu/eli/reg/2023/1230/oj/eng">https://eur-lex.europa.eu/eli/reg/2023/1230/oj/eng</a></li>
<li>Regulation (EU) 2023/1230 - DGUV, accessed on February 4, 2026, <a href="https://www.dguv.de/dguv-test/prod-testing-certi/conform-prod/machinery/eu-maschinenverordnung/index.jsp">https://www.dguv.de/dguv-test/prod-testing-certi/conform-prod/machinery/eu-maschinenverordnung/index.jsp</a></li>
<li>AI as product vs. AI as service: Unpacking the liability divide in EU safety legislation | IAPP, accessed on February 4, 2026, <a href="https://iapp.org/news/a/ai-as-product-vs-ai-as-service-unpacking-the-liability-divide-in-eu-safety-legislation">https://iapp.org/news/a/ai-as-product-vs-ai-as-service-unpacking-the-liability-divide-in-eu-safety-legislation</a></li>
</ol> </div> </article>   </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html>  