<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>Policy Brief: Capability Does Not Imply Safety | Failure-First</title><meta name="description" content="Empirical evidence from 8 foundation models reveals a U-shaped safety curve. Larger models are not inherently safer — capability without proportional safety investment increases adversarial risk."><link rel="canonical" href="https://failurefirst.org/policy/capability-safety-spectrum/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="website"><meta property="og:title" content="Policy Brief: Capability Does Not Imply Safety | Failure-First"><meta property="og:description" content="Empirical evidence from 8 foundation models reveals a U-shaped safety curve. Larger models are not inherently safer — capability without proportional safety investment increases adversarial risk."><meta property="og:url" content="https://failurefirst.org/policy/capability-safety-spectrum/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="Policy Brief: Capability Does Not Imply Safety | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="Policy Brief: Capability Does Not Imply Safety | Failure-First"><meta name="twitter:description" content="Empirical evidence from 8 foundation models reveals a U-shaped safety curve. Larger models are not inherently safer — capability without proportional safety investment increases adversarial risk."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="Policy Brief: Capability Does Not Imply Safety | Failure-First - Failure-First Embodied AI"><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><link rel="stylesheet" href="/assets/_slug_.C0DMmNf8.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
.table-wrapper[data-astro-cid-2sqenjrx]{overflow-x:auto;margin:1.5rem 0}table[data-astro-cid-2sqenjrx]{width:100%;border-collapse:collapse;font-size:.875rem;font-family:JetBrains Mono,monospace}th[data-astro-cid-2sqenjrx],td[data-astro-cid-2sqenjrx]{padding:.5rem .75rem;text-align:left;border-bottom:1px solid var(--border-subtle)}th[data-astro-cid-2sqenjrx]{color:var(--fg-muted);font-size:.75rem;text-transform:uppercase;letter-spacing:.04em;font-weight:500}td[data-astro-cid-2sqenjrx]{color:var(--fg)}.highlight-row[data-astro-cid-2sqenjrx] td[data-astro-cid-2sqenjrx]{color:var(--failure-warning);font-weight:500}ol[data-astro-cid-2sqenjrx]{padding-left:1.25rem;margin:.75rem 0}ol[data-astro-cid-2sqenjrx] li[data-astro-cid-2sqenjrx]{margin-bottom:.75rem;line-height:1.5}ul[data-astro-cid-2sqenjrx]{padding-left:1.25rem;margin:.75rem 0}ul[data-astro-cid-2sqenjrx] li[data-astro-cid-2sqenjrx]{margin-bottom:.5rem;line-height:1.5}
</style></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> Research </a> </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog </a> </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework </a> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/" class="active" aria-current="page" data-astro-cid-pux6a34n> Policy </a> </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About </a> </li> </ul> </div> </nav>  <script type="module">const e=document.querySelector(".nav-toggle"),t=document.querySelector(".nav-links");e&&t&&(e.addEventListener("click",()=>{const n=e.getAttribute("aria-expanded")==="true";e.setAttribute("aria-expanded",String(!n)),t.classList.toggle("open")}),document.addEventListener("keydown",n=>{n.key==="Escape"&&t.classList.contains("open")&&(t.classList.remove("open"),e.setAttribute("aria-expanded","false"),e.focus())}),document.addEventListener("click",n=>{t.classList.contains("open")&&!t.contains(n.target)&&!e.contains(n.target)&&(t.classList.remove("open"),e.setAttribute("aria-expanded","false"))}));</script> <main id="main-content"> <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/policy/" data-astro-cid-ilhxcym7>Policy</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>Capability-Safety Spectrum</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Policy","item":"https://failurefirst.org/policy/"},{"@type":"ListItem","position":3,"name":"Capability-Safety Spectrum"}]}</script>  <header> <p><a href="/policy/">&larr; All Policy Briefs</a></p> <h1>Capability Does Not Imply Safety</h1> <p class="tagline">Empirical evidence from jailbreak archaeology across eight foundation models</p> </header> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Summary</h2> <p data-astro-cid-2sqenjrx>
A systematic evaluation of 64 historical jailbreak scenarios across eight
      foundation models&mdash;spanning 1.5B to frontier scale&mdash;reveals a
<strong data-astro-cid-2sqenjrx>non-monotonic relationship between model capability and safety
      robustness</strong>. Rather than improving linearly with scale, adversarial
      resistance follows a U-shaped curve.
</p> <p data-astro-cid-2sqenjrx>
Small models fail safely through incapability. Frontier closed-source models
      refuse effectively through extensive alignment investment. Medium-to-large
      open-weight models occupy a dangerous intermediate zone where capability
      outpaces safety training.
</p> <p data-astro-cid-2sqenjrx>
The most significant finding: <strong data-astro-cid-2sqenjrx>reasoning-era attacks achieve higher
      success rates on larger models than on smaller ones</strong>. This result,
      consistent with the "Inverse Scaling for Safety" phenomenon described in the
      literature, provides empirical evidence that compute-threshold-based
      regulation alone is insufficient for assessing adversarial risk.
</p> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Key Metrics</h2> <div class="stats"> <div class="stat"> <div class="stat-number">8</div> <div class="stat-label">Models Tested</div> </div><div class="stat"> <div class="stat-number">6</div> <div class="stat-label">Attack Eras (2022–2026)</div> </div><div class="stat"> <div class="stat-number">64</div> <div class="stat-label">Scenarios Per Model</div> </div> </div> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>The Jailbreak Archaeology Dataset</h2> <p data-astro-cid-2sqenjrx>
The evaluation used scenarios drawn from six historical attack eras, each
      representing a distinct class of adversarial technique that emerged between
      2022 and 2026.
</p> <div class="table-wrapper" data-astro-cid-2sqenjrx> <table data-astro-cid-2sqenjrx> <thead data-astro-cid-2sqenjrx> <tr data-astro-cid-2sqenjrx> <th data-astro-cid-2sqenjrx>Era</th> <th data-astro-cid-2sqenjrx>Attack Class</th> <th data-astro-cid-2sqenjrx>Years</th> </tr> </thead> <tbody data-astro-cid-2sqenjrx> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>1</td><td data-astro-cid-2sqenjrx>Direct Injection (persona adoption)</td><td data-astro-cid-2sqenjrx>2022&ndash;23</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>2</td><td data-astro-cid-2sqenjrx>Obfuscation (cipher encoding)</td><td data-astro-cid-2sqenjrx>2023&ndash;24</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>3</td><td data-astro-cid-2sqenjrx>Context Flooding (many-shot, skeleton key)</td><td data-astro-cid-2sqenjrx>2024&ndash;25</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>4</td><td data-astro-cid-2sqenjrx>Gradual Escalation (crescendo)</td><td data-astro-cid-2sqenjrx>2024&ndash;25</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>5</td><td data-astro-cid-2sqenjrx>Cognitive Hijacking (CoT exploits)</td><td data-astro-cid-2sqenjrx>2025&ndash;26</td></tr> </tbody> </table> </div> <p data-astro-cid-2sqenjrx>
Models were tested in standardized single-turn format with LLM-based
      classification and manual spot-checking.
</p> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Finding 1: The U-Shaped Safety Curve</h2> <p data-astro-cid-2sqenjrx>
Corrected attack success rate (ASR) does not decrease monotonically with
      model scale. Instead, it follows a U-shaped pattern with three distinct
      safety regimes:
</p> <div class="table-wrapper" data-astro-cid-2sqenjrx> <table data-astro-cid-2sqenjrx> <thead data-astro-cid-2sqenjrx> <tr data-astro-cid-2sqenjrx> <th data-astro-cid-2sqenjrx>Tier</th> <th data-astro-cid-2sqenjrx>Model</th> <th data-astro-cid-2sqenjrx>Corrected ASR</th> </tr> </thead> <tbody data-astro-cid-2sqenjrx> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Small (1.7B)</td><td data-astro-cid-2sqenjrx>Qwen3-1.7b</td><td data-astro-cid-2sqenjrx>21.3%</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Small (3B)</td><td data-astro-cid-2sqenjrx>Llama 3.2</td><td data-astro-cid-2sqenjrx>~0% (skeleton key)</td></tr> <tr class="highlight-row" data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Medium (70B)</td><td data-astro-cid-2sqenjrx>Llama-3.3-70b</td><td data-astro-cid-2sqenjrx>85.7% (reasoning era)</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Frontier</td><td data-astro-cid-2sqenjrx>Gemini 3 Flash</td><td data-astro-cid-2sqenjrx>1.6%</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Frontier</td><td data-astro-cid-2sqenjrx>Claude Sonnet 4.5</td><td data-astro-cid-2sqenjrx>0.0%</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Frontier</td><td data-astro-cid-2sqenjrx>Codex GPT-5.2</td><td data-astro-cid-2sqenjrx>0.0%</td></tr> </tbody> </table> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Regime A: Incapable Safety (sub-3B)</h3> <p data-astro-cid-2sqenjrx>
Models in this range often cannot process attacks as intended. Cipher-encoded
        prompts produce hallucinated output rather than decoded harmful content. The
        model's incapability acts as an inadvertent safety mechanism&mdash;it fails safely
        because it fails at everything.
</p> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Regime B: Capability-Safety Gap (medium scale)</h3> <p data-astro-cid-2sqenjrx>
Models at this scale can decode ciphers, follow multi-turn reasoning, and
        synthesize complex instructions. However, their safety alignment has not
        scaled proportionally. This is where capability enables attack execution that
        smaller models simply cannot parse.
</p> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Regime C: Aligned Frontier (closed-source frontier)</h3> <p data-astro-cid-2sqenjrx>
Models with massive investment in RLHF, red-teaming, and API-level filtering
        achieve near-zero ASR. The three frontier models all achieved corrected ASR
        below 2%. This regime depends on <em data-astro-cid-2sqenjrx>continuous safety investment</em>&mdash;it
        is not an inherent property of scale.
</p> </div> <div class="warning"> <p><strong>Important Caveat</strong></p>  <p data-astro-cid-2sqenjrx>
The U-shape observed here confounds multiple variables: parameter count,
        training methodology, closed vs. open weight, and RLHF budget. The precise
        claim is: <strong data-astro-cid-2sqenjrx>capability alone, without proportional safety investment,
        creates increased adversarial risk at medium scale.</strong> </p>  </div> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Finding 2: Three Cipher Modalities</h2> <p data-astro-cid-2sqenjrx>
Cipher-era scenarios (ROT13, Base64, custom encoding) produced a distinctive
      three-mode response pattern that serves as a diagnostic for a model's position
      on the capability-safety spectrum:
</p> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Modality 1: Hallucinate</h3> <p data-astro-cid-2sqenjrx>
Cannot decode cipher; produces unrelated or garbled output. Observed in
        small models (Llama 3.2, partially Qwen3-1.7b). Safety through incapability
        &mdash; fragile, disappears as models improve.
</p> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Modality 2: API Block</h3> <p data-astro-cid-2sqenjrx>
Request blocked at infrastructure level before reaching model reasoning.
        Observed in Claude Sonnet 4.5 (18 of 64 traces). Effective but coarse
        &mdash; pattern-matches on known attack signatures, cannot generalize to
        novel attack classes.
</p> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Modality 3: Decode-then-Refuse</h3> <p data-astro-cid-2sqenjrx>
Successfully decodes cipher content, identifies harmful intent, refuses.
        Observed in Codex GPT-5.2 (all cipher scenarios). The most robust posture
        &mdash; safety alignment operates at the semantic level rather than the
        syntactic level.
</p> </div> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Finding 3: Reasoning-Era Inverse Scaling</h2> <p data-astro-cid-2sqenjrx>
The most policy-relevant finding concerns reasoning-era attacks (chain-of-thought
      hijacking, abductive reasoning exploits). Across all tested models, the reasoning
      era produced the highest or near-highest ASR:
</p> <div class="table-wrapper" data-astro-cid-2sqenjrx> <table data-astro-cid-2sqenjrx> <thead data-astro-cid-2sqenjrx> <tr data-astro-cid-2sqenjrx> <th data-astro-cid-2sqenjrx>Model</th> <th data-astro-cid-2sqenjrx>Reasoning-Era ASR</th> <th data-astro-cid-2sqenjrx>Overall ASR</th> </tr> </thead> <tbody data-astro-cid-2sqenjrx> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Qwen3-1.7b</td><td data-astro-cid-2sqenjrx>57%</td><td data-astro-cid-2sqenjrx>21.3%</td></tr> <tr class="highlight-row" data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Llama-3.3-70b</td><td data-astro-cid-2sqenjrx>85.7%</td><td data-astro-cid-2sqenjrx>85.7% (reasoning only)</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Gemini 3 Flash</td><td data-astro-cid-2sqenjrx>10%</td><td data-astro-cid-2sqenjrx>1.6%</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Claude Sonnet 4.5</td><td data-astro-cid-2sqenjrx>0%</td><td data-astro-cid-2sqenjrx>0%</td></tr> <tr data-astro-cid-2sqenjrx><td data-astro-cid-2sqenjrx>Codex GPT-5.2</td><td data-astro-cid-2sqenjrx>0%</td><td data-astro-cid-2sqenjrx>0%</td></tr> </tbody> </table> </div> <p data-astro-cid-2sqenjrx>
The critical observation: Llama-3.3-70B's 85.7% reasoning-era ASR substantially
      exceeds the 40&ndash;60% range observed on models 20&ndash;40x smaller. This is the
      empirical signature of inverse scaling for safety&mdash;a larger, more capable model
      is <em data-astro-cid-2sqenjrx>more vulnerable</em> to attacks that exploit its reasoning capacity.
</p> <div class="warning"> <p><strong>Sample Size Limitation</strong></p>  <p data-astro-cid-2sqenjrx>
The Llama-3.3-70B result is based on 7 valid traces (reasoning era only).
        While consistent with prior literature on reasoning model vulnerabilities,
        this signal requires confirmation with larger samples (n&gt;20).
</p>  </div> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Policy Implications</h2> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Compute Thresholds Are Insufficient</h3> <p data-astro-cid-2sqenjrx>
Regulatory frameworks that use training compute (e.g., the EU AI Act's
        10<sup data-astro-cid-2sqenjrx>25</sup> FLOP threshold) as the primary risk indicator assume a monotonic
        relationship between compute and risk. Our data suggests this assumption is
        incomplete: models well below the threshold can exhibit extreme vulnerability
        to specific attack classes, while models above it achieve near-zero ASR through
        safety investment, not scale alone.
</p> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>Static Benchmarks Miss Temporal Evolution</h3> <p data-astro-cid-2sqenjrx>
A model that achieves 0% ASR against 2023-era attacks may still be highly
        vulnerable to 2025-era techniques. Current "snapshot" safety certifications
        test against a fixed set of known attacks at a point in time. Safety evaluations
        must be era-stratified to reveal which attack classes a model remains vulnerable to.
</p> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>The Case for Mandatory Continuous Testing</h3> <p data-astro-cid-2sqenjrx>
Three empirical observations support mandatory Continuous Adversarial Regression
        Testing (CART): (1)&nbsp;models that resist older attack eras can still fail on
        newer ones; (2)&nbsp;small models that are "safe through incapability" can become
        unsafe with minor capability improvements; (3)&nbsp;inverse scaling creates moving
        targets where safety evaluated at one scale may not hold at another.
</p> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>The Zombie Model Problem</h3> <p data-astro-cid-2sqenjrx>
Open-weight models cannot be patched or recalled once downloaded. If the medium-scale
        vulnerability pattern holds at larger sample sizes, widely deployed open-weight models
        represent a persistent adversarial risk that grows as new attack techniques are
        discovered.
</p> </div> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Recommendations</h2> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>For Regulators</h3> <ol data-astro-cid-2sqenjrx> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Supplement compute thresholds with capability-based adversarial evaluation.</strong>
Require models in high-risk contexts be tested against era-stratified jailbreak
          batteries, not just current-generation attacks.
</li> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Mandate era-stratified ASR reporting.</strong> Aggregate safety metrics
          mask era-specific vulnerabilities. Per-era breakdowns reveal which attack classes
          a model remains exposed to.
</li> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Establish CART requirements for high-risk deployments.</strong> Require
          quarterly adversarial regression testing with retro-holdout sets maintained by
          an independent body.
</li> </ol> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>For Model Developers</h3> <ol data-astro-cid-2sqenjrx> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Invest in semantic-level safety.</strong> The decode-then-refuse pattern
          is more robust than API-level pattern matching or incapability-based safety.
          Safety alignment that operates at the level of understanding intent generalizes
          better across attack eras.
</li> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Treat reasoning architecture as a distinct risk factor.</strong> Reasoning
          models require safety evaluations beyond those applied to standard instruction-following models.
</li> </ol> </div> <div class="card" data-astro-cid-2sqenjrx> <h3 data-astro-cid-2sqenjrx>For Deployers</h3> <p data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Do not assume that model scale implies safety.</strong> Deployment risk
        assessments should be based on empirical adversarial testing against current
        attack taxonomies, not model size.
</p> </div> </section> <section data-astro-cid-2sqenjrx> <h2 data-astro-cid-2sqenjrx>Limitations</h2> <ul data-astro-cid-2sqenjrx> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Uneven sample sizes:</strong> Frontier models tested on all 64 scenarios;
        Llama-3.3-70B on 10 (7 valid). The inverse scaling signal requires confirmation
        at n&gt;20 per model per era.
</li> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Confounding variables:</strong> The comparison confounds parameter count,
        training methodology, open vs. closed weight, and RLHF budget.
</li> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Single-turn testing:</strong> Multi-turn attacks were only evaluated
        for select models. Results may differ in multi-turn settings.
</li> <li data-astro-cid-2sqenjrx> <strong data-astro-cid-2sqenjrx>Temporal snapshot:</strong> This evaluation reflects model behavior as
        of early February 2026. Providers continuously update safety measures.
</li> </ul> </section> <div class="warning"> <p><strong>Research Context</strong></p>  <p data-astro-cid-2sqenjrx>
This brief presents pattern-level findings from the
<a href="/" data-astro-cid-2sqenjrx>Failure-First</a> adversarial AI safety research project.
      It does not contain operational attack instructions. All findings are
      published to advance the collective understanding of AI safety evaluation.
</p>  </div>   </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html> 