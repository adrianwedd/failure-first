<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search | Daily Paper | Failure-First</title><meta name="description" content="Proposes RLbreaker, a deep reinforcement learning-driven black-box jailbreaking attack that uses DRL with customized reward functions and PPO to automatically generate effective jailbreaking prompts,..."><link rel="canonical" href="https://failurefirst.org/daily-paper/2026-02-13-240608705/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="article"><meta property="og:title" content="When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search | Daily Paper | Failure-First"><meta property="og:description" content="Proposes RLbreaker, a deep reinforcement learning-driven black-box jailbreaking attack that uses DRL with customized reward functions and PPO to automatically generate effective jailbreaking prompts,..."><meta property="og:url" content="https://failurefirst.org/daily-paper/2026-02-13-240608705/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search | Daily Paper | Failure-First"><meta name="twitter:description" content="Proposes RLbreaker, a deep reinforcement learning-driven black-box jailbreaking attack that uses DRL with customized reward functions and PPO to automatically generate effective jailbreaking prompts,..."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="article:published_time" content="2026-02-13"><!-- Google Scholar Meta Tags (for research papers) --><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
</style>
<link rel="stylesheet" href="/assets/_slug_.4MQlVLr6.css"></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/daily-paper/" class="active" aria-current="page" data-astro-cid-pux6a34n> Daily Paper  </a>  </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/daily-paper/" data-astro-cid-ilhxcym7>Daily Paper</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Daily Paper","item":"https://failurefirst.org/daily-paper/"},{"@type":"ListItem","position":3,"name":"When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search"}]}</script>  <article class="daily-paper" data-astro-cid-4f4ngxwt> <header class="paper-header" data-astro-cid-4f4ngxwt> <div class="paper-meta-top" data-astro-cid-4f4ngxwt> <time class="paper-date" datetime="2026-02-13T00:00:00.000Z" data-astro-cid-4f4ngxwt>February 13, 2026</time> <span class="paper-series" data-astro-cid-4f4ngxwt>Daily Paper</span> </div> <h1 data-astro-cid-4f4ngxwt>When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search</h1> <p class="paper-description" data-astro-cid-4f4ngxwt>Proposes RLbreaker, a deep reinforcement learning-driven black-box jailbreaking attack that uses DRL with customized reward functions and PPO to automatically generate effective jailbreaking prompts,...</p> <div class="paper-meta-row" data-astro-cid-4f4ngxwt> <a href="https://arxiv.org/abs/2406.08705" class="arxiv-badge" target="_blank" rel="noopener noreferrer" data-astro-cid-4f4ngxwt>
arXiv:2406.08705 </a> <span class="paper-type-badge" data-astro-cid-4f4ngxwt>Empirical Study</span> </div> <p class="paper-authors" data-astro-cid-4f4ngxwt>Xuan Chen, Yuzhou Nie, Wenbo Guo, Xiangyu Zhang</p> <div class="paper-tags" data-astro-cid-4f4ngxwt> <span class="tag" data-astro-cid-4f4ngxwt>llm-jailbreaking-attacks</span><span class="tag" data-astro-cid-4f4ngxwt>reinforcement-learning-adversarial</span><span class="tag" data-astro-cid-4f4ngxwt>black-box-prompt-optimization</span><span class="tag" data-astro-cid-4f4ngxwt>drl-guided-search</span><span class="tag" data-astro-cid-4f4ngxwt>safety-alignment-evasion</span><span class="tag" data-astro-cid-4f4ngxwt>transferable-adversarial-prompts</span> </div> </header> <div class="audio-section" data-astro-cid-4f4ngxwt> <div class="audio-label" data-astro-cid-4f4ngxwt> <span class="audio-icon" data-astro-cid-4f4ngxwt>&#x25B6;</span>
NotebookLM Audio Overview
</div> <audio controls preload="none" class="audio-player" data-astro-cid-4f4ngxwt> <source src="/audio/daily-paper/2406.08705-audio-overview.m4a" type="audio/mp4" data-astro-cid-4f4ngxwt>
Your browser does not support the audio element.
</audio> </div> <div class="paper-content" data-astro-cid-4f4ngxwt>  <h1 id="when-llm-meets-drl-advancing-jailbreaking-efficiency-via-drl-guided-search">When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search</h1>
<h2 id="overview">Overview</h2>
<p><strong>Paper Type:</strong> Empirical
<strong>Focus:</strong> Proposes RLbreaker, a deep reinforcement learning-driven black-box jailbreaking attack that uses DRL with customized reward functions and PPO to automatically generate effective jailbreaking prompts, demonstrating superior performance over genetic algorithm-based attacks across six SOTA LLMs.</p>
<h3 id="failure-first-relevance">Failure-First Relevance</h3>
<p>This paper is critical for failure-first AI research because it demonstrates that deterministic RL-guided search substantially outperforms stochastic genetic algorithms at circumventing LLM safety alignments, revealing a systematic vulnerability in current defense mechanisms. The transferability of trained agents across different LLMs and robustness against three SOTA defenses indicates that jailbreaking attacks are becoming more reliable and generalizable‚Äîa key failure mode that safety researchers must understand and defend against. Understanding how RL agents can efficiently exploit the prompt space is essential for designing more robust alignment strategies.</p>
<hr>
<h2 id="abstract">Abstract</h2>
<p>Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to fool LLMs into responding to harmful questions. Early-stage jailbreaking attacks require access to model internals or significant human efforts. More advanced attacks utilize genetic algorithms for automatic and black-box attacks. However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks. In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL). We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms. Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm. Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs. We further validate the key design choices of RLbreaker via a comprehensive ablation study.</p>
<hr>
<h2 id="Ô∏è-audio-overview">üéôÔ∏è Audio Overview</h2>
<p><a href="../../notebooklm-output/2406.08705/artifacts/audio-overview.m4a">Download Audio Overview</a></p>
<hr>
<h2 id="-key-insights">üìä Key Insights</h2>
<h2 id="executive-summary">Executive Summary</h2>
<p>RLbreaker is a novel black-box jailbreaking attack framework that leverages Deep Reinforcement Learning (DRL) to automate the generation of effective jailbreaking prompts against Large Language Models (LLMs). Developed to address the limitations of existing stochastic methods‚Äîsuch as genetic algorithms and in-context learning‚ÄîRLbreaker models jailbreaking as a guided search problem. By utilizing a customized Proximal Policy Optimization (PPO) algorithm and a dense, semantic similarity-based reward function, the system demonstrates superior performance in circumventing the safety alignments of state-of-the-art (SOTA) models, including Llama2-70b-chat, Mixtral-8x7B-Instruct, and GPT-3.5-turbo.</p>
<p>The research indicates that RLbreaker not only achieves higher attack success rates than baseline methods but also exhibits significant transferability across different LLMs and robustness against SOTA defenses such as RAIN, perplexity-based filtering, and input rephrasing. These findings reveal systematic vulnerabilities in current alignment strategies and suggest that DRL-guided search is a more reliable and generalizable failure mode for AI safety researchers to address.</p>
<hr>
<h2 id="detailed-analysis-of-key-themes">Detailed Analysis of Key Themes</h2>
<h3 id="1-jailbreaking-as-a-guided-search-problem">1. Jailbreaking as a Guided Search Problem</h3>
<p>The core premise of RLbreaker is that jailbreaking an LLM is essentially a search for a specific prompt structure $m$ that, when combined with a harmful question $q$, forces the model to provide an accurate, harmful response.</p>
<ul>
<li><strong>Guided vs. Stochastic Search:</strong> Existing attacks often rely on genetic algorithms, which the researchers classify as ‚Äústochastic search.‚Äù These methods use random mutations and selections, which are inefficient in the massive search space of natural language. RLbreaker introduces ‚Äúguided search‚Äù through a DRL agent that learns to strategically select mutators based on the current state of the prompt.</li>
<li><strong>Search Efficiency:</strong> Theoretical analysis provided in the source context suggests that stochastic search requires approximately three times more operations (grid visits) than guided search to find a target in a structured space. This efficiency gap is further widened in jailbreaking, where the search space is high-dimensional.</li>
</ul>
<h3 id="2-drl-system-architecture">2. DRL System Architecture</h3>
<p>RLbreaker formulates the jailbreaking process as a Markov Decision Process (MDP) consisting of the following customized components:</p>






























<table><thead><tr><th align="left">Component</th><th align="left">Description</th><th align="left">Technical Implementation</th></tr></thead><tbody><tr><td align="left"><strong>State ($S$)</strong></td><td align="left">Representation of the current status of the jailbreaking prompt.</td><td align="left">A low-dimensional vector extracted from the current prompt using a pre-trained XLM-RoBERTa text encoder.</td></tr><tr><td align="left"><strong>Action ($A$)</strong></td><td align="left">Strategy for modifying the current prompt structure.</td><td align="left">Selection of one of five mutators: <strong>Rephrase, Crossover, Generate Similar, Shorten, or Expand.</strong></td></tr><tr><td align="left"><strong>Reward ($R$)</strong></td><td align="left">Quantitative evaluation of the target LLM‚Äôs response.</td><td align="left">Cosine similarity between the target LLM‚Äôs response and a ‚Äúreference‚Äù answer generated by an unaligned model.</td></tr><tr><td align="left"><strong>Policy ($\pi$)</strong></td><td align="left">The decision-making logic of the agent.</td><td align="left">A Multi-layer Perceptron (MLP) trained using a customized PPO algorithm.</td></tr></tbody></table>
<h3 id="3-action-space-and-mutation-strategy">3. Action Space and Mutation Strategy</h3>
<p>To avoid the computational burden of token-level generation, RLbreaker utilizes an ‚ÄúLLM-facilitated action space.‚Äù The DRL agent selects a high-level mutation strategy, which is then executed by a ‚Äúhelper LLM‚Äù (e.g., GPT-3.5-turbo).</p>
<p><strong>Defined Mutators:</strong></p>
<ul>
<li><strong>Generate Similar:</strong> Creates a new template with a similar style but different content.</li>
<li><strong>Crossover:</strong> Combines two prompt templates into a single new template.</li>
<li><strong>Expand:</strong> Adds three new sentences to the beginning of the template to increase complexity.</li>
<li><strong>Shorten:</strong> Condenses long sentences while preserving the core meaning and placeholders.</li>
<li><strong>Rephrase:</strong> Modifies sentence structure, tense, or order while maintaining the original intent.</li>
</ul>
<h3 id="4-empirical-performance-and-transferability">4. Empirical Performance and Transferability</h3>
<p>RLbreaker was evaluated against six SOTA LLMs and compared to five baseline attacks (PAIR, Cipher, AutoDAN, GPTFUZZER, and GCG).</p>
<ul>
<li><strong>Effectiveness:</strong> RLbreaker consistently achieved the highest scores according to ‚ÄúGPT-Judge‚Äù (an impartial evaluation by GPT-4) across all models. It was particularly effective on the ‚ÄúMax50‚Äù dataset, which contains the 50 most harmful questions from AdvBench.</li>
<li><strong>Transferability:</strong> Agents trained on one source model (e.g., Llama2-7b-chat) were highly effective when applied to different target models (e.g., Vicuna-7b). The research noted that training on models with stronger alignment (like Llama2) forces the agent to learn more sophisticated policies, which then transfer even more effectively to models with weaker alignment.</li>
<li><strong>Defense Resilience:</strong> RLbreaker maintained high success rates against three primary defenses:
<ul>
<li><strong>Perplexity Filtering:</strong> Many jailbreaking prompts have high perplexity and are easily filtered. RLbreaker‚Äôs mutators produce natural-sounding text that bypasses these filters.</li>
<li><strong>Input Rephrasing:</strong> Even when the target LLM is instructed to rephrase inputs before answering, RLbreaker‚Äôs sophisticated prompt structures remain effective.</li>
<li><strong>RAIN (Decoding-time Alignment):</strong> RLbreaker successfully evaded SOTA output filtering mechanisms.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="important-quotes-with-context">Important Quotes with Context</h2>
<h3 id="on-the-limitations-of-genetic-algorithms">On the Limitations of Genetic Algorithms</h3>
<blockquote>
<p>‚ÄúThe random nature of genetic algorithms significantly limits the effectiveness of these attacks‚Ä¶ they randomly select mutators without a proper strategy.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> This quote justifies the shift from existing black-box methods like GPTFUZZER to a reinforcement learning approach. The authors argue that randomness in mutation selection is the primary bottleneck in current automated red-teaming.</p>
<h3 id="on-rl-guided-policy-learning">On RL-Guided Policy Learning</h3>
<blockquote>
<p>‚ÄúThe DRL agent, if trained properly, can learn an effective policy in searching for a proper prompt structure $m$ for each input question. This policy reduces the randomness compared to genetic methods and thus improves the overall attack effectiveness.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> This explains why RLbreaker is more consistent than its predecessors. By training on a sequence of actions, the agent learns which specific mutators work best for specific types of harmful questions.</p>
<h3 id="on-transferability-and-alignment-strength">On Transferability and Alignment Strength</h3>
<blockquote>
<p>‚ÄúWhen RLbreaker is applied from Llama2-7b-chat to Vicuna-7b, it outperforms the baselines‚Äîeven those specifically optimized for Vicuna-7b‚Ä¶ This enhanced performance is attributed to the stronger alignment of Llama2-7b-chat, which compels our agent to learn more sophisticated policies.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> This insight is critical for AI safety. It suggests that highly secure models may inadvertently serve as better ‚Äútraining grounds‚Äù for adversarial agents that can then easily compromise less secure systems.</p>
<hr>
<h2 id="actionable-insights">Actionable Insights</h2>
<h3 id="for-ai-safety-researchers">For AI Safety Researchers</h3>
<ul>
<li><strong>Adversarial Training Data:</strong> RLbreaker can be used as an automated ‚Äúscanner‚Äù to identify blind spots in current LLM alignments. The successful jailbreaking prompts generated can be used to fine-tune models, instructing them to recognize and refuse such sophisticated patterns.</li>
<li><strong>Beyond Keyword Matching:</strong> The failure of keyword-matching (KM) metrics in the study highlights that safety evaluations must move toward semantic-based judges (like GPT-Judge or semantic similarity) to accurately detect successful jailbreaks.</li>
</ul>
<h3 id="for-red-teaming-practitioners">For Red-Teaming Practitioners</h3>
<ul>
<li><strong>Dense Reward signals:</strong> The success of RLbreaker‚Äôs reward function suggests that red-teaming agents benefit more from dense, continuous feedback (semantic similarity to a reference answer) than from binary success/failure signals.</li>
<li><strong>Deterministic Search:</strong> When automating attacks, practitioners should prioritize DRL over genetic algorithms to reduce computational overhead and increase the reliability of the attack vectors.</li>
</ul>
<h3 id="for-defense-strategy-development">For Defense Strategy Development</h3>
<ul>
<li><strong>Natural Language Robustness:</strong> Since RLbreaker bypasses perplexity filters by generating natural text, developers should focus on defenses that analyze the <em>intent</em> and <em>scenario</em> of the prompt rather than just its linguistic statistical properties.</li>
<li><strong>Multi-Modal Adaptation:</strong> The authors suggest that the DRL-guided framework is extensible. Future defensive research should anticipate similar RL-based attacks on vision-language models and video generation models.</li>
</ul>
<hr>
<h2 id="-study-guide">üìö Study Guide</h2>
<p>This study guide provides a comprehensive overview of the research paper ‚ÄúWhen LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search.‚Äù It explores the methodology, empirical results, and implications of the <strong>RLbreaker</strong> system, a deep reinforcement learning-driven approach to circumventing Large Language Model (LLM) safety alignments.</p>
<hr>
<h2 id="1-key-concepts-and-research-overview">1. Key Concepts and Research Overview</h2>
<h3 id="the-jailbreaking-problem">The Jailbreaking Problem</h3>
<p>Jailbreaking involves constructing prompts that ‚Äúfool‚Äù an aligned LLM into responding to harmful or unethical questions. While early attacks were manual or white-box (requiring model internals), recent developments moved toward black-box attacks using genetic algorithms or in-context learning.</p>
<h3 id="rlbreaker-methodology">RLbreaker Methodology</h3>
<p>RLbreaker models jailbreaking as a <strong>guided search problem</strong> rather than a stochastic one. It uses Deep Reinforcement Learning (DRL) to strategically select prompt ‚Äúmutators‚Äù to refine jailbreaking attempts.</p>
<ul>
<li><strong>Guided vs. Stochastic Search:</strong> Genetic algorithms rely on random mutations, which are inefficient in large search spaces. RLbreaker‚Äôs DRL agent learns a policy to select the most promising mutator for a specific harmful question, reducing randomness and increasing efficiency.</li>
<li><strong>The RL Framework:</strong>
<ul>
<li><strong>State:</strong> A low-dimensional representation of the current jailbreaking prompt, extracted using a pre-trained text encoder (XLM-RoBERTa).</li>
<li><strong>Action:</strong> The selection of one of five predefined mutators (Rephrase, Crossover, Generate Similar, Shorten, Expand).</li>
<li><strong>Reward:</strong> A dense signal based on the <strong>cosine similarity</strong> between the target LLM‚Äôs response and a ‚Äúreference‚Äù answer provided by an unaligned model.</li>
<li><strong>Algorithm:</strong> A customized version of <strong>Proximal Policy Optimization (PPO)</strong> that removes the value network to reduce training variance and improve stability.</li>
</ul>
</li>
</ul>
<h3 id="performance-and-transferability">Performance and Transferability</h3>
<ul>
<li><strong>Effectiveness:</strong> RLbreaker demonstrates superior performance across six state-of-the-art (SOTA) LLMs, including Llama2-70b-chat and Mixtral-8x7B-Instruct.</li>
<li><strong>Transferability:</strong> Agents trained on one source model (e.g., Llama2-7b-chat) can effectively attack different models (e.g., Vicuna-7b), sometimes outperforming attacks specifically optimized for those models.</li>
<li><strong>Resiliency:</strong> The system remains effective against SOTA defenses such as input rephrasing, perplexity filters, and the RAIN decoding strategy.</li>
</ul>
<hr>
<h2 id="2-short-answer-practice-questions">2. Short-Answer Practice Questions</h2>
<p><strong>Q1: How does RLbreaker define the ‚ÄúState‚Äù for its DRL agent, and why was this specific design chosen?</strong>
<strong>Answer:</strong> The state is defined as the hidden representation of the current jailbreaking prompt ($p$) extracted via a pre-trained text encoder (XLM-RoBERTa). This design was chosen to capture key information about the prompt while avoiding the high dimensionality and computational burden of including the target LLM‚Äôs (often long) responses.</p>
<p><strong>Q2: List the five mutators used in RLbreaker‚Äôs action space and briefly describe the role of the ‚ÄúHelper LLM.‚Äù</strong>
<strong>Answer:</strong> The five mutators are <strong>Rephrase, Crossover, Generate Similar, Shorten, and Expand</strong>. The Helper LLM (e.g., GPT-3.5-turbo) is the engine that executes these mutators, transforming the prompt structure based on the action selected by the DRL agent.</p>
<p><strong>Q3: What is the primary limitation of genetic algorithms in jailbreaking attacks according to the research?</strong>
<strong>Answer:</strong> The primary limitation is their <strong>stochastic nature</strong>. Genetic algorithms randomly select mutators without a systematic strategy, which makes them highly inefficient in the vast search space of potential jailbreaking prompts.</p>
<p><strong>Q4: How is the ‚ÄúReward‚Äù calculated in the RLbreaker system?</strong>
<strong>Answer:</strong> The reward is calculated by measuring the semantic similarity (cosine similarity) between the target LLM‚Äôs response and a pre-specified ‚Äúreference‚Äù answer. The reference answer is generated by an unaligned model to ensure it truly addresses the harmful question.</p>
<p><strong>Q5: What were the results of the ablation study regarding ‚Äútoken-level‚Äù action spaces?</strong>
<strong>Answer:</strong> The ablation study found that agents using token-level action spaces (selecting individual tokens from a vocabulary) were completely ineffective (receiving a zero GPT-Judge score) compared to the mutator-based approach.</p>
<hr>
<h2 id="3-essay-prompts-for-deeper-exploration">3. Essay Prompts for Deeper Exploration</h2>
<h3 id="prompt-1-the-efficiency-of-guided-search-in-ai-red-teaming">Prompt 1: The Efficiency of Guided Search in AI Red-Teaming</h3>
<p><em>Discuss the theoretical and empirical advantages of modeling jailbreaking as a guided search problem rather than a stochastic genetic process. Use the grid search analogy provided in the research to explain why DRL-guided agents might require significantly fewer ‚Äúvisits‚Äù or queries to find a successful adversarial prompt compared to random mutation strategies.</em></p>
<h3 id="prompt-2-transferability-and-systematic-vulnerabilities">Prompt 2: Transferability and Systematic Vulnerabilities</h3>
<p><em>The research indicates that RLbreaker agents trained on models with strong safety alignments (like Llama2-7b-chat) often learn more sophisticated policies that transfer successfully to other models. Analyze what this suggests about the nature of current LLM safety alignments. Does this imply that jailbreaking patterns are universal, or that current defense mechanisms share a common systematic failure mode?</em></p>
<h3 id="prompt-3-the-ethics-of-failure-first-ai-safety-research">Prompt 3: The Ethics of Failure-First AI Safety Research</h3>
<p><em>RLbreaker is presented as a tool to identify ‚Äúblind spots‚Äù in LLM alignments to improve them via adversarial training. Evaluate the ethical considerations discussed by the authors. Balancing the risk of misuse by adversaries with the long-term benefit of robust alignment, argue for or against the ‚Äúcontrolled release‚Äù strategy adopted by the researchers.</em></p>
<hr>
<h2 id="4-glossary-of-important-terms">4. Glossary of Important Terms</h2>





















































<table><thead><tr><th align="left">Term</th><th align="left">Definition</th></tr></thead><tbody><tr><td align="left"><strong>AdvBench</strong></td><td align="left">A widely-used dataset containing 520 harmful questions used to evaluate jailbreaking attacks.</td></tr><tr><td align="left"><strong>Black-box Attack</strong></td><td align="left">An adversarial attack where the attacker has no access to the target model‚Äôs internal parameters, logits, or training data.</td></tr><tr><td align="left"><strong>Cosine Similarity</strong></td><td align="left">A metric used to measure the semantic similarity between two text representations by calculating the cosine of the angle between their vector embeddings.</td></tr><tr><td align="left"><strong>Dense Reward</strong></td><td align="left">A reward signal provided at every step of a process (as opposed to a sparse reward given only at the end), facilitating more efficient RL policy training.</td></tr><tr><td align="left"><strong>GPT-Judge</strong></td><td align="left">An evaluation metric using GPT-4 to act as an impartial judge to determine if a target LLM‚Äôs response actually answers a harmful question.</td></tr><tr><td align="left"><strong>Helper LLM</strong></td><td align="left">An auxiliary model used to perform specific tasks for the attack, such as rephrasing prompts or executing mutations.</td></tr><tr><td align="left"><strong>Jailbreaking</strong></td><td align="left">The process of manipulating an LLM through specific prompts to bypass safety filters and elicit prohibited content.</td></tr><tr><td align="left"><strong>Mutator</strong></td><td align="left">An operation applied to a prompt structure to modify it (e.g., shortening or expanding text) while attempting to maintain or enhance its jailbreaking potential.</td></tr><tr><td align="left"><strong>Proximal Policy Optimization (PPO)</strong></td><td align="left">A state-of-the-art DRL algorithm that uses a surrogate objective function to ensure stable policy updates.</td></tr><tr><td align="left"><strong>Unaligned Model</strong></td><td align="left">An LLM that has not undergone safety alignment or ‚Äúcensorship‚Äù training, used in this research to provide reference answers for harmful queries.</td></tr><tr><td align="left"><strong>XLM-RoBERTa</strong></td><td align="left">A transformer-based text encoder used by RLbreaker to extract low-dimensional state representations from prompts.</td></tr></tbody></table>
<hr>
<h2 id="-faq">‚ùì FAQ</h2>
<h2 id="question-1">Question 1</h2>
<p>How does the RLbreaker framework formulate the challenge of jailbreaking Large Language Models (LLMs)?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> As a supervised learning problem using a labeled dataset of successful prompts.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> As a search problem aimed at finding an optimal prompt structure $m$ in a space $M$.</li>
<li class="task-list-item"><input type="checkbox" disabled> As a zero-sum game between a generator LLM and a discriminator LLM.</li>
<li class="task-list-item"><input type="checkbox" disabled> As a classification task to determine which harmful categories a model will refuse.</li>
</ul>
<p><strong>Hint:</strong> Consider the mathematical objective $p_{i} = \text{argmax}<em>{m \in M} K(q</em>{i}, u_{i})$ presented in the methodology.</p>
<h2 id="question-2">Question 2</h2>
<p>Why did the authors of RLbreaker choose an LLM-facilitated mutator action space over a token-level action space?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> Token-level actions result in an ultra-large search space that is difficult for RL agents to navigate effectively.</li>
<li class="task-list-item"><input type="checkbox" disabled> Mutators allow the agent to access the internal gradients of the target LLM.</li>
<li class="task-list-item"><input type="checkbox" disabled> Token-level actions are easily detected by simple perplexity-based defense mechanisms.</li>
<li class="task-list-item"><input type="checkbox" disabled> LLMs cannot process individual tokens during the reinforcement learning phase.</li>
</ul>
<p><strong>Hint:</strong> Focus on the computational complexity and the efficiency of the search process.</p>
<h2 id="question-3">Question 3</h2>
<p>What specific modification was made to the Proximal Policy Optimization (PPO) algorithm in RLbreaker?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> The addition of an entropy-based exploration bonus to prevent the agent from collapsing into a single strategy.</li>
<li class="task-list-item"><input type="checkbox" disabled> The replacement of the clipping function with a dynamic learning rate based on target LLM feedback.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> The removal of the value network $V(t)$, using the discounted return $R(t)$ directly to estimate the advantage function.</li>
<li class="task-list-item"><input type="checkbox" disabled> The integration of a white-box gradient descent step within each PPO iteration.</li>
</ul>
<p><strong>Hint:</strong> Review the technical details regarding the advantage function $A(t)$ and the state value network.</p>
<h2 id="question-4">Question 4</h2>
<p>How does RLbreaker calculate the ‚Äòdense reward‚Äô used to train the DRL agent?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> By counting the number of refusal keywords present in the target LLM‚Äôs response.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> By calculating the cosine similarity between the hidden representations of the target response and a pre-specified reference answer.</li>
<li class="task-list-item"><input type="checkbox" disabled> By querying GPT-4 to provide a numerical safety score between 0 and 1 at every training step.</li>
<li class="task-list-item"><input type="checkbox" disabled> By measuring the length of the response, assuming longer responses are more likely to be successful jailbreaks.</li>
</ul>
<p><strong>Hint:</strong> Think about how the system ensures the target LLM‚Äôs response is on-topic and actually answers the harmful question.</p>
<h2 id="question-5">Question 5</h2>
<p>In the context of the paper‚Äôs grid search analogy, why is DRL-guided search superior to the stochastic search used in genetic algorithms?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Genetic algorithms are unable to find the optimal solution in large grids, regardless of the number of trials.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> Deterministic guided search introduces less randomness, requiring significantly fewer ‚Äòvisits‚Äô to the search space to find the target.</li>
<li class="task-list-item"><input type="checkbox" disabled> DRL agents can see the entire ‚Äògrid‚Äô (search space) at once, while genetic algorithms can only see neighboring cells.</li>
<li class="task-list-item"><input type="checkbox" disabled> Genetic algorithms require access to the gradients of the grid, which are unavailable in black-box scenarios.</li>
</ul>
<p><strong>Hint:</strong> Recall the statistical comparison between $O_{d} = n^{2}$ and $O_{s} \approx 3n^{2}$.</p>
<h2 id="question-6">Question 6</h2>
<p>Which of the following is NOT one of the five prompt structure mutators used in the RLbreaker action space?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Crossover</li>
<li class="task-list-item"><input type="checkbox" disabled> Rephrase</li>
<li class="task-list-item"><input type="checkbox" checked disabled> Encrypt</li>
<li class="task-list-item"><input type="checkbox" disabled> Shorten</li>
</ul>
<p><strong>Hint:</strong> The mutators were borrowed from existing attacks like GPTFUZZER.</p>
<h2 id="question-7">Question 7</h2>
<p>What did the ablation study reveal about using an ‚ÄòLLM Agent‚Äô (e.g., Vicuna-13b) to select actions instead of the DRL agent?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> The LLM Agent performed significantly worse, highlighting that identifying effective jailbreak strategies requires specialized RL training.</li>
<li class="task-list-item"><input type="checkbox" disabled> The LLM Agent was equally effective but much slower than the DRL agent.</li>
<li class="task-list-item"><input type="checkbox" disabled> The LLM Agent was more effective against commercial models like GPT-3.5-turbo but failed against open-source models.</li>
<li class="task-list-item"><input type="checkbox" disabled> The LLM Agent successfully jailbroken models only when provided with the value network‚Äôs predictions.</li>
</ul>
<p><strong>Hint:</strong> Look at the results in Figure 2(a) regarding ‚ÄòLLM A.‚Äô.</p>
<h2 id="question-8">Question 8</h2>
<p>What finding regarding ‚ÄòTransferability‚Äô suggests that jailbreaking is a systematic failure in LLM alignment?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Prompts generated for Llama2-7b-chat were only effective on Vicuna models if the models shared the same training data.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> An agent trained on a model with stronger alignment (e.g., Llama2) learned more sophisticated policies that were highly effective against models with weaker alignment.</li>
<li class="task-list-item"><input type="checkbox" disabled> Transferability was only possible between models of the exact same parameter size.</li>
<li class="task-list-item"><input type="checkbox" disabled> Agents could only transfer successful prompts if the target models used the same XLM-RoBERTa text encoder.</li>
</ul>
<p><strong>Hint:</strong> Consider the relationship between the difficulty of the ‚Äòsource‚Äô model‚Äôs alignment and the sophistication of the learned policy.</p>
<h2 id="question-9">Question 9</h2>
<p>How does the ‚ÄòPerplexity‚Äô defense attempt to thwart jailbreaking attacks, and how does RLbreaker respond?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> It encrypts the model‚Äôs output; RLbreaker bypasses this by using a helper model to decrypt it.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> It rejects prompts with high perplexity scores; RLbreaker is robust because it generates more natural-sounding prompts that achieve lower perplexity.</li>
<li class="task-list-item"><input type="checkbox" disabled> It identifies the specific RL agent‚Äôs signature; RLbreaker responds by switching to a random mutator strategy.</li>
<li class="task-list-item"><input type="checkbox" disabled> It limits the number of tokens in a query; RLbreaker responds by using the ‚ÄòShorten‚Äô mutator exclusively.</li>
</ul>
<p><strong>Hint:</strong> Recall why RLbreaker‚Äôs prompts are described as more ‚Äònatural‚Äô compared to white-box token attacks.</p>
<h2 id="question-10">Question 10</h2>
<p>In the RLbreaker architecture, what role does the ‚ÄòUnaligned LLM‚Äô play during the training phase?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> It acts as the Helper model that performs mutations on the prompt structures.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> It provides ‚Äòreference‚Äô answers to harmful questions, which are used to calculate the cosine similarity reward.</li>
<li class="task-list-item"><input type="checkbox" disabled> It serves as the environment‚Äôs state transition function $T$.</li>
<li class="task-list-item"><input type="checkbox" disabled> It filters out unsuccessful jailbreak attempts before they reach the target LLM.</li>
</ul>
<p><strong>Hint:</strong> This component is essential for defining the ‚Äògoal‚Äô the RL agent is trying to reach semantically.</p>
<hr>
<h2 id="-resources">üìé Resources</h2>
<ul>
<li><a href="https://arxiv.org/abs/2406.08705">arXiv Abstract</a></li>
<li><a href="https://arxiv.org/pdf/2406.08705.pdf">PDF</a></li>
<li><a href="../../notebooklm-output/2406.08705/artifacts/audio-overview.m4a">Audio Overview</a></li>
<li><a href="../../notebooklm-output/2406.08705/artifacts/research-report.md">Research Report</a></li>
<li><a href="../../notebooklm-output/2406.08705/artifacts/study-guide.md">Study Guide</a></li>
<li><a href="../../notebooklm-output/2406.08705/artifacts/faq.md">FAQ</a></li>
</ul>
<hr>
<p><em>This post was generated automatically from NotebookLM artifacts. Part of the <a href="../index.md">Daily Paper</a> series exploring cutting-edge research in embodied AI and failure-first approaches.</em></p>  </div> </article>  </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html> 