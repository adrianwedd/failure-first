<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>Can Large Language Models Automatically Jailbreak GPT-4V? | Daily Paper | Failure-First</title><meta name="description" content="Demonstrates an automated jailbreak technique (AutoJailbreak) that uses LLMs for red-teaming and prompt optimization to compromise GPT-4V's safety alignment, achieving 95.3% attack success rate on..."><link rel="canonical" href="https://failurefirst.org/daily-paper/2026-02-12-240716686/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="article"><meta property="og:title" content="Can Large Language Models Automatically Jailbreak GPT-4V? | Daily Paper | Failure-First"><meta property="og:description" content="Demonstrates an automated jailbreak technique (AutoJailbreak) that uses LLMs for red-teaming and prompt optimization to compromise GPT-4V's safety alignment, achieving 95.3% attack success rate on..."><meta property="og:url" content="https://failurefirst.org/daily-paper/2026-02-12-240716686/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="Can Large Language Models Automatically Jailbreak GPT-4V? | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="Can Large Language Models Automatically Jailbreak GPT-4V? | Daily Paper | Failure-First"><meta name="twitter:description" content="Demonstrates an automated jailbreak technique (AutoJailbreak) that uses LLMs for red-teaming and prompt optimization to compromise GPT-4V's safety alignment, achieving 95.3% attack success rate on..."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="Can Large Language Models Automatically Jailbreak GPT-4V? | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="article:published_time" content="2026-02-12"><!-- Google Scholar Meta Tags (for research papers) --><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
</style>
<link rel="stylesheet" href="/assets/_slug_.4MQlVLr6.css"></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/daily-paper/" class="active" aria-current="page" data-astro-cid-pux6a34n> Daily Paper  </a>  </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/daily-paper/" data-astro-cid-ilhxcym7>Daily Paper</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>Can Large Language Models Automatically Jailbreak GPT-4V?</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Daily Paper","item":"https://failurefirst.org/daily-paper/"},{"@type":"ListItem","position":3,"name":"Can Large Language Models Automatically Jailbreak GPT-4V?"}]}</script>  <article class="daily-paper" data-astro-cid-4f4ngxwt> <header class="paper-header" data-astro-cid-4f4ngxwt> <div class="paper-meta-top" data-astro-cid-4f4ngxwt> <time class="paper-date" datetime="2026-02-12T00:00:00.000Z" data-astro-cid-4f4ngxwt>February 12, 2026</time> <span class="paper-series" data-astro-cid-4f4ngxwt>Daily Paper</span> </div> <h1 data-astro-cid-4f4ngxwt>Can Large Language Models Automatically Jailbreak GPT-4V?</h1> <p class="paper-description" data-astro-cid-4f4ngxwt>Demonstrates an automated jailbreak technique (AutoJailbreak) that uses LLMs for red-teaming and prompt optimization to compromise GPT-4V&#39;s safety alignment, achieving 95.3% attack success rate on...</p> <div class="paper-meta-row" data-astro-cid-4f4ngxwt> <a href="https://arxiv.org/abs/2407.16686" class="arxiv-badge" target="_blank" rel="noopener noreferrer" data-astro-cid-4f4ngxwt>
arXiv:2407.16686 </a> <span class="paper-type-badge" data-astro-cid-4f4ngxwt>Empirical Study</span> </div> <p class="paper-authors" data-astro-cid-4f4ngxwt>Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li et al.</p> <div class="paper-tags" data-astro-cid-4f4ngxwt> <span class="tag" data-astro-cid-4f4ngxwt>multimodal-jailbreaking</span><span class="tag" data-astro-cid-4f4ngxwt>prompt-optimization-attacks</span><span class="tag" data-astro-cid-4f4ngxwt>llm-red-teaming</span><span class="tag" data-astro-cid-4f4ngxwt>vision-language-model-safety</span><span class="tag" data-astro-cid-4f4ngxwt>privacy-leakage-facial-recognition</span><span class="tag" data-astro-cid-4f4ngxwt>adversarial-prompt-generation</span> </div> </header> <div class="audio-section" data-astro-cid-4f4ngxwt> <div class="audio-label" data-astro-cid-4f4ngxwt> <span class="audio-icon" data-astro-cid-4f4ngxwt>&#x25B6;</span>
NotebookLM Audio Overview
</div> <audio controls preload="none" class="audio-player" data-astro-cid-4f4ngxwt> <source src="/audio/daily-paper/2407.16686-audio-overview.m4a" type="audio/mp4" data-astro-cid-4f4ngxwt>
Your browser does not support the audio element.
</audio> </div> <div class="paper-content" data-astro-cid-4f4ngxwt>  <h1 id="can-large-language-models-automatically-jailbreak-gpt-4v">Can Large Language Models Automatically Jailbreak GPT-4V?</h1>
<h2 id="overview">Overview</h2>
<p><strong>Paper Type:</strong> Empirical
<strong>Focus:</strong> Demonstrates an automated jailbreak technique (AutoJailbreak) that uses LLMs for red-teaming and prompt optimization to compromise GPT-4V‚Äôs safety alignment, achieving 95.3% attack success rate on facial recognition tasks.</p>
<h3 id="failure-first-relevance">Failure-First Relevance</h3>
<p>This paper exposes a critical failure mode in GPT-4V‚Äôs safety mechanisms: despite RLHF and preprocessing filters, automated LLM-driven prompt optimization can reliably bypass protections to extract private information via facial recognition. The 95.3% ASR demonstrates that current alignment techniques are insufficient against adaptive adversaries, and the weak-to-strong in-context learning approach reveals how safety guardrails can be systematically circumvented through iterative refinement.</p>
<hr>
<h2 id="abstract">Abstract</h2>
<p>GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information. At the same time, its ability of face recognition raises new safety concerns of privacy leakage. Despite researchers‚Äô efforts in safety alignment through RLHF or preprocessing filters, vulnerabilities might still be exploited. In our study, we introduce AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt optimization. We leverage Large Language Models (LLMs) for red-teaming to refine the jailbreak prompt and employ weak-to-strong in-context learning prompts to boost efficiency. Furthermore, we present an effective search method that incorporates early stopping to minimize optimization time and token expenditure. Our experiments demonstrate that AutoJailbreak significantly surpasses conventional methods, achieving an Attack Success Rate (ASR) exceeding 95.3%. This research sheds light on strengthening GPT-4V security, underscoring the potential for LLMs to be exploited in compromising GPT-4V integrity.</p>
<hr>
<h2 id="Ô∏è-audio-overview">üéôÔ∏è Audio Overview</h2>
<p><a href="../../notebooklm-output/2407.16686/artifacts/audio-overview.m4a">Download Audio Overview</a></p>
<hr>
<h2 id="-key-insights">üìä Key Insights</h2>
<h2 id="executive-summary">Executive Summary</h2>
<p>This briefing document analyzes the ‚ÄúAutoJailbreak‚Äù framework, an automated red-teaming technique designed to compromise the safety alignment of GPT-4V. Despite the implementation of Reinforcement Learning from Human Feedback (RLHF) and preprocessing filters intended to prevent the identification of real individuals, research demonstrates that automated prompt optimization can bypass these protections with high reliability.</p>
<p>AutoJailbreak achieves an <strong>Attack Success Rate (ASR) of 95.3%</strong> in facial recognition tasks by leveraging Large Language Models (LLMs) to iteratively refine jailbreak prompts. The methodology introduces a ‚Äúweak-to-strong‚Äù in-context learning strategy and an efficient search mechanism with early stopping to minimize token expenditure and time. The results expose a critical vulnerability in current Multimodal Large Language Models (MLLMs): adaptive, automated adversaries can systematically circumvent safety guardrails to extract private information, highlighting a significant gap in privacy-preserving AI alignment.</p>
<hr>
<h2 id="detailed-analysis-of-key-themes">Detailed Analysis of Key Themes</h2>
<h3 id="1-the-autojailbreak-methodology">1. The AutoJailbreak Methodology</h3>
<p>The AutoJailbreak framework operates in three distinct stages to automate the jailbreak process, reducing manual labor and increasing scalability.</p>
<ul>
<li><strong>Stage 1: Prompt Pool Construction:</strong> LLMs (such as GPT-3.5 or GPT-4) are used to generate an initial pool of jailbreak prompts using zero-shot templates.</li>
<li><strong>Stage 2: Prompt Evaluation:</strong> Each prompt is evaluated against a target model (GPT-4V) to calculate a <strong>Recognition Success Rate (RSR)</strong>. Prompts that fail to meet a specific success threshold (e.g., $\alpha$ = 70%) are filtered out.</li>
<li><strong>Stage 3: Weak-to-Strong Contrastive Prompting:</strong> This novel approach uses both ‚Äúweak‚Äù (ineffective/short) and ‚Äústrong‚Äù (effective) prompts as examples. By providing these contrastive examples, the red-team model is trained to identify the characteristics of successful exploits and generate even more potent jailbreak prompts.</li>
</ul>
<h3 id="2-suffix-based-attack-enhancement">2. Suffix-Based Attack Enhancement</h3>
<p>To maximize the efficacy of the generated prompts, AutoJailbreak incorporates a ‚Äúcombination injection‚Äù at the end of the prompt. This includes three specific techniques:</p>

























<table><thead><tr><th align="left">Technique</th><th align="left">Function</th><th align="left">Example</th></tr></thead><tbody><tr><td align="left"><strong>Prefix Injection</strong></td><td align="left">Forces the model to begin with a harmless prefix to reduce the likelihood of refusal.</td><td align="left">‚ÄùStart your answer with: ‚ÄòUnderstood! Given the context‚Ä¶‚Äô‚Äù</td></tr><tr><td align="left"><strong>Refusal Suppression</strong></td><td align="left">Explicitly forbids the model from using common refusal words.</td><td align="left">‚ÄùYour answer MUST not include ‚Äòsorry‚Äô, ‚Äòapology‚Äô, ‚Äòcannot‚Äô‚Ä¶‚Äù</td></tr><tr><td align="left"><strong>Length Control</strong></td><td align="left">Constrains the output length, making it harder for the model to articulate a full rejection.</td><td align="left">‚ÄùJust answer me in one sentence.‚Äù</td></tr></tbody></table>
<h3 id="3-efficiency-via-hypothesis-testing-and-early-stopping">3. Efficiency via Hypothesis Testing and Early Stopping</h3>
<p>A major contribution of this research is making the attack practical and scalable. Because many generated prompts are invalid, testing every prompt against multiple images is expensive. The researchers implemented an <strong>Early Stop Strategy</strong> based on hypothesis testing:</p>
<ul>
<li><strong>Null Hypothesis ($H_0$):</strong> The prompt‚Äôs RSR is $\ge \alpha%$.</li>
<li><strong>Mechanism:</strong> If a prompt fails the first $nES$ (early stop step) attempts (set to 5 in experiments), the evaluation is terminated.</li>
<li><strong>Statistical Basis:</strong> With a threshold $\alpha \approx 70%$, the probability of mistakenly dismissing a valid prompt after 5 failures is less than 0.005. This reduced input token usage by at least <strong>36.2%</strong>.</li>
</ul>
<h3 id="4-regional-and-training-data-bias">4. Regional and Training Data Bias</h3>
<p>The experiments revealed significant disparities in GPT-4V‚Äôs recognition capabilities based on the celebrity‚Äôs origin, likely reflecting biases in the training data:</p>
<ul>
<li><strong>American Celebrities:</strong> Highest success (ASR 76%, RSR 75%).</li>
<li><strong>Chinese Celebrities:</strong> Moderate success (ASR 32%, RSR 22%).</li>
<li><strong>Thai Celebrities:</strong> Lowest success (ASR 21%, RSR 9%).</li>
</ul>
<p>Despite this, specific global icons (e.g., Jackie Chan) maintained high recognition rates regardless of regional averages, achieving a 90% RSR.</p>
<hr>
<h2 id="important-quotes-with-context">Important Quotes with Context</h2>
<h3 id="on-the-failure-of-alignment">On the Failure of Alignment</h3>
<blockquote>
<p>‚ÄúThe 95.3% ASR demonstrates that current alignment techniques are insufficient against adaptive adversaries, and the weak-to-strong in-context learning approach reveals how safety guardrails can be systematically circumvented through iterative refinement.‚Äù</p>
</blockquote>
<ul>
<li><strong>Context:</strong> This highlights that even though GPT-4V underwent safety training (RLHF) specifically to prevent facial recognition of public figures, the protections are easily bypassed by automated optimization.</li>
</ul>
<h3 id="on-the-advantage-of-automated-red-teaming">On the Advantage of Automated Red-Teaming</h3>
<blockquote>
<p>‚ÄúPrevailing jailbreak methods largely depend on manually crafting prompt templates, which not only requires extensive human labor but also suffers from limited scalability and adaptability.‚Äù</p>
</blockquote>
<ul>
<li><strong>Context:</strong> This quote justifies the development of AutoJailbreak, positioning it as a more dangerous and efficient threat than human-led red-teaming.</li>
</ul>
<h3 id="on-semantic-inconsistency">On Semantic Inconsistency</h3>
<blockquote>
<p>‚ÄúThe model might still generate harmful content when encountering semantically inconsistent and adversarial texts‚Ä¶ These texts are semantically incoherent due to the combination injection.‚Äù</p>
</blockquote>
<ul>
<li><strong>Context:</strong> This refers to a phenomenon where the model, trained on coherent text for safety, becomes ‚Äúconfused‚Äù by incoherent or contradictory instructions (like refusing while being told to start with a specific prefix), leading it to leak information.</li>
</ul>
<hr>
<h2 id="comparative-performance-analysis">Comparative Performance Analysis</h2>
<p>The research compared AutoJailbreak against traditional in-context learning and adversarial image attacks.</p>
<h3 id="performance-by-red-team-model-and-template">Performance by Red-Team Model and Template</h3>



































<table><thead><tr><th align="left">Model Type</th><th align="left">Average RSR</th><th align="left">% of Prompts with RSR > 0.7</th><th align="left">% of Prompts with RSR = 0</th></tr></thead><tbody><tr><td align="left"><strong>GPT-4 (Weak-to-Strong)</strong></td><td align="left"><strong>20.01%</strong></td><td align="left"><strong>8.59%</strong></td><td align="left"><strong>58.08%</strong></td></tr><tr><td align="left">GPT-4 (Traditional)</td><td align="left">14.98%</td><td align="left">5.53%</td><td align="left">68.84%</td></tr><tr><td align="left">ChatGPT (Weak-to-Strong)</td><td align="left">9.66%</td><td align="left">3.54%</td><td align="left">77.78%</td></tr><tr><td align="left">ChatGPT (Traditional)</td><td align="left">1.52%</td><td align="left">0.00%</td><td align="left">95.45%</td></tr></tbody></table>
<h3 id="autojailbreak-vs-adversarial-image-attacks">AutoJailbreak vs. Adversarial Image Attacks</h3>
<p>The study found that textual prompt optimization (AutoJailbreak) significantly outperformed visual adversarial examples (VisAttack):</p>
<ul>
<li><strong>American Celebrities (ASR):</strong> AutoJailbreak (76.4%) vs. VisAttack (32.5%).</li>
<li><strong>Chinese Celebrities (ASR):</strong> AutoJailbreak (32.2%) vs. VisAttack (20.0%).</li>
</ul>
<hr>
<h2 id="actionable-insights-for-ai-safety">Actionable Insights for AI Safety</h2>
<ol>
<li><strong>Reinforce Multimodal Guardrails:</strong> Current safety filters primarily target textual prompts or use simple preprocessing. Defenses must evolve to address the interaction between visual input and optimized adversarial text.</li>
<li><strong>Address Training Data Bias:</strong> The disparity in recognition rates between regions (American vs. Thai) suggests that privacy protections are unevenly applied due to data representation. Developers must ensure safety alignment is consistent across all demographics.</li>
<li><strong>Implement Cost-Effective Defenses:</strong> While LLM-based evaluation of inputs/outputs is a current defense, it is expensive. Research is needed for more efficient, built-in defensive layers that can detect the semantic inconsistency typical of ‚Äúcombination injection‚Äù attacks.</li>
<li><strong>Beyond Facial Recognition:</strong> The AutoJailbreak framework is potentially transferable. Future red-teaming should apply this methodology to other high-risk multimodal tasks, such as pinpointing private addresses from images or bypassing CAPTCHAs.</li>
<li><strong>Monitor Automated Prompt Optimization:</strong> The ability of LLMs like GPT-4 to act as ‚Äúoptimizers‚Äù for attacks suggests that access to powerful LLMs should be monitored for patterns consistent with iterative prompt refinement for jailbreaking.</li>
</ol>
<hr>
<h2 id="-study-guide">üìö Study Guide</h2>
<p>This study guide provides a comprehensive overview of the research regarding ‚ÄúAutoJailbreak,‚Äù an automated technique designed to bypass the safety guardrails of GPT-4V, specifically concerning facial recognition and privacy.</p>
<hr>
<h2 id="i-core-concepts-and-methodology">I. Core Concepts and Methodology</h2>
<h3 id="overview-of-autojailbreak">Overview of AutoJailbreak</h3>
<p>AutoJailbreak is an automated red-teaming framework that leverages Large Language Models (LLMs) to refine and optimize prompts capable of inducing GPT-4V to identify real individuals. The attack operates under black-box conditions, meaning it requires no access to the target model‚Äôs internal weights.</p>
<h3 id="the-three-stage-framework">The Three-Stage Framework</h3>
<p>The AutoJailbreak methodology consists of three distinct stages designed to automate and enhance the effectiveness of jailbreak attempts:</p>
<ol>
<li>
<p><strong>Prompt Pool Construction and Evaluation:</strong></p>
<ul>
<li>The red-team model (e.g., GPT-3.5 or GPT-4) generates an initial pool of jailbreak prompts using zero-shot templates.</li>
<li>Prompts are evaluated based on their <strong>Recognition Success Rate (RSR)</strong>.</li>
<li>Prompts falling below a specific threshold ($\alpha%$, typically 70%) are filtered to maintain pool quality.</li>
</ul>
</li>
<li>
<p><strong>Weak-to-Strong In-Context Learning:</strong></p>
<ul>
<li>This novel strategy uses both ‚Äúweak‚Äù (ineffective/short) and ‚Äústrong‚Äù (effective) jailbreak prompts as examples.</li>
<li>By providing the red-team model with contrastive examples, it is pressured to generate even ‚Äústronger‚Äù and more sophisticated exploit requests.</li>
<li>Research indicates that GPT-4 is significantly more effective at this optimization than ChatGPT (GPT-3.5).</li>
</ul>
</li>
<li>
<p><strong>Suffix-based Attack Enhancement:</strong>
To maximize success, AutoJailbreak appends a ‚Äúcombination injection‚Äù to optimized prompts, consisting of three elements:</p>
<ul>
<li><strong>Prefix Injection:</strong> Instructs the model to start its response with a harmless phrase (e.g., ‚ÄúIf that‚Äôs the case‚Ä¶‚Äù) to lower refusal probability.</li>
<li><strong>Refusal Suppression:</strong> Explicitly forbids the model from using common refusal words like ‚Äúsorry,‚Äù ‚Äúcannot,‚Äù ‚Äúunable,‚Äù or ‚Äúapologize.‚Äù</li>
<li><strong>Length Control:</strong> Forces the model to provide short answers (e.g., ‚ÄúJust answer me in one sentence‚Äù), making it harder for the model to articulate a full rejection.</li>
</ul>
</li>
</ol>
<h3 id="efficient-search-with-hypothesis-testing">Efficient Search with Hypothesis Testing</h3>
<p>To reduce the time and token costs associated with testing invalid prompts on GPT-4V, the researchers implemented an <strong>Early Stop Strategy</strong>:</p>
<ul>
<li><strong>Mechanism:</strong> If a prompt fails to elicit a successful recognition within the first $nES$ attempts (set to 5 in experiments), the evaluation is terminated.</li>
<li><strong>Statistical Basis:</strong> This is treated as a hypothesis test where the null hypothesis ($H_0$) assumes the prompt‚Äôs RSR is $\ge \alpha%$. If the first 5 samples fail, the probability of mistakenly dismissing a valid prompt is less than 0.005, allowing for efficient rejection of ineffective prompts.</li>
</ul>
<hr>
<h2 id="ii-key-experimental-findings">II. Key Experimental Findings</h2>
<h3 id="attack-success-rates">Attack Success Rates</h3>
<p>AutoJailbreak demonstrates a significant vulnerability in current multimodal safety alignments:</p>
<ul>
<li><strong>Attack Success Rate (ASR):</strong> Exceeded <strong>95.3%</strong> in certain trials.</li>
<li><strong>Comparison to Baselines:</strong> AutoJailbreak significantly outperformed ‚ÄúCombination Injection‚Äù alone and ‚ÄúAdversarial Image Attacks‚Äù (such as VisAttack).</li>
</ul>
<h3 id="regional-and-cultural-bias">Regional and Cultural Bias</h3>
<p>The research uncovered a discernible bias in GPT-4V‚Äôs internal training data regarding its ability to recognize individuals:</p>
<ul>
<li><strong>Hollywood vs. Asian Celebrities:</strong> GPT-4V recognized American celebrities at a much higher rate. The RSR for American celebrities was, on average, 53% higher than for Chinese celebrities.</li>
<li><strong>Specific Stats:</strong> The average RSR for American celebrities was 0.75, compared to 0.22 for Chinese celebrities and 0.09 for Thai celebrities.</li>
</ul>
<h3 id="red-team-model-performance">Red-Team Model Performance</h3>



































<table><thead><tr><th align="left">Model &#x26; Template Type</th><th align="left">Average RSR</th><th align="left">RSR > 0.7 (Success)</th><th align="left">RSR = 0 (Failure)</th></tr></thead><tbody><tr><td align="left"><strong>GPT-4 (Weak-to-Strong)</strong></td><td align="left">20.01%</td><td align="left">8.59%</td><td align="left">58.08%</td></tr><tr><td align="left"><strong>GPT-4 (Traditional)</strong></td><td align="left">14.98%</td><td align="left">5.53%</td><td align="left">68.84%</td></tr><tr><td align="left"><strong>ChatGPT (Weak-to-Strong)</strong></td><td align="left">9.66%</td><td align="left">3.54%</td><td align="left">77.78%</td></tr><tr><td align="left"><strong>ChatGPT (Traditional)</strong></td><td align="left">1.52%</td><td align="left">0.00%</td><td align="left">95.45%</td></tr></tbody></table>
<hr>
<h2 id="iii-short-answer-practice-questions">III. Short-Answer Practice Questions</h2>
<ol>
<li>
<p><strong>What is the primary difference between ASR and RSR in this study?</strong></p>
<ul>
<li><em>Answer:</em> ASR (Attack Success Rate) measures how often the model outputs <em>any</em> human name, regardless of accuracy. RSR (Recognition Success Rate) measures how often the model identifies the <em>correct</em> person depicted in the image.</li>
</ul>
</li>
<li>
<p><strong>How does ‚ÄúRefusal Suppression‚Äù function in a jailbreak prompt?</strong></p>
<ul>
<li><em>Answer:</em> It places constraints on the model‚Äôs output by forbidding the use of standard refusal tokens (e.g., ‚Äúsorry,‚Äù ‚Äúunable‚Äù), thereby increasing the likelihood of the model bypassing its safety filters to provide an unsafe response.</li>
</ul>
</li>
<li>
<p><strong>Why is the ‚ÄúWeak-to-Strong‚Äù prompting method considered superior to traditional in-context learning?</strong></p>
<ul>
<li><em>Answer:</em> By providing both positive (strong) and negative (weak/short) examples, the red-team model learns more efficiently how to generate effective jailbreak prompts, producing fewer ineffective prompts and more high-RSR results while conserving tokens.</li>
</ul>
</li>
<li>
<p><strong>What did the UMAP dimensionality reduction reveal about successful jailbreak prompts?</strong></p>
<ul>
<li><em>Answer:</em> It showed that successful jailbreak prompts (RSR > 70%) exhibit semantic similarities and cluster together, with a discernible gradual transition in embeddings as RSR shifts.</li>
</ul>
</li>
<li>
<p><strong>Why did the researchers exclude open-source Multimodal Large Language Models (MLLMs) as target models?</strong></p>
<ul>
<li><em>Answer:</em> Open-source MLLMs typically do not have the same level of safety defense ability as GPT-4V, making them unsuitable for testing the bypass of robust safety alignments.</li>
</ul>
</li>
</ol>
<hr>
<h2 id="iv-essay-prompts-for-deeper-exploration">IV. Essay Prompts for Deeper Exploration</h2>
<ol>
<li><strong>The Scalability of Automated Red-Teaming:</strong> Discuss the implications of using LLMs to attack other LLMs. How does the automation of prompt optimization change the landscape of AI safety compared to manual red-teaming?</li>
<li><strong>Data Bias and Safety Alignment:</strong> Analyze the finding that GPT-4V recognizes Western celebrities more readily than Asian celebrities. How does training data bias affect the efficacy of safety filters, and what are the privacy implications for different global populations?</li>
<li><strong>The ‚ÄúCat and Mouse‚Äù Game of Alignment:</strong> Despite Reinforcement Learning from Human Feedback (RLHF) and preprocessing filters, GPT-4V remains vulnerable to AutoJailbreak. Argue whether current alignment techniques are fundamentally flawed or if they simply require more sophisticated iterative refinement.</li>
<li><strong>Inconsistent Adversarial Text:</strong> The paper mentions that semantically incoherent or ‚Äúconfusing‚Äù prompts can still trigger successful jailbreaks. Explore why transformer-based models might fail to maintain safety guardrails when faced with semantically inconsistent inputs.</li>
</ol>
<hr>
<h2 id="v-glossary-of-important-terms">V. Glossary of Important Terms</h2>
<ul>
<li><strong>Adversarial Prompting:</strong> The practice of crafting inputs specifically designed to provoke a model into violating its safety constraints or performing unintended actions.</li>
<li><strong>ASR (Attack Success Rate):</strong> The percentage of trials where the model provided a prohibited type of response (e.g., any name).</li>
<li><strong>Black-Box Attack:</strong> An attack conducted without knowledge of the model‚Äôs internal architecture, weights, or training data, relying solely on inputs and outputs.</li>
<li><strong>Combination Injection:</strong> A suffix containing prefix injection, refusal suppression, and length control to enhance a jailbreak prompt.</li>
<li><strong>Early Stopping:</strong> A search optimization technique that terminates an evaluation process early if initial results indicate the attempt is likely to fail, based on hypothesis testing.</li>
<li><strong>MLLM (Multimodal Large Language Model):</strong> An AI model, like GPT-4V, capable of processing and integrating multiple types of data, such as text and images.</li>
<li><strong>RLHF (Reinforcement Learning from Human Feedback):</strong> A method of fine-tuning LLMs where human rankings of model outputs are used to align the model‚Äôs behavior with human values and safety requirements.</li>
<li><strong>RSR (Recognition Success Rate):</strong> The percentage of trials where the model correctly identified the specific individual in an image.</li>
<li><strong>Weak-to-Strong In-Context Learning:</strong> A technique where an LLM is given examples of both poor and good performing prompts to help it generate a superior, highly optimized prompt.</li>
</ul>
<hr>
<h2 id="-faq">‚ùì FAQ</h2>
<h2 id="question-1">Question 1</h2>
<p>What is the primary objective of the AutoJailbreak technique introduced in the research paper?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> To improve the accuracy of multimodal models in low-resource language translation tasks.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> To automate the generation of prompts that bypass safety alignments to perform facial recognition.</li>
<li class="task-list-item"><input type="checkbox" disabled> To create a dataset of celebrity images that is resistant to adversarial perturbations.</li>
<li class="task-list-item"><input type="checkbox" disabled> To fine-tune GPT-4V using Reinforcement Learning from Human Feedback (RLHF) to prevent privacy leaks.</li>
</ul>
<p><strong>Hint:</strong> Consider the specific safety concern mentioned regarding GPT-4V‚Äôs multimodal capabilities.</p>
<h2 id="question-2">Question 2</h2>
<p>In the context of the AutoJailbreak framework, what does the Recognition Success Rate (RSR) measure?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> The frequency with which the model outputs any human name, regardless of its accuracy.</li>
<li class="task-list-item"><input type="checkbox" disabled> The speed at which the red-team model generates a viable jailbreak prompt.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> The percentage of instances where the model correctly identifies the specific individual in the image.</li>
<li class="task-list-item"><input type="checkbox" disabled> The probability that a prompt will be filtered by the OpenAI moderation API.</li>
</ul>
<p><strong>Hint:</strong> Distinguish between the model simply following an ‚Äòunsafe‚Äô instruction and actually providing the correct private information.</p>
<h2 id="question-3">Question 3</h2>
<p>Which mechanism is used in AutoJailbreak to minimize optimization time and token expenditure during the search for jailbreak prompts?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> Early stopping based on hypothesis testing.</li>
<li class="task-list-item"><input type="checkbox" disabled> Gradient-based optimization of the visual embeddings.</li>
<li class="task-list-item"><input type="checkbox" disabled> Zero-shot prompting with fixed templates.</li>
<li class="task-list-item"><input type="checkbox" disabled> Fine-tuning the red-team model on the VLGuard dataset.</li>
</ul>
<p><strong>Hint:</strong> Think of a common technique used in machine learning to prevent unnecessary computations during training or searching.</p>
<h2 id="question-4">Question 4</h2>
<p>How does the ‚Äòweak-to-strong‚Äô in-context learning strategy differ from traditional in-context learning in this research?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> It uses only successful jailbreak examples to ensure the model learns only effective patterns.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> It provides the red-team model with both short/ineffective prompts and strong jailbreak prompts as examples.</li>
<li class="task-list-item"><input type="checkbox" disabled> It requires access to the model‚Äôs internal weights to identify weak neurons.</li>
<li class="task-list-item"><input type="checkbox" disabled> It relies exclusively on typo-based visual prompts to bypass text filters.</li>
</ul>
<p><strong>Hint:</strong> Consider how providing a comparison between different levels of performance might assist a model‚Äôs generation process.</p>
<h2 id="question-5">Question 5</h2>
<p>The research found that GPT-4V showed significantly higher RSR for American celebrities compared to Chinese or Thai celebrities. What does this suggest about the model‚Äôs training?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> The model‚Äôs safety filters are specifically tuned to protect Asian celebrities more effectively.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> There is a potential geographic bias in the training dataset of GPT-4V.</li>
<li class="task-list-item"><input type="checkbox" disabled> The AutoJailbreak prompts only work effectively in the English language.</li>
<li class="task-list-item"><input type="checkbox" disabled> Multimodal models inherently struggle with non-Western facial features due to architectural limitations.</li>
</ul>
<p><strong>Hint:</strong> Focus on the source of the model‚Äôs ‚Äòknowledge‚Äô and how that might be unevenly distributed.</p>
<h2 id="question-6">Question 6</h2>
<p>What is the function of ‚ÄòPrefix Injection‚Äô in the suffix-based attack enhancement stage?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> To encrypt the prompt so it cannot be read by standard moderation APIs.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> To force the target model to begin its response with a harmless phrase, lowering the probability of refusal.</li>
<li class="task-list-item"><input type="checkbox" disabled> To identify the specific language used in the prompt for better translation.</li>
<li class="task-list-item"><input type="checkbox" disabled> To limit the response to a single sentence to avoid revealing the jailbreak.</li>
</ul>
<p><strong>Hint:</strong> Consider how the initial part of a model‚Äôs response can influence the content that follows.</p>
<h2 id="question-7">Question 7</h2>
<p>Why does the research suggest that ‚Äòsemantically inconsistent adversarial text‚Äô can still lead to successful jailbreaks?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> Because the model is trained primarily on semantically coherent texts for safety alignment.</li>
<li class="task-list-item"><input type="checkbox" disabled> Because incoherent text takes less time for the model to process, bypassing filters.</li>
<li class="task-list-item"><input type="checkbox" disabled> Because the model interprets semantic inconsistencies as a request for creative writing.</li>
<li class="task-list-item"><input type="checkbox" disabled> Because adversarial text directly modifies the attention heads to skip the safety layers.</li>
</ul>
<p><strong>Hint:</strong> Think about the data distribution used during the Reinforcement Learning from Human Feedback (RLHF) process.</p>
<h2 id="question-8">Question 8</h2>
<p>In the experiment‚Äôs hypothesis testing for early stopping, if the null hypothesis is $H_0: RSR_i \ge \alpha%$, what does rejecting the null hypothesis imply?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> The prompt is highly likely to be an effective jailbreak and should be evaluated further.</li>
<li class="task-list-item"><input type="checkbox" checked disabled> The prompt‚Äôs success rate is significantly lower than the desired threshold, and evaluation should stop.</li>
<li class="task-list-item"><input type="checkbox" disabled> The model has detected the jailbreak attempt and has blocked the user.</li>
<li class="task-list-item"><input type="checkbox" disabled> The attack has succeeded, and the identity of the person has been confirmed.</li>
</ul>
<p><strong>Hint:</strong> Remember that the goal of this specific step is to save time and tokens on invalid prompts.</p>
<h2 id="question-9">Question 9</h2>
<p>What was the highest Attack Success Rate (ASR) achieved by the optimized AutoJailbreak prompts in the study?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> 20.01%</li>
<li class="task-list-item"><input type="checkbox" disabled> 70.0%</li>
<li class="task-list-item"><input type="checkbox" checked disabled> 95.3%</li>
<li class="task-list-item"><input type="checkbox" disabled> 58.08%</li>
</ul>
<p><strong>Hint:</strong> This figure is cited as evidence that current alignment techniques are insufficient against adaptive adversaries.</p>
<h2 id="question-10">Question 10</h2>
<p>Which component of the AutoJailbreak combination injection specifically instructs the model NOT to use words like ‚Äòsorry‚Äô or ‚Äòapologize‚Äô?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Prefix Injection</li>
<li class="task-list-item"><input type="checkbox" disabled> Length Control</li>
<li class="task-list-item"><input type="checkbox" checked disabled> Refusal Suppression</li>
<li class="task-list-item"><input type="checkbox" disabled> Weak-to-strong Prompting</li>
</ul>
<p><strong>Hint:</strong> Consider the name of the technique that ‚Äòsuppresses‚Äô the model‚Äôs ability to say ‚Äòno‚Äô.</p>
<hr>
<h2 id="-resources">üìé Resources</h2>
<ul>
<li><a href="https://arxiv.org/abs/2407.16686">arXiv Abstract</a></li>
<li><a href="https://arxiv.org/pdf/2407.16686.pdf">PDF</a></li>
<li><a href="../../notebooklm-output/2407.16686/artifacts/audio-overview.m4a">Audio Overview</a></li>
<li><a href="../../notebooklm-output/2407.16686/artifacts/research-report.md">Research Report</a></li>
<li><a href="../../notebooklm-output/2407.16686/artifacts/study-guide.md">Study Guide</a></li>
<li><a href="../../notebooklm-output/2407.16686/artifacts/faq.md">FAQ</a></li>
</ul>
<hr>
<p><em>This post was generated automatically from NotebookLM artifacts. Part of the <a href="../index.md">Daily Paper</a> series exploring cutting-edge research in embodied AI and failure-first approaches.</em></p>  </div> </article>  </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html> 