<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>Distraction is All You Need for Multimodal Large Language Model Jailbreaking | Daily Paper | Failure-First</title><meta name="description" content="Demonstrates a novel jailbreaking attack (CS-DJ) against multimodal LLMs by exploiting visual complexity and attention dispersion through structured query decomposition and contrasting subimages,..."><link rel="canonical" href="https://failurefirst.org/daily-paper/2026-02-22-250210794/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="article"><meta property="og:title" content="Distraction is All You Need for Multimodal Large Language Model Jailbreaking | Daily Paper | Failure-First"><meta property="og:description" content="Demonstrates a novel jailbreaking attack (CS-DJ) against multimodal LLMs by exploiting visual complexity and attention dispersion through structured query decomposition and contrasting subimages,..."><meta property="og:url" content="https://failurefirst.org/daily-paper/2026-02-22-250210794/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="Distraction is All You Need for Multimodal Large Language Model Jailbreaking | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="Distraction is All You Need for Multimodal Large Language Model Jailbreaking | Daily Paper | Failure-First"><meta name="twitter:description" content="Demonstrates a novel jailbreaking attack (CS-DJ) against multimodal LLMs by exploiting visual complexity and attention dispersion through structured query decomposition and contrasting subimages,..."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="Distraction is All You Need for Multimodal Large Language Model Jailbreaking | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="article:published_time" content="2026-02-22"><!-- Google Scholar Meta Tags (for research papers) --><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
</style>
<link rel="stylesheet" href="/assets/_slug_.4MQlVLr6.css"></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/daily-paper/" class="active" aria-current="page" data-astro-cid-pux6a34n> Daily Paper  </a>  </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/daily-paper/" data-astro-cid-ilhxcym7>Daily Paper</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>Distraction is All You Need for Multimodal Large Language Model Jailbreaking</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Daily Paper","item":"https://failurefirst.org/daily-paper/"},{"@type":"ListItem","position":3,"name":"Distraction is All You Need for Multimodal Large Language Model Jailbreaking"}]}</script>  <article class="daily-paper" data-astro-cid-4f4ngxwt> <header class="paper-header" data-astro-cid-4f4ngxwt> <div class="paper-meta-top" data-astro-cid-4f4ngxwt> <time class="paper-date" datetime="2026-02-22T00:00:00.000Z" data-astro-cid-4f4ngxwt>February 22, 2026</time> <span class="paper-series" data-astro-cid-4f4ngxwt>Daily Paper</span> </div> <h1 data-astro-cid-4f4ngxwt>Distraction is All You Need for Multimodal Large Language Model Jailbreaking</h1> <p class="paper-description" data-astro-cid-4f4ngxwt>Demonstrates a novel jailbreaking attack (CS-DJ) against multimodal LLMs by exploiting visual complexity and attention dispersion through structured query decomposition and contrasting subimages,...</p> <div class="paper-meta-row" data-astro-cid-4f4ngxwt> <a href="https://arxiv.org/abs/2502.10794" class="arxiv-badge" target="_blank" rel="noopener noreferrer" data-astro-cid-4f4ngxwt>
arXiv:2502.10794 </a> <span class="paper-type-badge" data-astro-cid-4f4ngxwt>Empirical Study</span> </div> <p class="paper-authors" data-astro-cid-4f4ngxwt>Zuopeng Yang, Jiluan Fan, Anli Yan, Erdun Gao et al.</p>  </header>  <div class="paper-content" data-astro-cid-4f4ngxwt>  <h1 id="distraction-is-all-you-need-for-multimodal-large-language-model-jailbreaking">Distraction is All You Need for Multimodal Large Language Model Jailbreaking</h1>
<h2 id="abstract">Abstract</h2>
<p>Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce vulnerabilities, which may be exploited to bypass safety mechanisms. To address this, we analyze the relationship between image content and task and find that the complexity of subimages, rather than their content, is key. Building on this insight, we propose the Distraction Hypothesis, followed by a novel framework called Contrasting Subimage Distraction Jailbreaking (CS-DJ), to achieve jailbreaking by disrupting MLLMs alignment through multi-level distraction strategies. CS-DJ consists of two components: structured distraction, achieved through query decomposition that induces a distributional shift by fragmenting harmful prompts into sub-queries, and visual-enhanced distraction, realized by constructing contrasting subimages to disrupt the interactions among visual elements within the model. This dual strategy disperses the model‚Äôs attention, reducing its ability to detect and mitigate harmful content. Extensive experiments across five representative scenarios and four popular closed-source MLLMs, including GPT-4o-mini, GPT-4o, GPT-4V, and Gemini-1.5-Flash, demonstrate that CS-DJ achieves average success rates of 52.40% for the attack success rate and 74.10% for the ensemble attack success rate. These results reveal the potential of distraction-based approaches to exploit and bypass MLLMs‚Äô defenses, offering new insights for attack strategies.</p>
<hr>
<h2 id="Ô∏è-audio-overview">üéôÔ∏è Audio Overview</h2>
<p>(Audio overview not available)</p>
<hr>
<h2 id="-key-insights">üìä Key Insights</h2>
<h1 id="distraction-is-all-you-need-analysis-of-multimodal-large-language-model-jailbreaking">Distraction is All You Need: Analysis of Multimodal Large Language Model Jailbreaking</h1>
<h2 id="executive-summary">Executive Summary</h2>
<p>This briefing document analyzes the research paper ‚ÄúDistraction is All You Need for Multimodal Large Language Model Jailbreaking,‚Äù which identifies a significant security vulnerability in Multimodal Large Language Models (MLLMs). The central finding is that the complexity and diversity of visual elements, rather than their conceptual ‚Äúharmfulness,‚Äù are the primary drivers for bypassing safety mechanisms.</p>
<p>The researchers propose the <strong>Distraction Hypothesis</strong>, which suggests that overloading MLLMs with complex, fragmented inputs weakens their ability to detect and mitigate prohibited content. To exploit this, they developed the <strong>Contrasting Subimage Distraction Jailbreaking (CS-DJ)</strong> framework. This framework utilizes a dual-layered strategy of structured textual distraction and visual-enhanced distraction to achieve average Attack Success Rates (ASR) of 52.40% and Ensemble Attack Success Rates (EASR) of 74.10% against leading closed-source models, including GPT-4o and Gemini-1.5-Flash.</p>
<hr>
<h2 id="core-themes-and-the-distraction-hypothesis">Core Themes and The Distraction Hypothesis</h2>
<h3 id="the-distraction-hypothesis">The Distraction Hypothesis</h3>
<p>The researchers move beyond traditional jailbreaking methods‚Äîwhich often rely on injecting low-level noise or generating ‚Äúharmful‚Äù images‚Äîto focus on semantic-level distraction.</p>
<blockquote>
<p><strong>Distraction Hypothesis:</strong> ‚ÄúEncoding complex images in the input prompt increases token complexity/diversity, which raises the processing burden on MLLMs. This overload can weaken the model‚Äôs defenses, making it more prone to induce unintended outputs and improving jailbreak attack effectiveness.‚Äù</p>
</blockquote>
<h3 id="semantic-out-of-distribution-sood">Semantic Out-of-Distribution (SOOD)</h3>
<p>The vulnerability is grounded in the concept of Semantic Out-of-Distribution (SOOD) inputs. Because safety alignment (via RLHF) typically utilizes simple images and direct queries, inputs that are semantically diverse and locally inconsistent deviate from the model‚Äôs learned distribution, causing a degradation in its defensive capabilities.</p>
<hr>
<h2 id="detailed-analysis-of-the-cs-dj-framework">Detailed Analysis of the CS-DJ Framework</h2>
<p>The CS-DJ framework bypasses MLLM internal alignment through a three-step process designed to disperse the model‚Äôs attention.</p>
<h3 id="1-structured-distraction-query-decomposition">1. Structured Distraction (Query Decomposition)</h3>
<p>Instead of submitting a single harmful query, CS-DJ fragments the query into multiple sub-queries representing intermediate steps or different aspects of the original intent.</p>
<ul>
<li><strong>Method:</strong> An auxiliary model (e.g., Qwen2.5-3B-Instruct) decomposes the raw query.</li>
<li><strong>Transformation:</strong> Each sub-query is then transformed into a visual image (text rendered as an image). This modality shift further complicates the model‚Äôs ability to identify harmful patterns.</li>
</ul>
<h3 id="2-visual-enhanced-distraction-contrasting-subimages">2. Visual-Enhanced Distraction (Contrasting Subimages)</h3>
<p>The framework constructs a grid of subimages that have minimal similarity to the query and to each other.</p>
<ul>
<li><strong>Mechanism:</strong> Using CLIP-ViT-L/14, the system retrieves images from a dataset that maximize the ‚ÄúDistraction Distance‚Äù‚Äîa metric evaluating the L2 distance between CLIP-encoded vectors of the query and the visual elements.</li>
<li><strong>Selection:</strong> Subimages are chosen to be ‚Äúcontrasting‚Äù (least similar) to maximize the processing burden on the MLLM.</li>
</ul>
<h3 id="3-jailbreaking-execution-and-prompt-design">3. Jailbreaking Execution and Prompt Design</h3>
<p>The final input combines the sub-query images and the contrasting subimages into a single composite image, paired with a carefully designed harmless instruction.</p>





















<table><thead><tr><th align="left">Instruction Component</th><th align="left">Function</th></tr></thead><tbody><tr><td align="left"><strong>Role-Guiding</strong></td><td align="left">Establishes a benign persona or context for the interaction.</td></tr><tr><td align="left"><strong>Task-Guiding</strong></td><td align="left">Directs the model to solve multiple tasks simultaneously, dispersing focus.</td></tr><tr><td align="left"><strong>Visual-Guiding</strong></td><td align="left">Includes misleading cues suggesting that non-essential subimages contain vital information.</td></tr></tbody></table>
<hr>
<h2 id="performance-evaluation">Performance Evaluation</h2>
<p>The CS-DJ framework was tested across five scenarios (Animal, Financial, Privacy, Self-Harm, and Violence) against four major MLLMs.</p>
<h3 id="attack-success-rate-asr-comparison">Attack Success Rate (ASR) Comparison</h3>
<p>The following table highlights the performance of CS-DJ compared to the state-of-the-art ‚ÄúHades‚Äù baseline.</p>



































<table><thead><tr><th align="left">Victim Model</th><th align="left">Hades ASR (Avg)</th><th align="left">CS-DJ ASR (Avg)</th><th align="left">Improvement</th></tr></thead><tbody><tr><td align="left"><strong>GPT-4o-mini</strong></td><td align="left">6.08%</td><td align="left">57.80%</td><td align="left">+51.72%</td></tr><tr><td align="left"><strong>GPT-4o</strong></td><td align="left">5.51%</td><td align="left">42.24%</td><td align="left">+36.73%</td></tr><tr><td align="left"><strong>GPT-4V</strong></td><td align="left">20.33%</td><td align="left">45.44%</td><td align="left">+25.11%</td></tr><tr><td align="left"><strong>Gemini-1.5-Flash</strong></td><td align="left">9.20%</td><td align="left">64.11%</td><td align="left">+54.91%</td></tr></tbody></table>
<h3 id="key-experimental-findings">Key Experimental Findings</h3>
<ul>
<li><strong>Subimage Quantity:</strong> Success rates generally increase as the number of visual subimages increases. ASR showed significant growth from 0 subimages (3SQ) to 9 subimages (3SQ+9CSI).</li>
<li><strong>Query Decomposition:</strong> Decomposing a query into 6 sub-queries (6SQ) yielded a 29.86% ASR, compared to only 3.20% for raw queries (RQ) transformed into images.</li>
<li><strong>Information Complexity:</strong> Random noise images (RNI) do not distract the model effectively; subimages must have high information complexity to bypass defenses.</li>
<li><strong>Attention Dispersion:</strong> Attention map visualizations reveal that while baseline models focus on the ‚Äúharmful‚Äù portion of an image, CS-DJ successfully disperses the model‚Äôs attention across the entire multi-subimage grid.</li>
</ul>
<hr>
<h2 id="important-quotes-with-context">Important Quotes with Context</h2>
<p><strong>On the limitation of current safety training:</strong></p>
<blockquote>
<p>‚ÄúSignificant efforts have been devoted to using reinforcement learning from human feedback (RLHF) to align LLMs outputs‚Ä¶ however, the integration of visual inputs in MLLMs introduces a new challenge: securing models against vulnerabilities arising from newly integrated visual modalities.‚Äù</p>
</blockquote>
<ul>
<li><em>Context: Explaining why MLLMs are more susceptible to jailbreaking than text-only LLMs.</em></li>
</ul>
<p><strong>On the findings regarding image content:</strong></p>
<blockquote>
<p>‚ÄúOur findings reveal that it is the complexity of the subimages, rather than their conceptual content, that drives the jailbreak success.‚Äù</p>
</blockquote>
<ul>
<li><em>Context: Summarizing ablation studies that compared harmful images versus complex/contrasting images.</em></li>
</ul>
<p><strong>On the efficacy of distraction:</strong></p>
<blockquote>
<p>‚ÄúThe results demonstrate that distracting the model‚Äôs attention is a more effective strategy for enhancing jailbreak success rates [than enhancing visual harmfulness].‚Äù</p>
</blockquote>
<ul>
<li><em>Context: Comparing CS-DJ results against open-source models like LLaVA-OneVision.</em></li>
</ul>
<hr>
<h2 id="actionable-insights">Actionable Insights</h2>
<ol>
<li><strong>Defense Bottleneck:</strong> Current MLLM defenses are overly focused on identifying ‚Äúharmful‚Äù content and struggle with high-complexity, multi-tasking inputs that induce a distributional shift.</li>
<li><strong>Modality Exploitation:</strong> The ability to render text as images (typographic transformations) within a complex visual grid is a highly effective way to bypass text-based safety filters.</li>
<li><strong>Metric for Vulnerability:</strong> The ‚ÄúDistraction Distance‚Äù metric (measuring dispersion in the embedding space) can serve as a proxy for evaluating how likely a composite image is to trigger a jailbreak.</li>
<li><strong>Refinement of Alignment:</strong> To mitigate distraction-based attacks, safety alignment training must include more complex, multi-subimage, and out-of-distribution (OOD) scenarios rather than just simple, single-image-to-text pairs.</li>
<li><strong>Role of Instruction:</strong> While visual distraction is the primary driver, ‚Äútask-guiding‚Äù instructions that force the model to handle multiple objectives simultaneously significantly amplify the effectiveness of the attack.</li>
</ol>
<hr>
<h2 id="-study-guide">üìö Study Guide</h2>
<h1 id="distraction-and-multimodal-large-language-model-jailbreaking-a-study-guide">Distraction and Multimodal Large Language Model Jailbreaking: A Study Guide</h1>
<p>This study guide explores the vulnerabilities of Multimodal Large Language Models (MLLMs) to jailbreak attacks, specifically focusing on the ‚ÄúContrasting Subimage Distraction Jailbreaking‚Äù (CS-DJ) framework. It analyzes how multi-level distraction strategies can disrupt model alignment and bypass safety mechanisms.</p>
<hr>
<h2 id="1-key-concepts-and-theoretical-foundations">1. Key Concepts and Theoretical Foundations</h2>
<h3 id="the-distraction-hypothesis-1">The Distraction Hypothesis</h3>
<p>The central premise of this research is the <strong>Distraction Hypothesis</strong>. It posits that encoding complex images within an input prompt increases token complexity and diversity. This raises the processing burden on MLLMs, causing an ‚Äúoverload‚Äù that weakens the model‚Äôs internal safety defenses. This vulnerability makes the model more prone to producing unintended or harmful outputs.</p>
<h3 id="semantic-out-of-distribution-sood-1">Semantic Out-of-Distribution (SOOD)</h3>
<p>Unlike traditional adversarial attacks that rely on low-level visual noise, distraction-based jailbreaking leverages <strong>Semantic Out-of-Distribution</strong> (SOOD) inputs. These are semantically diverse and locally inconsistent inputs that deviate from the distributions the model encountered during safety alignment training (such as Reinforcement Learning from Human Feedback, or RLHF). When an input is classified as SOOD, the model struggles with semantic coherence, leading to a degradation of its defensive capabilities.</p>
<h3 id="the-cs-dj-framework">The CS-DJ Framework</h3>
<p><strong>Contrasting Subimage Distraction Jailbreaking (CS-DJ)</strong> is a novel framework designed to exploit visual and textual vulnerabilities through two primary distraction mechanisms:</p>
<ol>
<li><strong>Structured Distraction (Textual):</strong> Achieved via <strong>query decomposition</strong>, where a harmful prompt is fragmented into multiple sub-queries. This induces a distributional shift and disperses the model‚Äôs focus.</li>
<li><strong>Visual-Enhanced Distraction (Visual):</strong> Realized by constructing a composite image consisting of multiple <strong>contrasting subimages</strong>. These subimages are selected to have minimal similarity to the query and to each other, disrupting the interactions among visual elements within the model.</li>
</ol>
<hr>
<h2 id="2-methodology-of-the-cs-dj-attack">2. Methodology of the CS-DJ Attack</h2>
<p>The execution of a CS-DJ attack follows a three-step process to maximize model distraction.</p>
<h3 id="step-1-query-decomposition">Step 1: Query Decomposition</h3>
<p>The original harmful query ($Q$) is processed by an auxiliary model (such as Qwen2.5-3B-Instruct) to break it down into several sub-queries.</p>
<ul>
<li>Each sub-query represents a different aspect or intermediate step of the original harmful intent.</li>
<li>These sub-queries are then transformed into images (typographic visual elements) to further obscure their intent from the model‚Äôs textual safety filters.</li>
</ul>
<h3 id="step-2-contrasting-subimage-selection">Step 2: Contrasting Subimage Selection</h3>
<p>The framework retrieves ‚Äúdistractor‚Äù images from a dataset (e.g., LLaVA-CC3M) using the <strong>CLIP</strong> model. The goal is to maximize the <strong>Distraction Distance</strong>, a metric that calculates the L2 distance between CLIP-encoded vectors of the images and the query.</p>
<ul>
<li><strong>Process:</strong> The first subimage is selected for having the minimum cosine similarity to the query. Subsequent subimages are chosen based on their minimal similarity to both the query and the previously selected subimages.</li>
</ul>
<h3 id="step-3-jailbreaking-execution">Step 3: Jailbreaking Execution</h3>
<p>A final composite image is created, combining the sub-query images and the contrasting distractor subimages into a grid. This is paired with a carefully designed ‚Äúharmless‚Äù instruction ($P$) containing three sections:</p>
<ul>
<li><strong>Role-guiding:</strong> Establishes a benign scenario or persona for the model.</li>
<li><strong>Task-guiding:</strong> Directs the model to solve the ‚Äúproblems‚Äù presented in specific subimages (the sub-queries) simultaneously, increasing task complexity.</li>
<li><strong>Visual-guiding:</strong> Misleads the model by suggesting that the other (distractor) subimages might contain useful information, further diverting its attention.</li>
</ul>
<hr>
<h2 id="3-comparative-performance-and-findings">3. Comparative Performance and Findings</h2>
<p>Extensive experiments were conducted across five scenarios: <strong>Violence, Financial, Privacy, Self-Harm, and Animal</strong>.</p>
<h3 id="attack-success-rates-asr">Attack Success Rates (ASR)</h3>
<p>The study compared CS-DJ against <strong>Hades</strong>, a state-of-the-art jailbreak attack. CS-DJ consistently outperformed Hades across all evaluated closed-source models.</p>




















<table><thead><tr><th align="left">Metric</th><th align="left">Hades (Average)</th><th align="left">CS-DJ (Average)</th></tr></thead><tbody><tr><td align="left"><strong>Attack Success Rate (ASR)</strong></td><td align="left">~10.28%</td><td align="left"><strong>52.40%</strong></td></tr><tr><td align="left"><strong>Ensemble Attack Success Rate (EASR)</strong></td><td align="left">~20.50%</td><td align="left"><strong>74.10%</strong></td></tr></tbody></table>
<h3 id="vulnerability-by-model">Vulnerability by Model</h3>
<p>The research evaluated four popular closed-source MLLMs:</p>
<ul>
<li><strong>Gemini-1.5-Flash:</strong> Showed the highest vulnerability to CS-DJ, with an average ASR of 64.11%.</li>
<li><strong>GPT-4o-mini:</strong> Second most vulnerable (57.80% ASR).</li>
<li><strong>GPT-4V:</strong> Showed moderate vulnerability (45.44% ASR).</li>
<li><strong>GPT-4o:</strong> Demonstrated the strongest defense among the group, though still susceptible (42.24% ASR).</li>
</ul>
<hr>
<h2 id="4-short-answer-practice-questions">4. Short-Answer Practice Questions</h2>
<ol>
<li><strong>What is the ‚ÄúDistraction Distance‚Äù and how is it calculated?</strong></li>
<li><strong>How does the complexity of an image, rather than its conceptual content, affect jailbreak success according to the paper?</strong></li>
<li><strong>Identify the three components of the ‚Äúharmless instruction‚Äù used in the execution phase.</strong></li>
<li><strong>Why is query decomposition considered a form of ‚Äústructured distraction‚Äù?</strong></li>
<li><strong>Which closed-source model tested demonstrated the highest average Attack Success Rate (ASR) when targeted by CS-DJ?</strong></li>
<li><strong>How does CS-DJ differ from white-box adversarial attacks regarding the use of gradients?</strong></li>
<li><strong>What role does the CLIP-ViT-L/14 model play in the CS-DJ framework?</strong></li>
</ol>
<hr>
<h2 id="5-essay-prompts-for-deeper-exploration">5. Essay Prompts for Deeper Exploration</h2>
<ol>
<li><strong>The Evolution of MLLM Vulnerabilities:</strong> Discuss how the integration of visual modalities creates new security challenges that are distinct from those found in text-only Large Language Models. Use the concept of ‚Äúvisual-text decision boundaries‚Äù to explain why traditional text filters are insufficient.</li>
<li><strong>Complexity vs. Intent:</strong> Analyze the paper‚Äôs finding that image complexity is more critical for jailbreaking than the actual content of the subimages. What does this suggest about the internal processing mechanisms of transformer-based MLLMs?</li>
<li><strong>Defensive Strategies against Distraction:</strong> Based on the Distraction Hypothesis, propose potential defense mechanisms that MLLM developers could implement to mitigate the effectiveness of multi-subimage distraction attacks. Consider the role of RLHF and safety alignment in your answer.</li>
<li><strong>The Limitations of Semantic Distance Metrics:</strong> The authors note that CLIP-based Distraction Distance does not always perfectly correlate with ASR trends. Evaluate the challenges of measuring ‚Äúsemantic distraction‚Äù in a black-box environment and why a perfect metric remains elusive.</li>
</ol>
<hr>
<h2 id="6-glossary-of-important-terms">6. Glossary of Important Terms</h2>
<ul>
<li><strong>ASR (Attack Success Rate):</strong> The percentage of jailbreak attempts that successfully induce a harmful response from the model, as determined by a safety discriminator.</li>
<li><strong>Auxiliary Model:</strong> A secondary model (e.g., Qwen2.5-3B-Instruct) used by the attacker to perform specific tasks like query decomposition.</li>
<li><strong>CLIP (Contrastive Language-Image Pre-training):</strong> A model used to encode images and text into a shared vector space, allowing for the measurement of similarity between different modalities.</li>
<li><strong>EASR (Ensemble Attack Success Rate):</strong> A metric measuring the proportion of queries where at least one template in a group successfully bypasses the model‚Äôs defenses.</li>
<li><strong>Jailbreaking:</strong> The process of crafting specific inputs to manipulate an MLLM into bypassing its safety filters and producing prohibited content.</li>
<li><strong>MLLM (Multimodal Large Language Model):</strong> AI models designed to process and integrate multiple types of data, such as text and images (e.g., GPT-4o, Gemini).</li>
<li><strong>OOD (Out-of-Distribution):</strong> Inputs that fall outside the data distribution used during the model‚Äôs training and alignment phases.</li>
<li><strong>Query Decomposition:</strong> The technique of breaking a single, complex, or harmful prompt into smaller, seemingly innocuous sub-components.</li>
<li><strong>RLHF (Reinforcement Learning from Human Feedback):</strong> A training method used to align model outputs with human preferences and safety standards.</li>
<li><strong>Typographic Transformation:</strong> The process of converting text into a visual image (e.g., writing a harmful word in a red font on a white background) to bypass textual filters.</li>
</ul>
<hr>
<h2 id="-faq">‚ùì FAQ</h2>
<h1 id="jailbreak-quiz">Jailbreak Quiz</h1>
<h2 id="question-1">Question 1</h2>
<p>According to the Distraction Hypothesis proposed in the research, what is the primary cause of a Multimodal Large Language Model‚Äôs (MLLM) defense mechanism weakening?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> The encoding of complex and diverse images increases the processing burden, causing a defensive overload.</li>
<li class="task-list-item"><input type="checkbox" disabled> The model‚Äôs visual encoder becomes permanently desensitized after being exposed to high-frequency noise patterns.</li>
<li class="task-list-item"><input type="checkbox" disabled> Specific harmful pixels within the subimages directly trigger a bypass of the safety alignment layer.</li>
<li class="task-list-item"><input type="checkbox" disabled> The model prioritizes visual tokens over textual tokens, leading it to ignore safety instructions in the prompt.</li>
</ul>
<p><strong>Hint:</strong> Consider how an increased cognitive or processing load affects the model‚Äôs ability to maintain its safety boundaries.</p>
<h2 id="question-2">Question 2</h2>
<p>What are the two core components of the Contrasting Subimage Distraction Jailbreaking (CS-DJ) framework?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> Structured distraction via query decomposition and visual-enhanced distraction via contrasting subimages.</li>
<li class="task-list-item"><input type="checkbox" disabled> Adversarial perturbation injection and prompt-to-image semantic infection.</li>
<li class="task-list-item"><input type="checkbox" disabled> Typographic visual prompting and multi-step question rephrasing.</li>
<li class="task-list-item"><input type="checkbox" disabled> Reinforcement learning from human feedback (RLHF) and out-of-distribution (OOD) data filtering.</li>
</ul>
<p><strong>Hint:</strong> Think about the multi-level strategy that targets both the textual structure and the visual input diversity.</p>
<h2 id="question-3">Question 3</h2>
<p>In the context of the CS-DJ framework, how is ‚Äòstructured distraction‚Äô implemented?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> By fragmenting a harmful prompt into multiple sub-queries that represent different intermediate steps.</li>
<li class="task-list-item"><input type="checkbox" disabled> By shuffling the order of words within the prompt to confuse the model‚Äôs natural language processing unit.</li>
<li class="task-list-item"><input type="checkbox" disabled> By embedding hidden metadata in the textual instruction that overrides the model‚Äôs system prompt.</li>
<li class="task-list-item"><input type="checkbox" disabled> By using a secondary ‚Äòattacker‚Äô model to generate a completely unrelated benign story for the model to process.</li>
</ul>
<p><strong>Hint:</strong> Focus on the process of breaking a complex task down into its constituent parts to disperse the model‚Äôs focus.</p>
<h2 id="question-4">Question 4</h2>
<p>How does the CS-DJ framework select subimages for ‚Äòvisual-enhanced distraction‚Äô?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> It retrieves images from a dataset that have the minimal cosine similarity to both the query and to each other.</li>
<li class="task-list-item"><input type="checkbox" disabled> It generates highly harmful images using a diffusion model that match the semantics of the query.</li>
<li class="task-list-item"><input type="checkbox" disabled> It randomly selects any nine images from a pre-defined dataset of common household objects.</li>
<li class="task-list-item"><input type="checkbox" disabled> It utilizes high-resolution noise patterns to disrupt the model‚Äôs attention maps.</li>
</ul>
<p><strong>Hint:</strong> Think about the optimization goal that would result in the highest possible diversity among visual tokens.</p>
<h2 id="question-5">Question 5</h2>
<p>The researchers introduced the Distraction Distance metric ($D_{L}$) to quantify the dispersion within the multi-subimage structure. How is this metric calculated?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> $D_{L} = \sum_{i=1}^{N} \sum_{j \ne i} |v_{i} - v_{j}|_{2}$</li>
<li class="task-list-item"><input type="checkbox" disabled> $D_{L} = \frac{1}{N} \sum_{i=1}^{N} \cos(v_{i}, v_{query})$</li>
<li class="task-list-item"><input type="checkbox" disabled> $D_{L} = \max_{i,j} |v_{i} - v_{j}|_{2}$</li>
<li class="task-list-item"><input type="checkbox" disabled> $D_{L} = \sum_{i=1}^{N} \log(v_{i} \cdot v_{j})$</li>
</ul>
<p><strong>Hint:</strong> The calculation involves the sum of pairwise Euclidean distances between all encoded vector nodes in the structure.</p>
<h2 id="question-6">Question 6</h2>
<p>What surprising finding did the researchers discover regarding the relationship between image content and jailbreak success?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> The complexity of the subimages is more important for success than their actual conceptual or harmful content.</li>
<li class="task-list-item"><input type="checkbox" disabled> Models are only vulnerable to jailbreaking if the images contain violent or explicit imagery.</li>
<li class="task-list-item"><input type="checkbox" disabled> The success rate increases linearly with the resolution of the images, regardless of their complexity.</li>
<li class="task-list-item"><input type="checkbox" disabled> Subimages that are semantically identical to the text query are the most effective at bypassing defenses.</li>
</ul>
<p><strong>Hint:</strong> Consider whether the model is more confused by what an image ‚Äòshows‚Äô or how much ‚Äòinformation‚Äô it contains.</p>
<h2 id="question-7">Question 7</h2>
<p>Which of the following describes the ‚Äòvisual-guiding‚Äô component of the instruction prompt $P$ used in the jailbreaking execution phase?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> Misleading instructions informing the model that non-essential subimages may contain useful information.</li>
<li class="task-list-item"><input type="checkbox" disabled> A list of safety guidelines that the model must follow when analyzing the images.</li>
<li class="task-list-item"><input type="checkbox" disabled> Coordinates that tell the model exactly where to look within the composite image for harmful content.</li>
<li class="task-list-item"><input type="checkbox" disabled> A role-playing scenario where the model acts as a security expert evaluating vulnerabilities.</li>
</ul>
<p><strong>Hint:</strong> This part of the prompt is designed to make the model waste processing ‚Äòenergy‚Äô on irrelevant parts of the visual input.</p>
<h2 id="question-8">Question 8</h2>
<p>Based on the experimental results, how did increasing the number of sub-queries (from 3SQ to 6SQ) affect the Attack Success Rate (ASR) on GPT-4o?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> It significantly improved the ASR, adding an additional $11.06%$ to the success rate.</li>
<li class="task-list-item"><input type="checkbox" disabled> It caused a sharp decline in ASR because the tasks became too complex for the model to follow.</li>
<li class="task-list-item"><input type="checkbox" disabled> It had no measurable effect because the model‚Äôs vision encoder ignores text-transformed images.</li>
<li class="task-list-item"><input type="checkbox" disabled> It resulted in a $100%$ success rate across all categories due to total model confusion.</li>
</ul>
<p><strong>Hint:</strong> Look for the trend in success rates as the number of fragmented tasks increases in the ablation studies.</p>
<h2 id="question-9">Question 9</h2>
<p>In the theoretical foundation of the Distraction Hypothesis, an input is classified as Semantic Out-of-Distribution (SOOD) if it:</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> Leads the model to generate an undesired output, such as harmful content or unjustified refusals.</li>
<li class="task-list-item"><input type="checkbox" disabled> Contains low-level Gaussian noise that exceeds the threshold of the vision backbone‚Äôs sensitivity.</li>
<li class="task-list-item"><input type="checkbox" disabled> Is composed of images from a dataset that the model was never exposed to during its pre-training phase.</li>
<li class="task-list-item"><input type="checkbox" disabled> Has a Distraction Distance of zero, indicating perfect alignment with the model‚Äôs internal training data.</li>
</ul>
<p><strong>Hint:</strong> Consider the operational outcome that defines whether an input has successfully bypassed the model‚Äôs learned safety patterns.</p>
<h2 id="question-10">Question 10</h2>
<p>Why did the researchers find that using random noise images (RNI) resulted in a much lower ASR compared to contrasting subimages (CSI)?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> MLLMs have strong recognition capabilities for noise and can identify it as having minimal informational content.</li>
<li class="task-list-item"><input type="checkbox" disabled> Random noise contains hidden safety triggers that actually reinforce the model‚Äôs defensive layers.</li>
<li class="task-list-item"><input type="checkbox" disabled> The CLIP model cannot generate embeddings for noise, making it impossible to calculate Distraction Distance.</li>
<li class="task-list-item"><input type="checkbox" disabled> Noise images are automatically blocked by the model‚Äôs integrated image classifiers before processing.</li>
</ul>
<p><strong>Hint:</strong> Think about the difference between a ‚Äòblank‚Äô or ‚Äònoisy‚Äô input versus one filled with diverse, high-entropy information.</p>
<hr>
<h2 id="-resources">üìé Resources</h2>
<ul>
<li><a href="https://arxiv.org/abs/2502.10794">arXiv Abstract</a></li>
<li><a href="https://arxiv.org/pdf/2502.10794.pdf">PDF</a></li>
<li><a href="../../notebooklm-output/2502.10794/artifacts/audio-overview.mp3">Audio Overview</a></li>
<li><a href="../../notebooklm-output/2502.10794/artifacts/research-report.md">Research Report</a></li>
<li><a href="../../notebooklm-output/2502.10794/artifacts/study-guide.md">Study Guide</a></li>
<li><a href="../../notebooklm-output/2502.10794/artifacts/faq.md">FAQ</a></li>
</ul>
<hr>
<p><em>This post was generated automatically from NotebookLM artifacts. Part of the <a href="../index.md">Daily Paper</a> series exploring cutting-edge research in embodied AI and failure-first approaches.</em></p>  </div> </article>  </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html> 