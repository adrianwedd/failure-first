<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First</title><meta name="description" content="Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."><link rel="canonical" href="https://failurefirst.org/daily-paper/2026-02-18-240401318/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="article"><meta property="og:title" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First"><meta property="og:description" content="Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."><meta property="og:url" content="https://failurefirst.org/daily-paper/2026-02-18-240401318/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First"><meta name="twitter:description" content="Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="article:published_time" content="2026-02-18"><!-- Google Scholar Meta Tags (for research papers) --><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
</style>
<link rel="stylesheet" href="/assets/_slug_.4MQlVLr6.css"></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/daily-paper/" class="active" aria-current="page" data-astro-cid-pux6a34n> Daily Paper  </a>  </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/daily-paper/" data-astro-cid-ilhxcym7>Daily Paper</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Daily Paper","item":"https://failurefirst.org/daily-paper/"},{"@type":"ListItem","position":3,"name":"JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models"}]}</script>  <article class="daily-paper" data-astro-cid-4f4ngxwt> <header class="paper-header" data-astro-cid-4f4ngxwt> <div class="paper-meta-top" data-astro-cid-4f4ngxwt> <time class="paper-date" datetime="2026-02-18T00:00:00.000Z" data-astro-cid-4f4ngxwt>February 18, 2026</time> <span class="paper-series" data-astro-cid-4f4ngxwt>Daily Paper</span> </div> <h1 data-astro-cid-4f4ngxwt>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</h1> <p class="paper-description" data-astro-cid-4f4ngxwt>Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable...</p> <div class="paper-meta-row" data-astro-cid-4f4ngxwt> <a href="https://arxiv.org/abs/2404.01318" class="arxiv-badge" target="_blank" rel="noopener noreferrer" data-astro-cid-4f4ngxwt>
arXiv:2404.01318 </a> <span class="paper-type-badge" data-astro-cid-4f4ngxwt>Empirical Study</span> </div> <p class="paper-authors" data-astro-cid-4f4ngxwt>Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko et al.</p> <div class="paper-tags" data-astro-cid-4f4ngxwt> <span class="tag" data-astro-cid-4f4ngxwt>jailbreak-attacks</span><span class="tag" data-astro-cid-4f4ngxwt>llm-robustness-evaluation</span><span class="tag" data-astro-cid-4f4ngxwt>adversarial-prompts</span><span class="tag" data-astro-cid-4f4ngxwt>benchmark-standardization</span><span class="tag" data-astro-cid-4f4ngxwt>ai-safety-evaluation</span><span class="tag" data-astro-cid-4f4ngxwt>reproducibility-infrastructure</span> </div> </header> <div class="audio-section" data-astro-cid-4f4ngxwt> <div class="audio-label" data-astro-cid-4f4ngxwt> <span class="audio-icon" data-astro-cid-4f4ngxwt>&#x25B6;</span>
NotebookLM Audio Overview
</div> <audio controls preload="none" class="audio-player" data-astro-cid-4f4ngxwt> <source src="/audio/daily-paper/2404.01318-audio-overview.m4a" type="audio/mp4" data-astro-cid-4f4ngxwt>
Your browser does not support the audio element.
</audio> </div> <div class="paper-content" data-astro-cid-4f4ngxwt>  <h1 id="jailbreakbench-an-open-robustness-benchmark-for-jailbreaking-large-language-models">JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</h1>
<h2 id="overview">Overview</h2>
<p><strong>Paper Type:</strong> Empirical
<strong>Focus:</strong> Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable reproducible and comparable assessment of jailbreak attacks and defenses across LLMs.</p>
<h3 id="failure-first-relevance">Failure-First Relevance</h3>
<p>This paper directly addresses critical gaps in LLM safety evaluation by establishing standardized metrics, reproducible threat models, and open-source artifacts that enable systematic study of failure modes in alignment. By centralizing jailbreak evaluation methodology and creating comparable success rate measurements, it provides essential infrastructure for understanding how and why LLMs fail to maintain safety constraints under adversarial pressure. The leaderboard and artifact repository enable the research community to track defensive progress and identify emerging attack vectors before they become widespread threats.</p>
<hr>
<h2 id="abstract">Abstract</h2>
<p>Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors ‚Äî both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) ‚Äî which align with OpenAI‚Äôs usage policies; (3) a standardized evaluation framework at <a href="https://github.com/JailbreakBench/jailbreakbench">https://github.com/JailbreakBench/jailbreakbench</a> that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at <a href="https://jailbreakbench.github.io/">https://jailbreakbench.github.io/</a> that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.</p>
<hr>
<h2 id="Ô∏è-audio-overview">üéôÔ∏è Audio Overview</h2>
<p><a href="../../notebooklm-output/2404.01318/artifacts/audio-overview.m4a">Download Audio Overview</a></p>
<hr>
<h2 id="-key-insights">üìä Key Insights</h2>
<h2 id="executive-summary">Executive Summary</h2>
<p>JailbreakBench is an open-source, multi-component benchmark designed to address critical fragmentation in the evaluation of Large Language Model (LLM) robustness. While LLMs are increasingly deployed in safety-critical domains, they remain vulnerable to ‚Äújailbreaking‚Äù‚Äîadversarial attacks that bypass safety filters to generate harmful content.</p>
<p>Historically, the research community has lacked a standardized framework, leading to incomparable success rates and non-reproducible results. JailbreakBench solves these issues by providing a centralized repository of adversarial prompts (artifacts), a curated dataset of 100 harmful and 100 benign behaviors (JBB-Behaviors), a standardized evaluation pipeline, and a public leaderboard. This infrastructure enables researchers to systematically track the progress of attacks and defenses, facilitating ‚Äúfailure-first‚Äù AI safety research.</p>
<hr>
<h2 id="analysis-of-key-themes">Analysis of Key Themes</h2>
<h3 id="1-standardization-as-a-prerequisite-for-reproducibility">1. Standardization as a Prerequisite for Reproducibility</h3>
<p>The primary motivation behind JailbreakBench is the current lack of a ‚Äústandard of practice‚Äù in jailbreaking evaluation. The document identifies three specific challenges:</p>
<ul>
<li><strong>Incomparable Metrics:</strong> Different research papers compute costs and success rates in disparate ways.</li>
<li><strong>Lack of Transparency:</strong> Many works withhold adversarial prompts or involve closed-source code.</li>
<li><strong>Evolving APIs:</strong> Reliance on proprietary, ever-changing APIs makes past results impossible to replicate.</li>
</ul>
<p>JailbreakBench addresses this by open-sourcing everything from the chat templates and system prompts to the specific adversarial strings used to compromise models.</p>
<h3 id="2-the-four-component-architecture">2. The Four-Component Architecture</h3>
<p>The benchmark is built on four pillars designed to create a complete ecosystem for robustness testing:</p>






























<table><thead><tr><th align="left">Component</th><th align="left">Function</th><th align="left">Key Details</th></tr></thead><tbody><tr><td align="left"><strong>JBB-Behaviors Dataset</strong></td><td align="left">A curated set of 100 harmful behaviors.</td><td align="left">Aligned with OpenAI policies; includes 100 matching benign ‚Äúsanity check‚Äù behaviors.</td></tr><tr><td align="left"><strong>Artifact Repository</strong></td><td align="left">An evolving database of state-of-the-art (SOTA) prompts.</td><td align="left">Includes prompts for attacks like GCG, PAIR, and JailbreakChat.</td></tr><tr><td align="left"><strong>Evaluation Framework</strong></td><td align="left">A standardized software pipeline.</td><td align="left">Defines threat models, decoding parameters, and scoring functions.</td></tr><tr><td align="left"><strong>Leaderboard</strong></td><td align="left">A public tracking system.</td><td align="left">Compares robustness across models like Llama-2, Vicuna, GPT-3.5, and GPT-4.</td></tr></tbody></table>
<h3 id="3-the-llm-as-a-judge-paradigm">3. The ‚ÄúLLM-as-a-Judge‚Äù Paradigm</h3>
<p>A central challenge in jailbreaking is determining if an attack was actually successful. The document analyzes several classifiers, finding that human-like judgment is necessary to move beyond simple rule-based string matching.</p>
<ul>
<li><strong>Rule-based judges</strong> were found to be ineffective, with only 56% agreement with human experts.</li>
<li><strong>Llama-3-70B</strong> was selected as the benchmark‚Äôs primary judge because it achieves 90.7% agreement with human experts, comparable to GPT-4 (90.3%), while being an open-weight model that ensures long-term reproducibility.</li>
</ul>
<h3 id="4-vulnerability-trends-in-current-llms">4. Vulnerability Trends in Current LLMs</h3>
<p>Evaluations conducted using the benchmark reveal that even safety-aligned, closed-source models are highly vulnerable to adversarial pressure.</p>
<ul>
<li><strong>Prompt with RS (Random Search)</strong> was identified as the most effective attack, achieving a 90% success rate on Llama-2 and a 78% success rate on GPT-4.</li>
<li><strong>Defensive Gaps:</strong> While defenses like ‚ÄúErase-and-Check‚Äù showed significant promise in reducing success rates, many common defenses significantly increase inference time or fail against adaptive attacks.</li>
</ul>
<hr>
<h2 id="critical-data-points-attack-and-defense-performance">Critical Data Points: Attack and Defense Performance</h2>
<h3 id="attack-success-rates-asr">Attack Success Rates (ASR)</h3>
<p>The following table illustrates the vulnerability of undefended models to various attack artifacts included in the benchmark:</p>

































<table><thead><tr><th align="left">Attack Method</th><th align="left">Vicuna (Open)</th><th align="left">Llama-2 (Open)</th><th align="left">GPT-3.5 (Closed)</th><th align="left">GPT-4 (Closed)</th></tr></thead><tbody><tr><td align="left"><strong>PAIR</strong></td><td align="left">69%</td><td align="left">0%</td><td align="left">71%</td><td align="left">34%</td></tr><tr><td align="left"><strong>GCG</strong></td><td align="left">80%</td><td align="left">3%</td><td align="left">47%</td><td align="left">4%</td></tr><tr><td align="left"><strong>Prompt with RS</strong></td><td align="left">89%</td><td align="left">90%</td><td align="left">93%</td><td align="left">78%</td></tr></tbody></table>
<h3 id="judge-performance-comparison">Judge Performance Comparison</h3>
<p>Comparison of classifiers against a ground-truth dataset of 300 prompts (harmful and benign):</p>

































<table><thead><tr><th align="left">Metric</th><th align="left">Rule-based</th><th align="left">GPT-4</th><th align="left">Llama Guard 2</th><th align="left">Llama-3-70B</th></tr></thead><tbody><tr><td align="left"><strong>Agreement</strong></td><td align="left">56.0%</td><td align="left">90.3%</td><td align="left">87.7%</td><td align="left">90.7%</td></tr><tr><td align="left"><strong>False Positive Rate</strong></td><td align="left">64.2%</td><td align="left">10.0%</td><td align="left">13.2%</td><td align="left">11.6%</td></tr><tr><td align="left"><strong>False Negative Rate</strong></td><td align="left">9.1%</td><td align="left">9.1%</td><td align="left">10.9%</td><td align="left">5.5%</td></tr></tbody></table>
<hr>
<h2 id="important-quotes-with-context">Important Quotes with Context</h2>
<h3 id="on-the-necessity-of-open-artifacts">On the Necessity of Open Artifacts</h3>
<blockquote>
<p>‚ÄúIn releasing the jailbreak artifacts, we hope to expedite future research on jailbreaking, especially on the defense side‚Ä¶ we believe these jailbreaking artifacts can serve as an initial dataset for adversarial training against jailbreaks.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> The authors justify the potential risks of releasing harmful prompts by arguing that defensive research (e.g., fine-tuning models to refuse these specific prompts) cannot progress without access to the actual attack vectors.</p>
<h3 id="on-standardized-scoring">On Standardized Scoring</h3>
<blockquote>
<p>‚ÄúDetermining the success of an attack involves an understanding of human language and a subjective judgment of whether generated content is objectionable, which might be challenging even for humans.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> This highlights why simple ‚Äúkeyword filtering‚Äù is no longer sufficient for AI safety and why the benchmark emphasizes a sophisticated LLM-as-a-judge (Llama-3-70B).</p>
<h3 id="on-model-vulnerability">On Model Vulnerability</h3>
<blockquote>
<p>‚ÄúOverall, these results show that even recent and closed-source undefended models are highly vulnerable to jailbreaks.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> This summary follows the evaluation of GPT-4 and Llama-2, noting that the ‚ÄúPrompt with RS‚Äù attack maintained high success rates even against models with significant safety alignment.</p>
<hr>
<h2 id="actionable-insights">Actionable Insights</h2>
<h3 id="for-ai-safety-researchers">For AI Safety Researchers</h3>
<ul>
<li><strong>Utilize JBB-Behaviors for Refusal Evaluation:</strong> Use the matching benign behaviors to ensure that new safety filters are not ‚Äúover-refusing‚Äù (refusing safe content that merely shares keywords with harmful content).</li>
<li><strong>Prioritize Adaptive Attacks:</strong> When testing a new defense, do not rely solely on transferred artifacts from undefended models. Proper evaluation requires ‚Äúadaptive attacks‚Äù‚Äîthose specifically tailored to bypass the new defense mechanism.</li>
<li><strong>Adopt Llama-3-70B as the Standard Judge:</strong> To ensure results are comparable with the JailbreakBench leaderboard, researchers should use the provided Llama-3-70B judge prompt rather than custom or rule-based metrics.</li>
</ul>
<h3 id="for-developers-and-red-teamers">For Developers and Red-Teamers</h3>
<ul>
<li><strong>Baseline Your Model:</strong> Use the <code>jailbreakbench</code> library to run the four baseline attacks (GCG, PAIR, JB-Chat, RS) against any new LLM to establish a baseline robustness score.</li>
<li><strong>Implement ‚ÄúErase-and-Check‚Äù:</strong> According to initial JBB data, ‚ÄúErase-and-Check‚Äù is one of the most solid defenses across multiple models, though it should be monitored for its impact on inference speed.</li>
<li><strong>Monitor the Leaderboard:</strong> The evolving nature of the repository means that new ‚Äútransfer‚Äù prompts will be added regularly. Red-teamers should use these as a regression test suite for model safety updates.</li>
</ul>
<hr>
<h2 id="-study-guide">üìö Study Guide</h2>
<p>This study guide provides a comprehensive overview of <strong>JailbreakBench</strong>, an open-sourced benchmark designed to address the lack of standardization and reproducibility in the evaluation of Large Language Model (LLM) jailbreaking attacks and defenses.</p>
<hr>
<h2 id="1-key-concepts-and-architecture">1. Key Concepts and Architecture</h2>
<h3 id="overview-of-the-benchmark">Overview of the Benchmark</h3>
<p>JailbreakBench is a standardized framework created to measure how and why LLMs fail to maintain safety constraints under adversarial pressure. It addresses three primary challenges in the field:</p>
<ol>
<li><strong>Lack of Standards:</strong> No clear standard of practice existed for jailbreaking evaluation.</li>
<li><strong>Incomparability:</strong> Previous works computed costs and success rates in inconsistent ways.</li>
<li><strong>Reproducibility:</strong> Many researchers withheld adversarial prompts or relied on evolving, closed-source APIs.</li>
</ol>
<h3 id="the-four-core-components">The Four Core Components</h3>
<p>JailbreakBench is built upon four foundational pillars:</p>
<ul>
<li><strong>Jailbreak Artifacts Repository:</strong> An evolving collection of state-of-the-art adversarial prompts (jailbreak strings) used in successful attacks.</li>
<li><strong>JBB-Behaviors Dataset:</strong> A curated set of 100 harmful behaviors aligned with OpenAI‚Äôs usage policies, complemented by 100 matching benign behaviors used for ‚Äúrefusal rate‚Äù sanity checks.</li>
<li><strong>Standardized Evaluation Framework:</strong> A unified pipeline including defined threat models (black-box, white-box, transfer), system prompts, chat templates, and scoring functions.</li>
<li><strong>Leaderboard and Website:</strong> A public interface that tracks the performance of various LLMs against specific attacks and defenses.</li>
</ul>
<h3 id="the-jbb-behaviors-dataset">The JBB-Behaviors Dataset</h3>
<p>The benchmark uses 100 distinct misuse behaviors across ten broad categories. Approximately 45% are sourced from prior datasets (AdvBench, TDC/HarmBench), and 55% are original.</p>

















































<table><thead><tr><th align="left">Category</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><strong>Harassment/Discrimination</strong></td><td align="left">Hateful, harassing, or violent content generation.</td></tr><tr><td align="left"><strong>Malware/Hacking</strong></td><td align="left">Creation of malware or hacking scripts.</td></tr><tr><td align="left"><strong>Physical Harm</strong></td><td align="left">Activities posing a high risk of physical injury.</td></tr><tr><td align="left"><strong>Economic Harm</strong></td><td align="left">High-risk economic activities.</td></tr><tr><td align="left"><strong>Fraud/Deception</strong></td><td align="left">Deceptive activities or fraudulent schemes.</td></tr><tr><td align="left"><strong>Disinformation</strong></td><td align="left">Generation of false or misleading information.</td></tr><tr><td align="left"><strong>Sexual/Adult Content</strong></td><td align="left">Pornography or CSAM.</td></tr><tr><td align="left"><strong>Privacy</strong></td><td align="left">Activities violating individual privacy.</td></tr><tr><td align="left"><strong>Expert Advice</strong></td><td align="left">Tailored legal, medical, or economic advice.</td></tr><tr><td align="left"><strong>Gov. Decision-making</strong></td><td align="left">High-risk government recommendations.</td></tr></tbody></table>
<h3 id="jailbreaking-classifiers-the-judge">Jailbreaking Classifiers (The Judge)</h3>
<p>Evaluating the success of a jailbreak is subjective. JailbreakBench analyzed six classifiers to find a reliable ‚Äújudge.‚Äù</p>
<ul>
<li><strong>The Winner:</strong> <strong>Llama-3-70B</strong> was selected as the primary judge. It achieved a 90.7% agreement rate with human experts, comparable to GPT-4 (90.3%), but with the benefit of being an open-weight model, ensuring greater reproducibility.</li>
<li><strong>Other Candidates:</strong> Rule-based (lowest agreement at 56%), GPT-4, HarmBench, Llama Guard, and Llama Guard 2.</li>
</ul>
<hr>
<h2 id="2-short-answer-practice-questions">2. Short-Answer Practice Questions</h2>
<p><strong>1. What is the primary purpose of including 100 ‚Äúbenign behaviors‚Äù in the JBB-Behaviors dataset?</strong>
<em>Answer:</em> They serve as a sanity check for refusal rates. They ensure that defenses or models are not ‚Äúover-refusing‚Äù or becoming overly conservative by simply detecting keywords associated with harmful topics.</p>
<p><strong>2. Why does JailbreakBench prioritize the use of Llama-3-70B over GPT-4 as its evaluation judge?</strong>
<em>Answer:</em> While both perform similarly, GPT-4 is a closed-source model that is expensive to query and subject to unannounced changes by the provider. Llama-3-70B is an open-weight model, which supports the benchmark‚Äôs core goal of reproducibility.</p>
<p><strong>3. Describe the ‚ÄúPrompt with Random Search (RS)‚Äù attack and its performance in the benchmark.</strong>
<em>Answer:</em> Prompt with RS is an attack enhanced by self-transfer. In the benchmark‚Äôs evaluation, it proved to be the most effective attack on average, achieving high success rates (90% on Llama-2 and 78% on GPT-4) with high query efficiency.</p>
<p><strong>4. What are ‚Äútest-time defenses‚Äù in the context of LLMs?</strong>
<em>Answer:</em> These are ‚Äúwrappers‚Äù or filters placed around an LLM to detect or mitigate jailbreaks during inference. Examples include SmoothLLM (perturbing inputs) and Perplexity Filtering (detecting unusual token sequences).</p>
<p><strong>5. How does the benchmark handle ‚ÄúAdaptive Attacks‚Äù?</strong>
<em>Answer:</em> JailbreakBench encourages the use of adaptive attacks‚Äîattacks specifically tailored to bypass the defense being tested‚Äîto provide a realistic assessment of a defense‚Äôs worst-case robustness.</p>
<p><strong>6. Which defense was identified as the ‚Äúmost solid‚Äù against the initial set of baseline attacks?</strong>
<em>Answer:</em> <strong>Erase-and-Check</strong> appeared to be the most robust defense, although the ‚ÄúPrompt with RS‚Äù attack still achieved non-trivial success against it.</p>
<hr>
<h2 id="3-essay-prompts-for-deeper-exploration">3. Essay Prompts for Deeper Exploration</h2>
<p><strong>1. The Ethics of Open-Sourcing Malice:</strong>
The creators of JailbreakBench decided to open-source ‚Äújailbreak artifacts‚Äù (the exact prompts that break LLM safety). Analyze the ethical trade-offs of this decision. Consider the potential for misuse by malicious actors versus the benefits of providing a dataset for adversarial training and defensive research.</p>
<p><strong>2. The Challenge of Automated Alignment Evaluation:</strong>
Discuss the complexities involved in using an ‚ÄúLLM-as-a-judge.‚Äù Why is it difficult to create a rule-based classifier for jailbreaks, and what are the risks of a classifier having a high False Positive Rate (FPR) or False Negative Rate (FNR) in the context of AI safety?</p>
<p><strong>3. Standardizing the Threat Model:</strong>
How does the introduction of a standardized evaluation framework (system prompts, chat templates, and deterministic sampling) change the landscape of AI safety research? Explain how these controls help resolve the ‚Äúreproducibility crisis‚Äù in prior jailbreak studies.</p>
<hr>
<h2 id="4-glossary-of-important-terms">4. Glossary of Important Terms</h2>
<ul>
<li><strong>Adversarial Alignment:</strong> The process of training a model to refuse harmful requests even when the user intentionally tries to bypass safety filters.</li>
<li><strong>Attack Success Rate (ASR):</strong> The percentage of adversarial prompts that successfully elicit a harmful or objectionable response from the target LLM.</li>
<li><strong>Greedy Coordinate Gradient (GCG):</strong> An optimization-based attack that finds a single adversarial suffix to append to a prompt to trigger a jailbreak.</li>
<li><strong>Jailbreak Artifacts:</strong> Specific adversarial strings or prompts that have been proven to cause an LLM to bypass its safety training.</li>
<li><strong>Perplexity Filtering:</strong> A defense mechanism that calculates the ‚Äúrandomness‚Äù or ‚Äúuncertainty‚Äù of a prompt. Since adversarial suffixes are often gibberish, they typically have high perplexity and can be filtered out.</li>
<li><strong>Prompt Automatic Iterative Refinement (PAIR):</strong> An attack method that uses an auxiliary LLM to automatically generate and refine jailbreak prompts through iteration.</li>
<li><strong>Refusal Rate:</strong> The frequency with which a model declines to answer a prompt. This is measured on both harmful and benign datasets to check for over-sensitivity.</li>
<li><strong>SmoothLLM:</strong> A test-time defense that creates multiple perturbed versions of an input prompt and checks if the model refuses the majority of them, thereby ‚Äúsmoothing‚Äù out adversarial noise.</li>
<li><strong>Threat Model:</strong> A formal description of the attacker‚Äôs capabilities (e.g., ‚ÄúBlack-box‚Äù means the attacker only sees the model‚Äôs output; ‚ÄúWhite-box‚Äù means they have access to the model‚Äôs internal weights).</li>
</ul>
<hr>
<h2 id="-faq">‚ùì FAQ</h2>
<h2 id="question-1">Question 1</h2>
<p>Which of the following components in the JailbreakBench architecture is specifically designed to address the issue of ‚Äòjailbreak artifacts‚Äô disappearing when crowd-sourced websites go offline?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> The JBB-Behaviors dataset</li>
<li class="task-list-item"><input type="checkbox" checked disabled> The open-source repository of state-of-the-art adversarial prompts</li>
<li class="task-list-item"><input type="checkbox" disabled> The standardized evaluation framework</li>
<li class="task-list-item"><input type="checkbox" disabled> The Llama-3-70B judge classifier</li>
</ul>
<p><strong>Hint:</strong> Consider the specific term used in the paper for archived attack data.</p>
<h2 id="question-2">Question 2</h2>
<p>The JBB-Behaviors dataset categorizes its 100 misuse behaviors based on which organization‚Äôs policies?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> The NIST AI Risk Management Framework</li>
<li class="task-list-item"><input type="checkbox" disabled> Meta‚Äôs Llama Guard Safety Policy</li>
<li class="task-list-item"><input type="checkbox" checked disabled> OpenAI‚Äôs usage policies</li>
<li class="task-list-item"><input type="checkbox" disabled> The NeurIPS Datasets and Benchmarks Track guidelines</li>
</ul>
<p><strong>Hint:</strong> The dataset includes categories like ‚ÄòMalware/Hacking‚Äô and ‚ÄòEconomic harm‚Äô aligned with this company‚Äôs terms.</p>
<h2 id="question-3">Question 3</h2>
<p>In the judge selection process, why did the authors choose Llama-3-70B over GPT-4 despite both achieving similar agreement rates with human experts?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Llama-3-70B demonstrated a significantly lower False Positive Rate ($FPR$)</li>
<li class="task-list-item"><input type="checkbox" checked disabled> GPT-4 is a closed-source model subject to evolving APIs</li>
<li class="task-list-item"><input type="checkbox" disabled> Llama-3-70B was able to use the default Chao et al. (2023) system prompt without refusal</li>
<li class="task-list-item"><input type="checkbox" disabled> GPT-4 failed to achieve 90% agreement with expert annotators</li>
</ul>
<p><strong>Hint:</strong> Think about the core principle of JailbreakBench regarding reproducibility and accessibility.</p>
<h2 id="question-4">Question 4</h2>
<p>What is the primary function of the ‚ÄòBenign Behaviors‚Äô included in the JBB-Behaviors dataset?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> To provide a training set for RLHF alignment</li>
<li class="task-list-item"><input type="checkbox" checked disabled> To evaluate the ‚Äòover-refusal‚Äô or refusal rates of models and defenses</li>
<li class="task-list-item"><input type="checkbox" disabled> To act as the target string for adversarial optimization in GCG</li>
<li class="task-list-item"><input type="checkbox" disabled> To increase the dataset size to meet NeurIPS requirements</li>
</ul>
<p><strong>Hint:</strong> This helps identify if a model is ‚Äòoverly conservative‚Äô.</p>
<h2 id="question-5">Question 5</h2>
<p>Which attack method was found to be the most effective overall, achieving a $90%$ Attack Success Rate ($ASR$) on Llama-2-7B-chat?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Greedy Coordinate Gradient (GCG)</li>
<li class="task-list-item"><input type="checkbox" disabled> Prompt Automatic Iterative Refinement (PAIR)</li>
<li class="task-list-item"><input type="checkbox" checked disabled> Prompt with Random Search (RS) enhanced by self-transfer</li>
<li class="task-list-item"><input type="checkbox" disabled> Always Intelligent and Machiavellian (AIM)</li>
</ul>
<p><strong>Hint:</strong> This attack utilizes a manually optimized prompt template and pre-computed initialization.</p>
<h2 id="question-6">Question 6</h2>
<p>According to the evaluation of baseline defenses, which defense appeared to be the ‚Äòmost solid,‚Äô although it still allowed non-trivial success from certain attacks?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Perplexity Filter</li>
<li class="task-list-item"><input type="checkbox" disabled> SmoothLLM</li>
<li class="task-list-item"><input type="checkbox" checked disabled> Erase-and-Check</li>
<li class="task-list-item"><input type="checkbox" disabled> Synonym Substitution</li>
</ul>
<p><strong>Hint:</strong> This defense involves checking substrings of the input for potential jailbreak sequences.</p>
<h2 id="question-7">Question 7</h2>
<p>How does JailbreakBench formalize the task of jailbreaking in its theoretical background section?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> As a maximize problem: $\max P \in T^\star \text{ subject to } \text{Safety}(P) = 0$</li>
<li class="task-list-item"><input type="checkbox" checked disabled> As a search for prompt $P$ such that $JUDGE(LLM(P), G) = True$</li>
<li class="task-list-item"><input type="checkbox" disabled> As a cross-entropy loss minimization between $LLM(P)$ and a target string</li>
<li class="task-list-item"><input type="checkbox" disabled> As a Reinforcement Learning problem where reward is human preference</li>
</ul>
<p><strong>Hint:</strong> Look for the relationship between the prompt, the model output, and the judge‚Äôs verdict.</p>
<h2 id="question-8">Question 8</h2>
<p>Which metric is used in JailbreakBench to estimate the efficiency of a jailbreaking attack beyond its success rate?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" checked disabled> Average number of queries and tokens used</li>
<li class="task-list-item"><input type="checkbox" disabled> GPU hours required for optimization</li>
<li class="task-list-item"><input type="checkbox" disabled> Wall-clock time to generate a jailbreak</li>
<li class="task-list-item"><input type="checkbox" disabled> The semantic distance between the goal and the prompt</li>
</ul>
<p><strong>Hint:</strong> These metrics are standard for evaluating the ‚Äòcost‚Äô of black-box attacks.</p>
<h2 id="question-9">Question 9</h2>
<p>In the comparison of classifiers, which model showed a noticeably high False Positive Rate ($FPR = 26.8%$) on the 100 benign examples from XS-Test?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> Llama Guard 2</li>
<li class="task-list-item"><input type="checkbox" checked disabled> HarmBench (Llama-2-13B judge)</li>
<li class="task-list-item"><input type="checkbox" disabled> Rule-based classifier</li>
<li class="task-list-item"><input type="checkbox" disabled> GPT-4</li>
</ul>
<p><strong>Hint:</strong> This judge was introduced in a different benchmark mentioned in the paper.</p>
<h2 id="question-10">Question 10</h2>
<p>What is a significant limitation of the current JailbreakBench framework as noted in the ‚ÄòOutlook‚Äô or ‚ÄòLimitations‚Äô sections?</p>
<ul class="contains-task-list">
<li class="task-list-item"><input type="checkbox" disabled> It only supports white-box attacks on open-source models</li>
<li class="task-list-item"><input type="checkbox" checked disabled> It focuses solely on text-based modalities</li>
<li class="task-list-item"><input type="checkbox" disabled> It does not provide any standardized system prompts</li>
<li class="task-list-item"><input type="checkbox" disabled> The leaderboard is static and cannot accept new submissions</li>
</ul>
<p><strong>Hint:</strong> Consider the types of inputs (modalities) the benchmark accepts.</p>
<hr>
<h2 id="-resources">üìé Resources</h2>
<ul>
<li><a href="https://arxiv.org/abs/2404.01318">arXiv Abstract</a></li>
<li><a href="https://arxiv.org/pdf/2404.01318.pdf">PDF</a></li>
<li><a href="../../notebooklm-output/2404.01318/artifacts/audio-overview.m4a">Audio Overview</a></li>
<li><a href="../../notebooklm-output/2404.01318/artifacts/research-report.md">Research Report</a></li>
<li><a href="../../notebooklm-output/2404.01318/artifacts/study-guide.md">Study Guide</a></li>
<li><a href="../../notebooklm-output/2404.01318/artifacts/faq.md">FAQ</a></li>
</ul>
<hr>
<p><em>This post was generated automatically from NotebookLM artifacts. Part of the <a href="../index.md">Daily Paper</a> series exploring cutting-edge research in embodied AI and failure-first approaches.</em></p>  </div> </article>  </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html> 