<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First</title><meta name="description" content="Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."><link rel="canonical" href="https://failurefirst.org/daily-paper/2026-02-18-240401318/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="article"><meta property="og:title" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First"><meta property="og:description" content="Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."><meta property="og:url" content="https://failurefirst.org/daily-paper/2026-02-18-240401318/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First"><meta name="twitter:description" content="Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable..."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="article:published_time" content="2026-02-18"><!-- Google Scholar Meta Tags (for research papers) --><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
</style>
<link rel="stylesheet" href="/assets/_slug_.4MQlVLr6.css"></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/daily-paper/" class="active" aria-current="page" data-astro-cid-pux6a34n> Daily Paper  </a>  </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/daily-paper/" data-astro-cid-ilhxcym7>Daily Paper</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Daily Paper","item":"https://failurefirst.org/daily-paper/"},{"@type":"ListItem","position":3,"name":"JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models"}]}</script>  <article class="daily-paper" data-astro-cid-4f4ngxwt> <header class="paper-header" data-astro-cid-4f4ngxwt> <div class="paper-meta-top" data-astro-cid-4f4ngxwt> <time class="paper-date" datetime="2026-02-18T00:00:00.000Z" data-astro-cid-4f4ngxwt>February 18, 2026</time> <span class="paper-series" data-astro-cid-4f4ngxwt>Daily Paper</span> </div> <h1 data-astro-cid-4f4ngxwt>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</h1> <p class="paper-description" data-astro-cid-4f4ngxwt>Introduces JailbreakBench, an open-sourced benchmark with standardized evaluation framework, dataset of 100 harmful behaviors, repository of adversarial prompts, and leaderboard to enable...</p> <div class="paper-meta-row" data-astro-cid-4f4ngxwt> <a href="https://arxiv.org/abs/2404.01318" class="arxiv-badge" target="_blank" rel="noopener noreferrer" data-astro-cid-4f4ngxwt>
arXiv:2404.01318 </a> <span class="paper-type-badge" data-astro-cid-4f4ngxwt>Empirical Study</span> </div> <p class="paper-authors" data-astro-cid-4f4ngxwt>Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko et al.</p> <div class="paper-tags" data-astro-cid-4f4ngxwt> <span class="tag" data-astro-cid-4f4ngxwt>jailbreak-attacks</span><span class="tag" data-astro-cid-4f4ngxwt>llm-robustness-evaluation</span><span class="tag" data-astro-cid-4f4ngxwt>adversarial-prompts</span><span class="tag" data-astro-cid-4f4ngxwt>benchmark-standardization</span><span class="tag" data-astro-cid-4f4ngxwt>ai-safety-evaluation</span><span class="tag" data-astro-cid-4f4ngxwt>reproducibility-infrastructure</span> </div> </header> <div class="audio-section" data-astro-cid-4f4ngxwt> <div class="audio-label" data-astro-cid-4f4ngxwt> <span class="audio-icon" data-astro-cid-4f4ngxwt>&#x25B6;</span>
NotebookLM Audio Overview
</div> <audio controls preload="none" class="audio-player" data-astro-cid-4f4ngxwt> <source src="/audio/daily-paper/2404.01318-audio-overview.m4a" type="audio/mp4" data-astro-cid-4f4ngxwt>
Your browser does not support the audio element.
</audio> </div> <div class="paper-content" data-astro-cid-4f4ngxwt>  <h1 id="jailbreakbench-an-open-robustness-benchmark-for-jailbreaking-large-language-models">JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</h1>
<p>If you‚Äôre trying to understand how LLMs fail under adversarial pressure, you face an immediate problem: there‚Äôs no agreed-upon way to measure it. Different research groups test jailbreaks differently, report success rates in incomparable ways, and often refuse to share the prompts that actually work. This fragmentation makes it nearly impossible to know whether a new defense actually improves robustness or just evades one particular attack methodology. Without standardization, the field accumulates isolated findings that don‚Äôt cumulate into actionable knowledge about failure modes.</p>
<p><a href="https://arxiv.org/abs/2404.01318">arxiv.org</a> introduces JailbreakBench, which tackles this by establishing shared infrastructure: a dataset of 100 harmful behaviors aligned with real-world usage policies, an open repository of adversarial prompts that actually succeed, a standardized evaluation framework with clearly defined threat models and scoring functions, and a public leaderboard tracking attack and defense performance across multiple LLMs. The researchers built this around reproducibility as a first principle‚Äîeverything is open-sourced, with explicit chat templates and system prompts so results can be replicated without proprietary APIs.</p>
<p>For practitioners working on safety, this matters because you can now measure whether your defenses actually work and compare your results to others‚Äô work in a meaningful way. More importantly, the leaderboard and artifact repository create visibility into which attack vectors are most reliable and how defenses degrade over time. You can watch failure modes emerge and spread across different models, rather than discovering them after deployment. This is the infrastructure that turns jailbreak research from scattered red-teaming into systematic failure analysis‚Äîwhich is precisely what you need to build systems that fail predictably rather than catastrophically.</p>
<hr>
<h2 id="Ô∏è-audio-overview">üéôÔ∏è Audio Overview</h2>
<audio controls style="width: 100%; max-width: 800px;">
  <source src="/audio/daily-paper/2404.01318-audio-overview.m4a" type="audio/mp4">
  Your browser does not support the audio element.
</audio>
<hr>
<h2 id="Ô∏è-mind-map">üó∫Ô∏è Mind Map</h2>
<p><a href="/mindmaps/daily-paper/2404.01318-mindmap.json">Download mind map (JSON)</a></p>
<hr>
<h2 id="-infographic">üìä Infographic</h2>
<p><img src="/images/daily-paper/2404.01318-infographic.png" alt="Infographic: key concepts and findings"></p>
<hr>
<h2 id="abstract">Abstract</h2>
<p>Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (2) a jailbreaking dataset comprising 100 behaviors ‚Äî both original and sourced from prior work (Zou et al., 2023; Mazeika et al., 2023, 2024) ‚Äî which align with OpenAI‚Äôs usage policies; (3) a standardized evaluation framework at <a href="https://github.com/JailbreakBench/jailbreakbench">https://github.com/JailbreakBench/jailbreakbench</a> that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at <a href="https://jailbreakbench.github.io/">https://jailbreakbench.github.io/</a> that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.</p>
<hr>
<h2 id="key-insights">Key Insights</h2>
<h2 id="executive-summary">Executive Summary</h2>
<p>JailbreakBench is an open-source, multi-component benchmark designed to address critical fragmentation in the evaluation of Large Language Model (LLM) robustness. While LLMs are increasingly deployed in safety-critical domains, they remain vulnerable to ‚Äújailbreaking‚Äù‚Äîadversarial attacks that bypass safety filters to generate harmful content.</p>
<p>Historically, the research community has lacked a standardized framework, leading to incomparable success rates and non-reproducible results. JailbreakBench solves these issues by providing a centralized repository of adversarial prompts (artifacts), a curated dataset of 100 harmful and 100 benign behaviors (JBB-Behaviors), a standardized evaluation pipeline, and a public leaderboard. This infrastructure enables researchers to systematically track the progress of attacks and defenses, facilitating ‚Äúfailure-first‚Äù AI safety research.</p>
<hr>
<h2 id="analysis-of-key-themes">Analysis of Key Themes</h2>
<h3 id="1-standardization-as-a-prerequisite-for-reproducibility">1. Standardization as a Prerequisite for Reproducibility</h3>
<p>The primary motivation behind JailbreakBench is the current lack of a ‚Äústandard of practice‚Äù in jailbreaking evaluation. The document identifies three specific challenges:</p>
<ul>
<li><strong>Incomparable Metrics:</strong> Different research papers compute costs and success rates in disparate ways.</li>
<li><strong>Lack of Transparency:</strong> Many works withhold adversarial prompts or involve closed-source code.</li>
<li><strong>Evolving APIs:</strong> Reliance on proprietary, ever-changing APIs makes past results impossible to replicate.</li>
</ul>
<p>JailbreakBench addresses this by open-sourcing everything from the chat templates and system prompts to the specific adversarial strings used to compromise models.</p>
<h3 id="2-the-four-component-architecture">2. The Four-Component Architecture</h3>
<p>The benchmark is built on four pillars designed to create a complete ecosystem for robustness testing:</p>






























<table><thead><tr><th align="left">Component</th><th align="left">Function</th><th align="left">Key Details</th></tr></thead><tbody><tr><td align="left"><strong>JBB-Behaviors Dataset</strong></td><td align="left">A curated set of 100 harmful behaviors.</td><td align="left">Aligned with OpenAI policies; includes 100 matching benign ‚Äúsanity check‚Äù behaviors.</td></tr><tr><td align="left"><strong>Artifact Repository</strong></td><td align="left">An evolving database of state-of-the-art (SOTA) prompts.</td><td align="left">Includes prompts for attacks like GCG, PAIR, and JailbreakChat.</td></tr><tr><td align="left"><strong>Evaluation Framework</strong></td><td align="left">A standardized software pipeline.</td><td align="left">Defines threat models, decoding parameters, and scoring functions.</td></tr><tr><td align="left"><strong>Leaderboard</strong></td><td align="left">A public tracking system.</td><td align="left">Compares robustness across models like Llama-2, Vicuna, GPT-3.5, and GPT-4.</td></tr></tbody></table>
<h3 id="3-the-llm-as-a-judge-paradigm">3. The ‚ÄúLLM-as-a-Judge‚Äù Paradigm</h3>
<p>A central challenge in jailbreaking is determining if an attack was actually successful. The document analyzes several classifiers, finding that human-like judgment is necessary to move beyond simple rule-based string matching.</p>
<ul>
<li><strong>Rule-based judges</strong> were found to be ineffective, with only 56% agreement with human experts.</li>
<li><strong>Llama-3-70B</strong> was selected as the benchmark‚Äôs primary judge because it achieves 90.7% agreement with human experts, comparable to GPT-4 (90.3%), while being an open-weight model that ensures long-term reproducibility.</li>
</ul>
<h3 id="4-vulnerability-trends-in-current-llms">4. Vulnerability Trends in Current LLMs</h3>
<p>Evaluations conducted using the benchmark reveal that even safety-aligned, closed-source models are highly vulnerable to adversarial pressure.</p>
<ul>
<li><strong>Prompt with RS (Random Search)</strong> was identified as the most effective attack, achieving a 90% success rate on Llama-2 and a 78% success rate on GPT-4.</li>
<li><strong>Defensive Gaps:</strong> While defenses like ‚ÄúErase-and-Check‚Äù showed significant promise in reducing success rates, many common defenses significantly increase inference time or fail against adaptive attacks.</li>
</ul>
<hr>
<h2 id="critical-data-points-attack-and-defense-performance">Critical Data Points: Attack and Defense Performance</h2>
<h3 id="attack-success-rates-asr">Attack Success Rates (ASR)</h3>
<p>The following table illustrates the vulnerability of undefended models to various attack artifacts included in the benchmark:</p>

































<table><thead><tr><th align="left">Attack Method</th><th align="left">Vicuna (Open)</th><th align="left">Llama-2 (Open)</th><th align="left">GPT-3.5 (Closed)</th><th align="left">GPT-4 (Closed)</th></tr></thead><tbody><tr><td align="left"><strong>PAIR</strong></td><td align="left">69%</td><td align="left">0%</td><td align="left">71%</td><td align="left">34%</td></tr><tr><td align="left"><strong>GCG</strong></td><td align="left">80%</td><td align="left">3%</td><td align="left">47%</td><td align="left">4%</td></tr><tr><td align="left"><strong>Prompt with RS</strong></td><td align="left">89%</td><td align="left">90%</td><td align="left">93%</td><td align="left">78%</td></tr></tbody></table>
<h3 id="judge-performance-comparison">Judge Performance Comparison</h3>
<p>Comparison of classifiers against a ground-truth dataset of 300 prompts (harmful and benign):</p>

































<table><thead><tr><th align="left">Metric</th><th align="left">Rule-based</th><th align="left">GPT-4</th><th align="left">Llama Guard 2</th><th align="left">Llama-3-70B</th></tr></thead><tbody><tr><td align="left"><strong>Agreement</strong></td><td align="left">56.0%</td><td align="left">90.3%</td><td align="left">87.7%</td><td align="left">90.7%</td></tr><tr><td align="left"><strong>False Positive Rate</strong></td><td align="left">64.2%</td><td align="left">10.0%</td><td align="left">13.2%</td><td align="left">11.6%</td></tr><tr><td align="left"><strong>False Negative Rate</strong></td><td align="left">9.1%</td><td align="left">9.1%</td><td align="left">10.9%</td><td align="left">5.5%</td></tr></tbody></table>
<hr>
<h2 id="important-quotes-with-context">Important Quotes with Context</h2>
<h3 id="on-the-necessity-of-open-artifacts">On the Necessity of Open Artifacts</h3>
<blockquote>
<p>‚ÄúIn releasing the jailbreak artifacts, we hope to expedite future research on jailbreaking, especially on the defense side‚Ä¶ we believe these jailbreaking artifacts can serve as an initial dataset for adversarial training against jailbreaks.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> The authors justify the potential risks of releasing harmful prompts by arguing that defensive research (e.g., fine-tuning models to refuse these specific prompts) cannot progress without access to the actual attack vectors.</p>
<h3 id="on-standardized-scoring">On Standardized Scoring</h3>
<blockquote>
<p>‚ÄúDetermining the success of an attack involves an understanding of human language and a subjective judgment of whether generated content is objectionable, which might be challenging even for humans.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> This highlights why simple ‚Äúkeyword filtering‚Äù is no longer sufficient for AI safety and why the benchmark emphasizes a sophisticated LLM-as-a-judge (Llama-3-70B).</p>
<h3 id="on-model-vulnerability">On Model Vulnerability</h3>
<blockquote>
<p>‚ÄúOverall, these results show that even recent and closed-source undefended models are highly vulnerable to jailbreaks.‚Äù</p>
</blockquote>
<p><strong>Context:</strong> This summary follows the evaluation of GPT-4 and Llama-2, noting that the ‚ÄúPrompt with RS‚Äù attack maintained high success rates even against models with significant safety alignment.</p>
<hr>
<h2 id="actionable-insights">Actionable Insights</h2>
<h3 id="for-ai-safety-researchers">For AI Safety Researchers</h3>
<ul>
<li><strong>Utilize JBB-Behaviors for Refusal Evaluation:</strong> Use the matching benign behaviors to ensure that new safety filters are not ‚Äúover-refusing‚Äù (refusing safe content that merely shares keywords with harmful content).</li>
<li><strong>Prioritize Adaptive Attacks:</strong> When testing a new defense, do not rely solely on transferred artifacts from undefended models. Proper evaluation requires ‚Äúadaptive attacks‚Äù‚Äîthose specifically tailored to bypass the new defense mechanism.</li>
<li><strong>Adopt Llama-3-70B as the Standard Judge:</strong> To ensure results are comparable with the JailbreakBench leaderboard, researchers should use the provided Llama-3-70B judge prompt rather than custom or rule-based metrics.</li>
</ul>
<h3 id="for-developers-and-red-teamers">For Developers and Red-Teamers</h3>
<ul>
<li><strong>Baseline Your Model:</strong> Use the <code>jailbreakbench</code> library to run the four baseline attacks (GCG, PAIR, JB-Chat, RS) against any new LLM to establish a baseline robustness score.</li>
<li><strong>Implement ‚ÄúErase-and-Check‚Äù:</strong> According to initial JBB data, ‚ÄúErase-and-Check‚Äù is one of the most solid defenses across multiple models, though it should be monitored for its impact on inference speed.</li>
<li><strong>Monitor the Leaderboard:</strong> The evolving nature of the repository means that new ‚Äútransfer‚Äù prompts will be added regularly. Red-teamers should use these as a regression test suite for model safety updates.</li>
</ul>
<hr>
<p><em>Read the <a href="https://arxiv.org/abs/2404.01318">full paper on arXiv</a> ¬∑ <a href="https://arxiv.org/pdf/2404.01318.pdf">PDF</a></em></p>  </div> </article>  </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html> 