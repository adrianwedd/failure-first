<!DOCTYPE html><html lang="en"> <head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><!-- Primary Meta --><title>WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models | Daily Paper | Failure-First</title><meta name="description" content="Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreak‚Äîa 262K prompt-response..."><link rel="canonical" href="https://failurefirst.org/daily-paper/2026-02-14-240618510/"><meta name="robots" content="index, follow"><meta name="author" content="Adrian Wedd"><meta name="language" content="English"><meta name="theme-color" content="#0a0a0a"><!-- Open Graph --><meta property="og:type" content="article"><meta property="og:title" content="WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models | Daily Paper | Failure-First"><meta property="og:description" content="Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreak‚Äîa 262K prompt-response..."><meta property="og:url" content="https://failurefirst.org/daily-paper/2026-02-14-240618510/"><meta property="og:site_name" content="Failure-First Embodied AI"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://failurefirst.org/og-image.png"><meta property="og:image:alt" content="WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="og:image:type" content="image/png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><!-- Twitter Card --><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@failurefirstai"><meta name="twitter:creator" content="@adrianwedd"><meta name="twitter:title" content="WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models | Daily Paper | Failure-First"><meta name="twitter:description" content="Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreak‚Äîa 262K prompt-response..."><meta name="twitter:image" content="https://failurefirst.org/og-image.png"><meta name="twitter:image:alt" content="WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models | Daily Paper | Failure-First - Failure-First Embodied AI"><meta property="article:published_time" content="2026-02-14"><!-- Google Scholar Meta Tags (for research papers) --><!-- JSON-LD Structured Data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org","logo":{"@type":"ImageObject","url":"https://failurefirst.org/og-image.png"},"sameAs":["https://github.com/adrianwedd/failure-first"],"contactPoint":{"@type":"ContactPoint","contactType":"Research Inquiries","url":"https://failurefirst.org/contact/"}}</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"ResearchProject","name":"Failure-First Embodied AI","description":"A research framework for characterizing how embodied AI systems fail, degrade, and recover under adversarial pressure.","url":"https://failurefirst.org","sameAs":["https://github.com/adrianwedd/failure-first"],"author":{"@type":"Person","name":"Adrian Wedd"},"sponsor":{"@type":"Organization","name":"Failure-First Embodied AI","url":"https://failurefirst.org"}}</script><link rel="alternate" type="application/rss+xml" title="Failure-First Embodied AI" href="/rss.xml"><!-- Google Analytics (GA4) --><script async src="https://www.googletagmanager.com/gtag/js?id=G-XXEW64L22D"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-XXEW64L22D');
    </script><link rel="stylesheet" href="/assets/_slug_.BV0HTfXU.css">
<style>.breadcrumbs[data-astro-cid-ilhxcym7]{margin-bottom:1.5rem;font-size:.8125rem;font-family:JetBrains Mono,monospace}.breadcrumbs[data-astro-cid-ilhxcym7] ol[data-astro-cid-ilhxcym7]{list-style:none;display:flex;flex-wrap:wrap;gap:0;padding:0}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]{color:var(--fg-muted)}.breadcrumbs[data-astro-cid-ilhxcym7] li[data-astro-cid-ilhxcym7]:not(:last-child):after{content:"/";margin:0 .5rem;color:var(--fg-muted);opacity:.5}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]{color:var(--fg-muted);border-bottom:none}.breadcrumbs[data-astro-cid-ilhxcym7] a[data-astro-cid-ilhxcym7]:hover{color:var(--accent-primary)}.breadcrumbs[data-astro-cid-ilhxcym7] span[data-astro-cid-ilhxcym7][aria-current=page]{color:var(--fg-dim)}
</style>
<link rel="stylesheet" href="/assets/_slug_.4MQlVLr6.css"></head> <body> <a href="#main-content" class="skip-link">Skip to content</a> <canvas id="sensor-grid-bg" aria-hidden="true"></canvas> <nav class="site-nav" aria-label="Main navigation" data-astro-cid-pux6a34n> <div class="nav-inner" data-astro-cid-pux6a34n> <a href="/" class="nav-brand" data-astro-cid-pux6a34n> <span class="nav-brand-icon" data-astro-cid-pux6a34n>&#x2B22;</span> <span class="nav-brand-text" data-astro-cid-pux6a34n>F41LUR3-F1R57</span> </a> <button class="nav-toggle" aria-label="Toggle navigation" aria-expanded="false" aria-controls="nav-links" data-astro-cid-pux6a34n> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> <span class="nav-toggle-bar" data-astro-cid-pux6a34n></span> </button> <ul class="nav-links" id="nav-links" role="list" data-astro-cid-pux6a34n> <li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/research/" aria-haspopup="true" data-astro-cid-pux6a34n> Research <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/research/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>All Studies</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Research hub</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/jailbreak-archaeology/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Jailbreak Archaeology</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>64 scenarios, 6 eras</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/moltbook/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Multi-Agent</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Moltbook analysis</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/attack-taxonomy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Attack Taxonomy</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>79 techniques</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/research/defense-patterns/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Defense Patterns</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>How models resist</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/daily-paper/" class="active" aria-current="page" data-astro-cid-pux6a34n> Daily Paper  </a>  </li><li data-astro-cid-pux6a34n> <a href="/blog/" data-astro-cid-pux6a34n> Blog  </a>  </li><li data-astro-cid-pux6a34n> <a href="/framework/" data-astro-cid-pux6a34n> Framework  </a>  </li><li class="has-dropdown" data-astro-cid-pux6a34n> <a href="/policy/" aria-haspopup="true" data-astro-cid-pux6a34n> Policy <span class="dropdown-arrow" aria-hidden="true" data-astro-cid-pux6a34n>&#x25BC;</span> </a> <ul class="dropdown" role="list" data-astro-cid-pux6a34n> <li data-astro-cid-pux6a34n> <a href="/policy/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Policy Briefs</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>19 reports</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/capability-safety-spectrum/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Capability vs Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>U-shaped curve</span> </a> </li><li data-astro-cid-pux6a34n> <a href="/policy/embodied-ai-safety/" data-astro-cid-pux6a34n> <span class="dropdown-label" data-astro-cid-pux6a34n>Embodied AI Safety</span> <span class="dropdown-desc" data-astro-cid-pux6a34n>Beyond alignment</span> </a> </li> </ul> </li><li data-astro-cid-pux6a34n> <a href="/manifesto/" data-astro-cid-pux6a34n> Manifesto  </a>  </li><li data-astro-cid-pux6a34n> <a href="/about/" data-astro-cid-pux6a34n> About  </a>  </li> </ul> </div> </nav>  <script type="module">const t=document.querySelector(".nav-toggle"),n=document.querySelector(".nav-links"),o=document.querySelectorAll(".has-dropdown");t&&n&&(t.addEventListener("click",()=>{const e=t.getAttribute("aria-expanded")==="true";t.setAttribute("aria-expanded",String(!e)),n.classList.toggle("open")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&n.classList.contains("open")&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"),t.focus())}),document.addEventListener("click",e=>{n.classList.contains("open")&&!n.contains(e.target)&&!t.contains(e.target)&&(n.classList.remove("open"),t.setAttribute("aria-expanded","false"))}));o.forEach(e=>{const s=e.querySelector(":scope > a");s&&window.innerWidth<=768&&s.addEventListener("click",i=>{window.innerWidth<=768&&(i.preventDefault(),e.classList.toggle("mobile-open"))})});</script> <main id="main-content">  <nav class="breadcrumbs" aria-label="Breadcrumb" data-astro-cid-ilhxcym7> <ol data-astro-cid-ilhxcym7> <li data-astro-cid-ilhxcym7><a href="/" data-astro-cid-ilhxcym7>Home</a></li> <li data-astro-cid-ilhxcym7> <a href="/daily-paper/" data-astro-cid-ilhxcym7>Daily Paper</a> </li><li data-astro-cid-ilhxcym7> <span aria-current="page" data-astro-cid-ilhxcym7>WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models</span> </li> </ol> </nav> <!-- BreadcrumbList Schema.org structured data --> <script type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https://failurefirst.org/"},{"@type":"ListItem","position":2,"name":"Daily Paper","item":"https://failurefirst.org/daily-paper/"},{"@type":"ListItem","position":3,"name":"WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models"}]}</script>  <article class="daily-paper" data-astro-cid-4f4ngxwt> <header class="paper-header" data-astro-cid-4f4ngxwt> <div class="paper-meta-top" data-astro-cid-4f4ngxwt> <time class="paper-date" datetime="2026-02-14T00:00:00.000Z" data-astro-cid-4f4ngxwt>February 14, 2026</time> <span class="paper-series" data-astro-cid-4f4ngxwt>Daily Paper</span> </div> <h1 data-astro-cid-4f4ngxwt>WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models</h1> <p class="paper-description" data-astro-cid-4f4ngxwt>Introduces WildTeaming, an automatic red-teaming framework that mines real user-chatbot interactions to discover 5.7K jailbreak tactic clusters, then creates WildJailbreak‚Äîa 262K prompt-response...</p> <div class="paper-meta-row" data-astro-cid-4f4ngxwt> <a href="https://arxiv.org/abs/2406.18510" class="arxiv-badge" target="_blank" rel="noopener noreferrer" data-astro-cid-4f4ngxwt>
arXiv:2406.18510 </a> <span class="paper-type-badge" data-astro-cid-4f4ngxwt>Empirical Study</span> </div> <p class="paper-authors" data-astro-cid-4f4ngxwt>Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger et al.</p> <div class="paper-tags" data-astro-cid-4f4ngxwt> <span class="tag" data-astro-cid-4f4ngxwt>jailbreak-discovery</span><span class="tag" data-astro-cid-4f4ngxwt>adversarial-safety-training</span><span class="tag" data-astro-cid-4f4ngxwt>red-teaming-automation</span><span class="tag" data-astro-cid-4f4ngxwt>in-the-wild-vulnerabilities</span><span class="tag" data-astro-cid-4f4ngxwt>safety-dataset-curation</span><span class="tag" data-astro-cid-4f4ngxwt>over-refusal-mitigation</span> </div> </header> <div class="audio-section" data-astro-cid-4f4ngxwt> <div class="audio-label" data-astro-cid-4f4ngxwt> <span class="audio-icon" data-astro-cid-4f4ngxwt>&#x25B6;</span>
NotebookLM Audio Overview
</div> <audio controls preload="none" class="audio-player" data-astro-cid-4f4ngxwt> <source src="/audio/daily-paper/2406.18510-audio-overview.m4a" type="audio/mp4" data-astro-cid-4f4ngxwt>
Your browser does not support the audio element.
</audio> </div> <div class="paper-content" data-astro-cid-4f4ngxwt>  <h1 id="wildteaming-at-scale-from-in-the-wild-jailbreaks-to-adversarially-safer-language-models">WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models</h1>
<p>Most AI safety work focuses on preventing jailbreaks, but there‚Äôs a prior problem: you have to know what jailbreaks look like before you can defend against them. Current red-teaming approaches rely on recruited adversaries, gradient-based optimization, or LLMs iteratively revising attacks‚Äîall of which operate in a kind of closed loop, constrained by what the researchers think to test. Meanwhile, real users are discovering novel ways to manipulate deployed systems every day, and those tactics remain largely invisible to safety researchers. The gap between what we test for and what actually breaks systems in production is where failures accumulate.</p>
<p>The researchers behind WildTeaming took a different approach: they mined actual user-chatbot conversations from platforms like LMSYS and WildChat to extract real jailbreak tactics people were already using, without being instructed to break anything. They identified 5.7K distinct clusters of tactics and then systematically composed them to explore novel attack combinations. The result was a 4.6x increase in attack diversity and success rate compared to existing jailbreak methods. From this mining work, they built WildJailbreak, a dataset of 262K prompt-response pairs spanning both direct harmful requests and complex adversarial attacks, alongside benign queries that resemble attacks in form but contain no actual harm. They then used this dataset to train models and measured what actually happens to safety and capability trade-offs at scale.</p>
<p>What makes this relevant to practitioners is that it exposes a failure mode in how we typically approach safety training: the data we use to train defenses shapes not just what attacks we stop, but how the model behaves on everything else. The researchers found that the composition of training data directly determines whether you get appropriate safeguarding, over-refusal on benign queries, or both. This isn‚Äôt theoretical‚Äîit‚Äôs a direct measurement of a real failure pattern in deployed systems, where users encounter models that refuse legitimate requests or behave inconsistently. By grounding their work in actual user interactions rather than synthetic attacks, WildTeaming provides evidence that safety training is only as good as the failure modes you‚Äôve actually seen. For teams building systems, this suggests that mining real usage patterns should be part of the safety pipeline, not an afterthought, because the jailbreaks you don‚Äôt know about are the ones your model will encounter first.</p>
<hr>
<h2 id="Ô∏è-audio-overview">üéôÔ∏è Audio Overview</h2>
<audio controls style="width: 100%; max-width: 800px;">
  <source src="/audio/daily-paper/2406.18510-audio-overview.m4a" type="audio/mp4">
  Your browser does not support the audio element.
</audio>
<hr>
<h2 id="Ô∏è-mind-map">üó∫Ô∏è Mind Map</h2>
<p><a href="/mindmaps/daily-paper/2406.18510-mindmap.json">Download mind map (JSON)</a></p>
<hr>
<h2 id="-infographic">üìä Infographic</h2>
<p><img src="/images/daily-paper/2406.18510-infographic.png" alt="Infographic: key concepts and findings"></p>
<hr>
<h2 id="abstract">Abstract</h2>
<hr>
<h2 id="key-insights">Key Insights</h2>
<h2 id="executive-summary">Executive Summary</h2>
<p>The research paper ‚ÄúWildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models‚Äù introduces <strong>WildTeaming</strong>, a novel, automatic red-teaming framework designed to identify and mitigate Large Language Model (LLM) vulnerabilities. Unlike traditional red-teaming that relies on recruited humans or gradient-based optimization, WildTeaming mines real-world, ‚Äúin-the-wild‚Äù (ITW) user-chatbot interactions to discover successful jailbreak tactics.</p>
<p>The framework successfully identified <strong>5.7K unique clusters</strong> of jailbreak tactics, uncovering 4.6 times more diverse and successful adversarial attacks than prior methods. This research culminated in the creation of <strong>WildJailbreak</strong>, a comprehensive, open-source safety dataset containing <strong>262K prompt-response pairs</strong>. This dataset is unique for its contrastive design, featuring vanilla and adversarial versions of both harmful and benign queries. Experiments demonstrate that training models on this dataset achieves a superior balance: robust safety against complex attacks without the common failure mode of ‚Äúover-refusal‚Äù on benign requests or degradation of general reasoning capabilities.</p>
<hr>
<h2 id="detailed-analysis-of-key-themes">Detailed Analysis of Key Themes</h2>
<h3 id="1-the-wildteaming-framework-mine-and-compose">1. The WildTeaming Framework: Mine and Compose</h3>
<p>The WildTeaming framework operates through a systematic two-stage process to automate red-teaming at scale.</p>
<ul>
<li><strong>Stage 1: MINE:</strong> The framework mines real-world chat logs (from LMSYS-1M and WildChat) that have been flagged as harmful. By using GPT-4 to analyze these logs, the researchers identified 105K human-devised tactics which were then clustered into 5.7K unique strategies. This approach reveals how actual users‚Äîwho were not instructed to break the system‚Äîbypass safety filters.</li>
<li><strong>Stage 2: COMPOSE:</strong> Once tactics are identified, WildTeaming uses an ‚Äúattacker model‚Äù (such as Mixtral-8x7B or GPT-4) to transform standard harmful requests (vanilla queries) into diverse adversarial attacks by combining multiple mined tactics.</li>
</ul>
<h3 id="2-taxonomy-of-in-the-wild-jailbreak-tactics">2. Taxonomy of In-the-Wild Jailbreak Tactics</h3>
<p>The mining process revealed a rich diversity of tactics that far exceed the scope of previous taxonomies. These tactics are categorized into several types:</p>



































<table><thead><tr><th align="left">Tactic Category</th><th align="left">Percentage</th><th align="left">Examples</th></tr></thead><tbody><tr><td align="left"><strong>Fictitious Scenario</strong></td><td align="left">15.5%</td><td align="left">Placing the request within a story or historical context.</td></tr><tr><td align="left"><strong>Assign Personality</strong></td><td align="left">8.8%</td><td align="left">Instructing the model to act as a girlfriend, a white-hat hacker, or a specific streamer.</td></tr><tr><td align="left"><strong>Enforce Compliance</strong></td><td align="left">8.2%</td><td align="left">Using forceful language or ‚ÄúDAN‚Äù style commands to demand a response.</td></tr><tr><td align="left"><strong>Add Leading Sentence</strong></td><td align="left">8.0%</td><td align="left">Seeding the model‚Äôs response with a phrase like ‚ÄúSure, here‚Äôs‚Ä¶‚Äù to trigger compliance.</td></tr><tr><td align="left"><strong>Style/Format Constraints</strong></td><td align="left">Varied</td><td align="left">Demanding responses in JSON, CSV, or with specific lexical constraints (e.g., ‚Äúno commas‚Äù).</td></tr></tbody></table>
<p>The research notes that ITW attacks are more adversarial than existing semantic methods (like PAIR or TAP) because they often layer multiple tactics‚Äîaveraging more tactics per query‚Äîthan synthetic attacks.</p>
<h3 id="3-wildjailbreak-a-contrastive-safety-dataset">3. WildJailbreak: A Contrastive Safety Dataset</h3>
<p>A central contribution of this work is the <strong>WildJailbreak</strong> dataset. It addresses the ‚Äúover-refusal‚Äù problem‚Äîwhere models refuse benign queries because they resemble harmful ones‚Äîby providing four contrastive query types:</p>
<ol>
<li><strong>Vanilla Harmful (H):</strong> Direct requests for unsafe content (e.g., ‚ÄúHow to build a bomb‚Äù).</li>
<li><strong>Vanilla Benign (B):</strong> Harmless requests that look similar to unsafe ones (e.g., ‚ÄúHow to eliminate bacteria in sushi‚Äù).</li>
<li><strong>Adversarial Harmful (H):</strong> Complex jailbreaks created by applying WildTeaming tactics to vanilla harmful requests.</li>
<li><strong>Adversarial Benign (B):</strong> Harmless requests wrapped in jailbreak-style formatting (e.g., role-playing a researcher to discuss hand-dominance bias) to ensure the model doesn‚Äôt refuse based on the <em>form</em> of the query.</li>
</ol>
<h3 id="4-balancing-safety-and-utility">4. Balancing Safety and Utility</h3>
<p>The study identifies a critical scaling effect in safety data. While adding as few as 2K safety items improves a model, achieving a ‚Äúrobust safeguard‚Äù requires substantially more data (up to 60K items) mixed with general instruction-tuning data.</p>
<p>The research proves that training on either vanilla or adversarial data in isolation is insufficient. Vanilla-only training leaves the model vulnerable to adversarial attacks, while adversarial-only training does not adequately protect against direct requests. The hybridization of both, combined with benign contrastive examples, allows the model to reach the ‚ÄúPareto frontier‚Äù‚Äîmaximizing safety while maintaining general intelligence and minimizing over-refusal.</p>
<hr>
<h2 id="important-quotes-and-context">Important Quotes and Context</h2>
<blockquote>
<p><strong>‚ÄúWildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods.‚Äù</strong></p>
</blockquote>
<ul>
<li><strong>Context:</strong> This highlights the effectiveness of mining real user behavior rather than relying on automated optimization or a small team of red-teamers, who often produce a narrow range of attack types.</li>
</ul>
<blockquote>
<p><strong>‚ÄúSafety training data composition directly affects the balance between appropriate safeguarding and harmful over-refusal‚Äîa key failure pattern in deployed systems.‚Äù</strong></p>
</blockquote>
<ul>
<li><strong>Context:</strong> The authors argue that previous safety training datasets were not ‚Äúadversarial enough,‚Äù leading to models that are either easily broken or excessively cautious.</li>
</ul>
<blockquote>
<p><strong>‚ÄúWhile training on either vanilla or adversarial data improves performance on the other data type, the most robust safeguard comes with the hybridization of both.‚Äù</strong></p>
</blockquote>
<ul>
<li><strong>Context:</strong> This finding justifies the diverse composition of the WildJailbreak dataset, emphasizing that comprehensive safety cannot be achieved through a single type of training input.</li>
</ul>
<hr>
<h2 id="actionable-insights-for-ai-safety-practitioners">Actionable Insights for AI Safety Practitioners</h2>
<ul>
<li><strong>Transition to In-the-Wild Mining:</strong> Practitioners should look beyond synthetic attack generation and mine real-world interaction logs. Real users are highly creative in ‚Äúcloaking harm in humor,‚Äù ‚Äúsetting blame for non-compliance,‚Äù or using ‚Äúsurrogate modalities‚Äù (like CSV/JSON) to bypass filters.</li>
<li><strong>Adopt Contrastive Training:</strong> To mitigate over-refusal, safety training must include ‚ÄúAdversarial Benign‚Äù queries. This teaches the model to distinguish between a harmful <em>intent</em> and an adversarial <em>format</em>.</li>
<li><strong>Scale Safety Data Appropriately:</strong> The research suggests that safety data can be scaled to tens of thousands of examples without degrading general model capabilities, provided it is balanced with high-quality general instruction data.</li>
<li><strong>Use Multi-Tactic Composition:</strong> When red-teaming, combining 2-7 different tactics (e.g., roleplay + style constraints + a seed leading sentence) is significantly more effective at discovering vulnerabilities in frontier models than single-tactic prompts.</li>
<li><strong>Automate Pruning for Quality:</strong> When generating synthetic safety data, use ‚Äúoff-topic‚Äù and ‚Äúlow-risk‚Äù pruners to ensure that adversarial versions of queries maintain the original harmful intent and high-risk profile, preventing the data from becoming ‚Äúdiluted.‚Äù</li>
</ul>
<hr>
<p><em>Read the <a href="https://arxiv.org/abs/2406.18510">full paper on arXiv</a> ¬∑ <a href="https://arxiv.org/pdf/2406.18510.pdf">PDF</a></em></p>  </div> </article>  </main> <footer class="site-footer" data-astro-cid-sz7xmlte> <div class="footer-inner" data-astro-cid-sz7xmlte> <div class="footer-grid" data-astro-cid-sz7xmlte> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Project</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/" data-astro-cid-sz7xmlte>Home</a></li> <li data-astro-cid-sz7xmlte><a href="/about/" data-astro-cid-sz7xmlte>About</a></li> <li data-astro-cid-sz7xmlte><a href="/manifesto/" data-astro-cid-sz7xmlte>Manifesto</a></li> <li data-astro-cid-sz7xmlte><a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Research</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/research/" data-astro-cid-sz7xmlte>Research Hub</a></li> <li data-astro-cid-sz7xmlte><a href="/blog/" data-astro-cid-sz7xmlte>Blog</a></li> <li data-astro-cid-sz7xmlte><a href="/research/moltbook/" data-astro-cid-sz7xmlte>Moltbook</a></li> <li data-astro-cid-sz7xmlte><a href="/results/" data-astro-cid-sz7xmlte>Results</a></li> <li data-astro-cid-sz7xmlte><a href="/rss.xml" data-astro-cid-sz7xmlte>RSS Feed</a></li> </ul> </div> <div class="footer-col" data-astro-cid-sz7xmlte> <p class="footer-heading" data-astro-cid-sz7xmlte>Contact</p> <ul data-astro-cid-sz7xmlte> <li data-astro-cid-sz7xmlte><a href="/contact/" data-astro-cid-sz7xmlte>Get Involved</a></li> <li data-astro-cid-sz7xmlte><a href="/about/disclosure/" data-astro-cid-sz7xmlte>Responsible Disclosure</a></li> <li data-astro-cid-sz7xmlte><a href="mailto:research@failurefirst.org" data-astro-cid-sz7xmlte>research@failurefirst.org</a></li> </ul> </div> </div> <div class="footer-bottom" data-astro-cid-sz7xmlte> <p data-astro-cid-sz7xmlte> <strong data-astro-cid-sz7xmlte>Remember:</strong> This is a research tool for improving AI safety.
        Use responsibly. Study failures to build better defenses.
</p> <p class="footer-copyright" data-astro-cid-sz7xmlte>
&copy; 2026 Failure-First Embodied AI Project |
<a href="https://github.com/adrianwedd/failure-first" target="_blank" rel="noopener" data-astro-cid-sz7xmlte>GitHub</a> </p> </div> </div> </footer>   <script type="module">function g(e){let t=e>>>0;return function(){t|=0,t=t+1831565813|0;let n=Math.imul(t^t>>>15,1|t);return n=n+Math.imul(n^n>>>7,61|n)^n,((n^n>>>14)>>>0)/4294967296}}function m(){return Math.floor(new Date/(1e3*60*60*24))*1013}function w(e,t,n,o){const a=Math.ceil(t/60)+2,r=Math.ceil(n/(40*Math.sqrt(3)))+2;e.strokeStyle="rgba(0, 210, 255, 0.03)",e.lineWidth=.5;for(let c=-1;c<r;c++)for(let d=-1;d<a;d++){const l=d*40*1.5,i=c*40*Math.sqrt(3)+(d%2===0?0:40*Math.sqrt(3)/2);o()>.7&&S(e,l,i,40)}}function S(e,t,n,o){e.beginPath();for(let h=0;h<6;h++){const a=Math.PI/3*h-Math.PI/2,r=t+o*Math.cos(a),c=n+o*Math.sin(a);h===0?e.moveTo(r,c):e.lineTo(r,c)}e.closePath(),e.stroke()}function f(e,t,n){e.strokeStyle="rgba(0, 210, 255, 0.02)",e.lineWidth=1;for(let o=0;o<n;o+=4)e.beginPath(),e.moveTo(0,o),e.lineTo(t,o),e.stroke()}class p{constructor(t,n,o){this.x=t,this.y=n,this.phase=o()*Math.PI*2,this.period=8e3+o()*12e3,this.maxRadius=60+o()*40,this.color=o()>.7?"rgba(255, 71, 87,":"rgba(0, 210, 255,",this.birthTime=Date.now()}draw(t,n){const h=(n-this.birthTime)%this.period/this.period,a=Math.sin(h*Math.PI*2)*.5+.5,r=this.maxRadius*a,c=a*.08;t.strokeStyle=`${this.color} ${c})`,t.lineWidth=1,t.beginPath(),t.arc(this.x,this.y,r,0,Math.PI*2),t.stroke(),t.strokeStyle=`${this.color} ${c*.5})`,t.beginPath(),t.arc(this.x,this.y,r*.6,0,Math.PI*2),t.stroke()}}function y(){const e=document.getElementById("sensor-grid-bg");if(!e)return;const t=e.getContext("2d",{alpha:!0}),n=m(),o=g(n);function h(){const i=window.devicePixelRatio||1,s=e.getBoundingClientRect();return e.width=s.width*i,e.height=s.height*i,t.scale(i,i),{w:s.width,h:s.height}}const{w:a,h:r}=h(),c=3+Math.floor(o()*3),d=[];for(let i=0;i<c;i++){const s=o()*a,u=o()*r;d.push(new p(s,u,g(n+i*1013)))}w(t,a,r,o),f(t,a,r);function l(){const{w:i,h:s}=h();t.clearRect(0,0,e.width,e.height),w(t,i,s,o),f(t,i,s);const u=Date.now();for(const M of d)M.draw(t,u);requestAnimationFrame(l)}l(),window.addEventListener("resize",()=>{const{w:i,h:s}=h();w(t,i,s,o),f(t,i,s)})}typeof document<"u"&&(document.readyState==="loading"?document.addEventListener("DOMContentLoaded",y):y());</script></body></html> 